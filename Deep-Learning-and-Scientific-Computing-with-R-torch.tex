% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{krantz}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{makeidx}
\makeindex
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\hypersetup{
  pdftitle={Deep Learning and Scientific Computing with R torch},
  pdfauthor={Sigrid Keydana},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Deep Learning and Scientific Computing with R torch}
\author{Sigrid Keydana}
\date{2023-04-01}

\begin{document}
\maketitle
\renewcommand*\contentsname{Contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}

\markboth{Preface}{Preface}

This is a book about \texttt{torch}, the R interface to PyTorch.
PyTorch, as of this writing, is one of the major deep-learning and
scientific-computing frameworks, widely used across industries and areas
of research. With \texttt{torch}, you get to access its rich
functionality directly from R, with no need to install, let alone learn,
Python. Though still ``young'' as a project, \texttt{torch} already has
a vibrant community of users and developers; the latter not just
extending the core framework, but also, building on it in their own
packages.

In this text, I'm attempting to attain three goals, corresponding to the
book's three major sections.

The first is a thorough introduction to core \texttt{torch}: the basic
structures without whom nothing would work. Even though, in future work,
you'll likely go with higher-level syntactic constructs when possible,
it is important to know what it is they take care of, and to have
understood the core concepts. What's more, from a practical point of
view, you just need to be ``fluent'' in \texttt{torch} to some degree,
so you don't have to resort to ``trial-and-error-programming'' too
often.

In the second section, basics explained, we proceed to explore various
applications of deep learning, ranging from image recognition over time
series and tabular data to audio classification. Here, too, the focus is
on conceptual explanation. In addition, each chapter presents an
approach you can use as a ``template'' for your own applications.
Whenever adequate, I also try to point out the importance of
incorporating domain knowledge, as opposed to the not-uncommon ``big
data, big models, big compute'' approach.

The third section is special in that it highlights some of the
non-deep-learning things you can do with \texttt{torch}: matrix
computations (e.g., various ways of solving linear-regression problems),
calculating the Discrete Fourier Transform, and wavelet analysis. Here,
more than anywhere else, the conceptual approach is very important to
me. Let me explain.

For one, I expect that in terms of educational background, my readers
will vary quite a bit. With R being increasingly taught, and used, in
the natural sciences, as well as other areas close to applied
mathematics, there will be those who feel they can't benefit much from a
conceptual (though formula-guided!) explanation of how, say, the
Discrete Fourier Transform works. To others, however, much of this may
be uncharted territory, never to be entered if all goes its normal way.
This may hold, for example, for people with a humanist,
not-traditionally-empirically-oriented background, such as literature,
cultural studies, or the philologies. Of course, chances are that if
you're among the latter, you may find my explanations, though
concept-focused, still highly (or: too) mathematical. In that case,
please rest assured that, to the understanding of these things (like
many others worthwhile of understanding), it is a long way; but we have
a life's time.

Secondly, even though deep learning has been ``the'' paradigm of the
last decade, recent developments seem to indicate that interest in
mathematical/domain-based foundations is (\emph{again} -- this being a
recurring phenomenon) on the rise (Consider, for example, the Geometric
Deep Learning approach, systematically explained in Bronstein et al.
(2021), and conceptually introduced in
\href{https://blogs.rstudio.com/ai/posts/2021-08-26-geometric-deep-learning/}{\emph{Beyond
alchemy: A first look at geometric deep learning}}.) In the future, I
assume that we'll likely see more and more ``hybrid'' approaches that
integrate deep-learning techniques and domain knowledge. The Fourier
Transform is not going away.

Last but not least, on this topic, let me make clear that, of course,
all chapters have \texttt{torch} code. In case of the Fourier Transform,
for example, you'll see not just the official way of doing this, using
dedicated functionality, but also, various ways of coding the algorithm
yourself -- in a surprisingly small number of lines, and with highly
impressive performance.

This, in a nutshell, is what to expect from the book. Before I close,
there is one thing I absolutely need to say, all the more since even
though I'd have liked to, I did not find occasion to address it much in
the book, given the technicality of the content. In our societies, as
adoption of machine/deep learning (``AI'') is growing, so are
opportunities for misuse, by governments as well as private
organizations. Often, harm may not even be intended; but still, outcomes
can be catastrophic, especially for people belonging to minorities, or
groups already at a disadvantage. Like that, even the inevitable, in
most of today's political systems, drive to make profits results in, at
the very least, societies imbued with highly questionable features
(think: surveillance, and the ``quantification of everything''); and
most likely, in discrimination, unfairness, and severe harm. Here, I
cannot do more than draw attention to this problem, point you to an
introductory blog post that perhaps you'll find useful:
\href{https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/}{\emph{Starting
to think about AI Fairness}}, and just ask you to, please, be actively
aware of this problem in public life as well as your own work and
applications.

Finally, let me end with saying thank you. There are far too many people
to thank that I could ever be sure I haven't left anyone out; so instead
I'll keep this short. I'm extremely grateful to my publisher, CRC Press
(first and foremost, David Grubbs and Curtis Hill) for the
extraordinarily pleasant interactions during all of the writing and
editing phases. And \emph{very} special thanks, for their support
related to this book as well as their respective roles in the process,
go to Daniel Falbel, the creator and maintainer of \texttt{torch}, who
in-depth reviewed this book and helped me with many technical issues;
Tracy Teal, my manager, who supported and encouraged me in every
possible way; and Posit (formerly, RStudio), my employer, who lets me do
things like this for a living.

Sigrid Keydana

\part{Getting familiar with torch}

\hypertarget{sec:basics-overview}{%
\chapter{Overview}\label{sec:basics-overview}}

This book has three sections. The second and third will explore various
deep learning applications and essential scientific computation
techniques, respectively. Before though, in this first part, we are
going to learn about \texttt{torch}'s basic building blocks: tensors,
automatic differentiation, optimizers, and modules. I'd call this part
``\texttt{torch} basics'', or, following a common template, ``Getting
started with \texttt{torch}'', were it not for a certain false
impression this could create. These are basics, true, but basics in the
sense of \emph{foundations}: Having worked through the next chapters,
you'll have solid conceptions about how \texttt{torch} works, and you'll
have seen enough code to feel comfortable experimenting with the more
involved examples encountered in later sections. In other words, you'll
be, to some degree, \emph{fluent} in \texttt{torch}.

In addition, you'll have coded a neural network from scratch -- twice,
even: One version will involve just raw tensors and their in-built
capabilities, while the other will make use of dedicated \texttt{torch}
structures that encapsulate, in an object-oriented way, functionality
essential to neural network training. As a consequence, you'll be
excellently equipped for part two, where we'll see how to apply deep
learning to different tasks and domains.

\hypertarget{sec:basics-torch}{%
\chapter{\texorpdfstring{On \texttt{torch}, and how to get
it}{On torch, and how to get it}}\label{sec:basics-torch}}

\hypertarget{in-torch-world}{%
\section{\texorpdfstring{In \texttt{torch}
world}{In torch world}}\label{in-torch-world}}

\texttt{torch} is an R port of PyTorch, one of the two (as of this
writing) most-employed deep learning frameworks in industry and
research. By its design, it is also an excellent tool to use in various
types of scientific computation tasks (a subset of which you'll
encounter in the book's final part). It is written entirely in R and C++
(including a bit of C). No Python installation is required to use it.

On the Python (PyTorch) side, the ecosystem appears as a set of
concentric cycles. In the middle, there's PyTorch\index{PyTorch} itself,
the core library without which nothing could work. Surrounding it, we
have the inner circle of what could be called framework libraries,
dedicated to special types of data (images, sound, text \ldots), or
centered on workflow tasks, like deployment. Then, there is the broader
ecosystem of add-ons, specializations, and libraries for whom PyTorch is
a building block, or a tool.

On the R side, we have the same ``heart'' -- all depends on core
\texttt{torch} -- and we do have the same types of libraries; but the
categories, the ``circles'', appear less clearly set off from each
other. There are no strict boundaries. There's just a vibrant community
of developers, of diverse origin and with diverse goals, working to
further develop and extend \texttt{torch}, so it can help more and more
people accomplish their various tasks. The ecosystem growing so quickly,
I'll refrain from naming individual packages -- at any time, visit
\href{https://torch.mlverse.org/packages/}{the \texttt{torch} website}
to see a featured subset.

There are three packages, though, that I \emph{will} name here, since
they are used in the book: \texttt{torchvision} , \texttt{torchaudio},
and \texttt{luz}. The former two bundle domain-specific transformations,
deep learning models, datasets, and utilities for images (incl.~video)
and audio data, respectively. The third is a high-level, intuitive,
nice-to-use interface to \texttt{torch}, allowing to define, train, and
evaluate a neural network in just a few lines of code. Like
\texttt{torch} itself, all three packages can be installed from CRAN.

\hypertarget{installing-and-running-torch}{%
\section{Installing and running
torch}\label{installing-and-running-torch}}

\texttt{torch} is available for Windows, MacOS, and Linux. If you have a
compatible GPU, and the necessary NVidia software installed, you can
benefit from significant speedup, a speedup that will depend on the type
of model trained. All examples in this book, though, have been chosen so
they can be run on the CPU, without posing taxing demands on your
patience.

Due to their often-transient character, I won't elaborate on
compatibility issues here, in the book; analogously, I'll refrain from
listing concrete installation\index{installation} instructions. At any
time, you'll find up-to-date information in the
\href{https://cran.r-project.org/web/packages/torch/vignettes/installation.html}{vignette};
and you're more than welcome, should you encounter problems or have
questions, to open an issue in the \texttt{torch} GitHub repository.

\hypertarget{sec:tensors}{%
\chapter{Tensors}\label{sec:tensors}}

\hypertarget{whats-in-a-tensor}{%
\section{What's in a tensor?}\label{whats-in-a-tensor}}

To do anything useful with \texttt{torch}, you need to know about
tensors. Not tensors in the math/physics sense. In deep learning
frameworks such as TensorFlow and (Py-)Torch, \emph{tensors} are
``just'' multi-dimensional arrays optimized for fast computation -- not
on the CPU only but also, on specialized devices such as GPUs and TPUs.

In fact, a \texttt{torch} \texttt{tensor} is like an R \texttt{array},
in that it can be of arbitrary dimensionality. But unlike
\texttt{array}, it is designed for fast and scalable execution of
mathematical calculations, and you can move it to the GPU. (It also has
an extra capability of enormous practical impact -- automatic
differentiation -- but we reserve that for the next chapter.)

Technically, a \texttt{tensor} feels a lot like an R6 object, in that
you can access its fields and methods using \texttt{\$}-syntax. Let's
create one and print it:

\begin{verbatim}
library(torch)

t1 <- torch_tensor(1)
t1
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
[ CPUFloatType{1} ]
\end{verbatim}

This is a tensor that holds just a single value, 1. It ``lives'' on the
CPU, and its type is \texttt{Float} . Now take a look at the 1 in
braces, \texttt{\{1\}}. This is \emph{not} yet another indication of the
tensor's value. It indicates the tensor shape, or put differently: the
space it lives in and the extent of its dimensions. Here, we have a
one-dimensional tensor, that is, a vector. Just as in base R, vectors
can consist of a single element only. (Remember that base R does not
differentiate between \texttt{1} and \texttt{c(1)}).

We can use the aforementioned \texttt{\$}-syntax to individually
ascertain these properties, accessing the respective fields in the
object one-by-one:

\begin{verbatim}
t1$dtype
\end{verbatim}

\begin{verbatim}
torch_Float
\end{verbatim}

\begin{verbatim}
t1$device
\end{verbatim}

\begin{verbatim}
torch_device(type='cpu')
\end{verbatim}

\begin{verbatim}
t1$shape
\end{verbatim}

\begin{verbatim}
[1] 1
\end{verbatim}

We can also directly change some of these properties, making use of the
tensor object's \texttt{\$to()} method:

\begin{verbatim}
t2 <- t1$to(dtype = torch_int())
t2$dtype
\end{verbatim}

\begin{verbatim}
torch_Int
\end{verbatim}

\begin{verbatim}
# only applicable if you have a GPU
t2 <- t1$to(device = "cuda")
t2$device
\end{verbatim}

\begin{verbatim}
torch_device(type='cuda', index=0)
\end{verbatim}

How about changing the shape? This is a topic deserving of treatment of
its own, but as a first warm-up, let's play around a bit. Without
changing its value, we can turn this one-dimensional ``vector tensor''
into a two-dimensional ``matrix tensor'':

\begin{verbatim}
t3 <- t1$view(c(1, 1))
t3$shape
\end{verbatim}

\begin{verbatim}
[1] 1 1
\end{verbatim}

Conceptually, this is analogous to how in R, we can have a one-element
vector as well as a one-element matrix:

\begin{verbatim}
c(1)
matrix(1)
\end{verbatim}

\begin{verbatim}
[1] 1

     [,1]
[1,]    1
\end{verbatim}

Now that we have an idea what a tensor is, let's think about ways to
create some.

\hypertarget{creating-tensors}{%
\section{Creating tensors}\label{creating-tensors}}

We've already seen one way to create a tensor: calling
\texttt{torch\_tensor()} and passing in an R value. This way generalizes
to multi-dimensional objects; we'll see a few examples soon.

However, that procedure can get unwieldy when we have to pass in lots of
different values. Luckily, there is an alternative approach that applies
whenever values should be identical throughout, or follow an apparent
pattern. We'll illustrate this technique as well in this section.

\hypertarget{tensors-from-values}{%
\subsection{\texorpdfstring{Tensors from
values\index{tensors!create from values}}{Tensors from values}}\label{tensors-from-values}}

Above, we passed in a one-element vector to \texttt{torch\_tensor()}; we
can pass in longer vectors just the same way:

\begin{verbatim}
torch_tensor(1:5)
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 2
 3
 4
 5
[ CPULongType{5} ]
\end{verbatim}

When given an R value (or a sequence of values), \texttt{torch}
determines a suitable data type itself. Here, the assumption is that an
integer type is desired, and \texttt{torch} chooses the
highest-precision type available (\texttt{torch\_long()} is synonymous
to \texttt{torch\_int64()}).

If we want a floating-point tensor instead, we can use \texttt{\$to()}
on the newly created instance (as we saw above). Alternatively, we can
just let \texttt{torch\_tensor()} know right away:

\begin{verbatim}
torch_tensor(1:5, dtype = torch_float())
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 2
 3
 4
 5
[ CPUFloatType{5} ]
\end{verbatim}

Analogously, the default device is the CPU; but we can also create a
tensor that, right from the outset, is located on the GPU:

\begin{verbatim}
torch_tensor(1:5, device = "cuda")
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 2
 3
 4
 5
[ CPUFloatType{5} ]
\end{verbatim}

Now, so far all we've been creating is vectors; what about matrices,
that is, two-dimensional tensors?

We can pass in an R matrix just the same way:

\begin{verbatim}
torch_tensor(matrix(1:9, ncol = 3))
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  4  7
 2  5  8
 3  6  9
[ CPULongType{3,3} ]
\end{verbatim}

Look at the result. The numbers 1 to 9 appear column after column, just
as in the R matrix we created it from. This may, or may not, be the
intended outcome. If it's not, just pass \texttt{byrow\ =\ TRUE} to the
call to \texttt{matrix()}:

\begin{verbatim}
torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  2  3
 4  5  6
 7  8  9
[ CPULongType{3,3} ]
\end{verbatim}

What about higher-dimensional data? Following the same principle, we can
pass in an array:

\begin{verbatim}
torch_tensor(array(1:24, dim = c(4, 3, 2)))
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,.,.) = 
   1  13
   5  17
   9  21

(2,.,.) = 
   2  14
   6  18
  10  22

(3,.,.) = 
   3  15
   7  19
  11  23

(4,.,.) = 
   4  16
   8  20
  12  24
[ CPULongType{4,3,2} ]
\end{verbatim}

Again, the result follows R's array population logic. If that's not what
you want, it is probably easier to build up the tensor programmatically.

Before you start to panic, though, think about how rarely you'll need to
do this. In practice, you'll mostly be creating tensors from an R
dataset. We'll take a close look at that in the last subsection,
``Tensors from datasets''. Before though, it is instructive to spend a
little time inspecting that last output.

Here, pictorially, is the object we created
(fig.~\ref{fig-tensors-dimensions}). Let's call the axis that extends to
the right \texttt{x}, the one that goes into the page, \texttt{y}, and
the one that points up, \texttt{z}. Then the tensor extends 4, 3, and 2
units, respectively, in the x, y, and z directions.

\begin{figure}[H]

{\centering \includegraphics{images/tensors-dimensions.png}

}

\caption{\label{fig-tensors-dimensions}A 4x3x2 tensor.}

\end{figure}

The array we passed to \texttt{torch\_tensor()} prints like this:

\begin{verbatim}
array(1:24, dim = c(4, 3, 2))
\end{verbatim}

\begin{verbatim}
, , 1

     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12

, , 2

     [,1] [,2] [,3]
[1,]   13   17   21
[2,]   14   18   22
[3,]   15   19   23
[4,]   16   20   24
\end{verbatim}

Compare that with how the tensor prints, above. \texttt{Array} and
\texttt{tensor} slice the object in different ways. The tensor slices
its values into \texttt{3x2} rectangles, extending up and to the back,
one for each of the four \texttt{x}-values. The array, on the other
hand, splits them up by \texttt{z}-value, resulting in two big
\texttt{4x3} slices that go up and to the right.

Alternatively, we could say that the tensor starts thinking from the
left/the ``outside''; the array, from the right/the ``inside''.

\hypertarget{tensors-from-specifications}{%
\subsection{\texorpdfstring{Tensors from
specifications\index{tensors!create from specifications}}{Tensors from specifications}}\label{tensors-from-specifications}}

There are two broad conditions when \texttt{torch}'s bulk creation
functions will come in handy: For one, when you don't care about
individual tensor values, but only about their distribution. Secondly,
if they follow some conventional pattern.

When we use bulk creation functions, instead of individual \emph{values}
we specify the \emph{shape} they should have. Here, for example, we
instantiate a 3x3 tensor, populated with standard-normally distributed
values:

\begin{verbatim}
torch_randn(3, 3)
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.6532  0.6557  2.0251
-0.7914 -1.7220  1.0387
 0.1931  1.0536 -0.2077
[ CPUFloatType{3,3} ]
\end{verbatim}

And here is the equivalent for values that are uniformly distributed
between zero and one:

\begin{verbatim}
torch_rand(3, 3)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.2498  0.5356  0.6515
 0.3556  0.5799  0.1284
 0.9884  0.4361  0.8040
[ CPUFloatType{3,3} ]
\end{verbatim}

Often, we require tensors of all ones, or all zeroes:

\begin{verbatim}
torch_zeros(2, 5)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0  0  0  0  0
 0  0  0  0  0
[ CPUFloatType{2,5} ]
\end{verbatim}

\begin{verbatim}
torch_ones(2, 2)
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  1
 1  1
[ CPUFloatType{2,2} ]
\end{verbatim}

Many more of these bulk creation functions exist. To wrap up, let's see
how to create some matrix types that are common in linear algebra.
Here's an identity matrix:

\begin{verbatim}
torch_eye(n = 5)
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  0  0  0  0
 0  1  0  0  0
 0  0  1  0  0
 0  0  0  1  0
 0  0  0  0  1
[ CPUFloatType{5,5} ]
\end{verbatim}

And here, a diagonal matrix:

\begin{verbatim}
torch_diag(c(1, 2, 3))
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  0  0
 0  2  0
 0  0  3
[ CPUFloatType{3,3} ]
\end{verbatim}

\hypertarget{tensors-from-datasets}{%
\subsection{\texorpdfstring{Tensors from
datasets\index{tensors!create from datasets}}{Tensors from datasets}}\label{tensors-from-datasets}}

Now we look at how to create tensors from R datasets. Depending on the
dataset itself, this process can feel ``automatic'' or require some
thought and action.

First, let's try \texttt{JohnsonJohnson} that comes with base R. It is a
time series of quarterly earnings per Johnson \& Johnson share.

\begin{verbatim}
JohnsonJohnson
\end{verbatim}

\begin{verbatim}
      Qtr1  Qtr2  Qtr3  Qtr4
1960  0.71  0.63  0.85  0.44
1961  0.61  0.69  0.92  0.55
1962  0.72  0.77  0.92  0.60
1963  0.83  0.80  1.00  0.77
1964  0.92  1.00  1.24  1.00
1965  1.16  1.30  1.45  1.25
1966  1.26  1.38  1.86  1.56
1967  1.53  1.59  1.83  1.86
1968  1.53  2.07  2.34  2.25
1969  2.16  2.43  2.70  2.25
1970  2.79  3.42  3.69  3.60
1971  3.60  4.32  4.32  4.05
1972  4.86  5.04  5.04  4.41
1973  5.58  5.85  6.57  5.31
1974  6.03  6.39  6.93  5.85
1975  6.93  7.74  7.83  6.12
1976  7.74  8.91  8.28  6.84
1977  9.54 10.26  9.54  8.73
1978 11.88 12.06 12.15  8.91
1979 14.04 12.96 14.85  9.99
1980 16.20 14.67 16.02 11.61
\end{verbatim}

Can we just pass this to \texttt{torch\_tensor()} and magically get what
we want?

\begin{verbatim}
torch_tensor(JohnsonJohnson)
\end{verbatim}

\begin{verbatim}
torch_tensor
  0.7100
  0.6300
  0.8500
  0.4400
  0.6100
  0.6900
  0.9200
  0.5500
  0.7200
  0.7700
  0.9200
  0.6000
  0.8300
  0.8000
  1.0000
  0.7700
  0.9200
  1.0000
  1.2400
  1.0000
  1.1600
  1.3000
  1.4500
  1.2500
  1.2600
  1.3800
  1.8600
  1.5600
  1.5300
  1.5900
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{84} ]
\end{verbatim}

Looks like we can! The values are arranged exactly the way we want them;
quarter after quarter.

Magic? Not really. \texttt{torch} can only work with what it is given;
and here, what it is given is actually a vector of \texttt{double}s
arranged in quarterly order. The data just print the way they do because
they are of class \texttt{ts}:

\begin{verbatim}
unclass(JohnsonJohnson)
\end{verbatim}

\begin{verbatim}
[1]  0.71  0.63  0.85  0.44  0.61  0.69  0.92  0.55  0.72
[10] 0.77  0.92  0.60  0.83  0.80  1.00  0.77 0.92  1.00
[19] 1.24  1.00  1.16  1.30  1.45  1.25  1.26  1.38  1.86
[28] 1.56  1.53  1.59  1.83  1.86 1.53  2.07  2.34  2.25
[37] 2.16  2.43  2.70  2.25  2.79  3.42  3.69  3.60  3.60
[46] 4.32  4.32  4.05 4.86  5.04  5.04  4.41  5.58  5.85
[55] 6.57  5.31  6.03  6.39  6.93  5.85  6.93  7.74  7.83
[64] 6.12 7.74  8.91  8.28  6.84  9.54 10.26  9.54  8.73
[73] 11.88 12.06 12.15  8.91 14.04 12.96 14.85  9.99 16.20
[82] 14.67 16.02 11.61 
attr(,"tsp")
[1] 1960.00 1980.75    4.00
\end{verbatim}

So this went well. Let's try another one. Who is not kept up at night,
pondering trunk thickness of orange trees?

\begin{verbatim}
library(dplyr)

glimpse(Orange)
\end{verbatim}

\begin{verbatim}
Rows: 35
Columns: 3
$ Tree          <ord> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,...
$ age           <dbl> 118, 484, 664, 1004, 1231, 1372, 1582,...
$ circumference <dbl> 30, 58, 87, 115, 120, 142, 145, 33, 69,...
\end{verbatim}

\begin{verbatim}
torch_tensor(Orange)
\end{verbatim}

\begin{verbatim}
Error in torch_tensor_cpp(data, dtype, device, requires_grad,
pin_memory) : R type not handled
\end{verbatim}

Which type is \emph{not handled} here? It seems obvious that the
``culprit'' must be \texttt{Tree}, an ordered-factor column. Let's first
check if \texttt{torch} can handle factors:

\begin{verbatim}
f <- factor(c("a", "b", "c"), ordered = TRUE)
torch_tensor(f)
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 2
 3
[ CPULongType{3} ]
\end{verbatim}

So this worked fine. Then what else could it be? The problem here is the
containing structure, the \texttt{data.frame}. We need to call
\texttt{as.matrix()} on it first. Due to the presence of the factor,
though, this will result in a matrix of all strings, which is not what
we want. Therefore, we first extract the underlying levels (integers)
from the factor, and then convert the \texttt{data.frame} to a matrix:

\begin{verbatim}
orange_ <- Orange %>% 
  mutate(Tree = as.numeric(Tree)) %>%
  as.matrix()

torch_tensor(orange_) %>% print(n = 7)
\end{verbatim}

\begin{verbatim}
torch_tensor
    2   118    30
    2   484    58
    2   664    87
    2  1004   115
    2  1231   120
    2  1372   142
    2  1582   145
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{35,3} ]
\end{verbatim}

Let's try the same thing with another \texttt{data.frame}, \texttt{okc}
from \texttt{modeldata}:

\begin{verbatim}
library(modeldata)

data(okc)
okc %>% glimpse()
\end{verbatim}

\begin{verbatim}
Rows: 59,855
Columns: 6
$ age      <int> 22, 35, 38, 23, 29, 29, 32, 31, 24,...
$ diet     <chr> "strictly anything", "mostly other",...
$ height   <int> 75, 70, 68, 71, 66, 67, 65, 65, 67, 65,...
$ location <chr> "south san francisco", "oakland",... 
$ date     <date> 2012-06-28, 2012-06-29, 2012-06-27,...
$ Class    <fct> other, other, other, other, other, stem,...
\end{verbatim}

We have two integer columns, which is fine, and one factor column, which
we know how to handle. But what about the \texttt{character} and
\texttt{date} columns? Trying to create a tensor from the \texttt{date}
column individually, we see:

\begin{verbatim}
print(torch_tensor(okc$date), n = 7)
\end{verbatim}

\begin{verbatim}
torch_tensor
 15519
 15520
 15518
 15519
 15518
 15520
 15516
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{59855} ]
\end{verbatim}

This didn't throw an error, but what does it mean? These are the actual
values stored in an R \texttt{Date}, namely, the number of days since
January 1, 1970. Technically, thus, we have a working conversion --
whether the result makes sense pragmatically is a question of how you're
going to use it. Put differently, you'll probably want to further
process these data before using them in a computation, and how you do
this will depend on the context.

Next, let's see about \texttt{location}, one of the columns of type
\texttt{character}. What happens if we just pass it to \texttt{torch}
as-is?

\begin{verbatim}
torch_tensor(okc$location)
\end{verbatim}

\begin{verbatim}
Error in torch_tensor_cpp(data, dtype, device, requires_grad,
pin_memory) : R type not handled
\end{verbatim}

In fact, there are no tensors in \texttt{torch} that store strings. We
have to apply some scheme that converts them to a numeric type first. In
cases like the present one, where every observation contains a single
entity (as opposed to, say, a sentence or a paragraph), the easiest way
of doing this from R is to first convert to \texttt{factor}, then to
\texttt{numeric}, and then, to \texttt{tensor}:

\begin{verbatim}
okc$location %>%
  factor() %>%
  as.numeric() %>%
  torch_tensor() %>%
  print(n = 7)
\end{verbatim}

\begin{verbatim}
torch_tensor
 120
  74
 102
  10
 102
 102
 102
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{59855} ]
\end{verbatim}

True, this works well technically. It \emph{does}, however, reduce
information. For example, the first and third locations are ``south san
francisco'' and ``san francisco'', respectively. Once converted to
factors, these are just as distant, semantically, as are ``san
francisco'' and any other location. Again, whether this is of relevance
depends on the specifics of the data, as well as your goal. If you think
it does matter, you have a range of options, including, for example,
grouping observations by some criterion, or converting to
latitude/longitude. These considerations are by no means
\texttt{torch}-specific; we just mention them here because they affect
the ``data ingestion workflow'' to \texttt{torch}.

Finally, no excursion into the world of real-life data science is
complete without a consideration of \texttt{NA}s. Let's see:

\begin{verbatim}
torch_tensor(c(1, NA, 3))
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
nan
 3
[ CPUFloatType{3} ]
\end{verbatim}

R's \texttt{NA} gets converted to \texttt{NaN}. Can you work with that?
Some \texttt{torch} function can. For example,
\texttt{torch\_nanquantile()} just ignores the \texttt{NaN}s:

\begin{verbatim}
torch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)
\end{verbatim}

\begin{verbatim}
torch_tensor
 2
[ CPUFloatType{1} ]
\end{verbatim}

However, if you're going to train a neural network, for example, you'll
need to think about how to meaningfully replace these missing values
first. But that's a topic for a later time.

\hypertarget{operations-on-tensors}{%
\section{\texorpdfstring{Operations on
tensors\index{tensors!operations on}}{Operations on tensors}}\label{operations-on-tensors}}

We can perform all the usual mathematical operations on tensors.: add,
subtract, divide \ldots{} These operations are available as functions
(starting with \texttt{torch\_}) as well as as methods on objects
(invoked with \texttt{\$}-syntax). For example, the following are
equivalent:

\begin{verbatim}
t1 <- torch_tensor(c(1, 2))
t2 <- torch_tensor(c(3, 4))

torch_add(t1, t2)
# equivalently
t1$add(t2)
\end{verbatim}

\begin{verbatim}
torch_tensor
 4
 6
[ CPUFloatType{2} ]
\end{verbatim}

In both cases, a new object is created; neither \texttt{t1} nor
\texttt{t2} are modified. There exists an alternate method that modifies
its object in-place:

\begin{verbatim}
t1$add_(t2)
\end{verbatim}

\begin{verbatim}
torch_tensor
 4
 6
[ CPUFloatType{2} ]
\end{verbatim}

\begin{verbatim}
t1
\end{verbatim}

\begin{verbatim}
torch_tensor
 4
 6
[ CPUFloatType{2} ]
\end{verbatim}

In fact, the same pattern applies for other operations: Whenever you see
an underscore appended, the object is modified in-place.

Naturally, in a scientific-computing setting, matrix operations are of
special interest. Let's start with the dot product of two
one-dimensional structures, i.e., vectors.

\begin{verbatim}
t1 <- torch_tensor(1:3)
t2 <- torch_tensor(4:6)
t1$dot(t2)
\end{verbatim}

\begin{verbatim}
torch_tensor
32
[ CPULongType{} ]
\end{verbatim}

Were you thinking this shouldn't work? Should we have needed to
transpose (\texttt{torch\_t()}) one of the tensors? In fact, this also
works:

\begin{verbatim}
t1$t()$dot(t2)
\end{verbatim}

\begin{verbatim}
torch_tensor
32
[ CPULongType{} ]
\end{verbatim}

The reason the first call worked, too, is that \texttt{torch} does not
distinguish between row vectors and column vectors. In consequence, if
we multiply a vector with a matrix, using \texttt{torch\_matmul()}, we
don't need to worry about the vector's orientation either:

\begin{verbatim}
t3 <- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))
t3$matmul(t1)
\end{verbatim}

\begin{verbatim}
torch_tensor
 14
 32
 50
 68
[ CPULongType{4} ]
\end{verbatim}

The same function, \texttt{torch\_matmul()}, would be used to multiply
two matrices. Note how this is different from what
\texttt{torch\_multiply()} does, namely, scalar-multiply its arguments:

\begin{verbatim}
torch_multiply(t1, t2)
\end{verbatim}

\begin{verbatim}
torch_tensor
  4
 10
 18
[ CPULongType{3} ]
\end{verbatim}

Many more tensor operations exist, some of which you'll meet over the
course of this journey. But there is one group that deserves special
mention.

\hypertarget{summary-operations}{%
\subsection{Summary operations}\label{summary-operations}}

If you have an R matrix and are about to compute a sum, this could,
normally, mean one of three things: the global sum, row sums, or column
sums. Let's see all three of them at work (using \texttt{apply()} for a
reason):

\begin{verbatim}
m <- outer(1:3, 1:6)

sum(m)
apply(m, 1, sum)
apply(m, 2, sum)
\end{verbatim}

\begin{verbatim}
[1] 126
[1]  21 42 63
[1]   6 12 18 24 30 36
\end{verbatim}

And now, the \texttt{torch} equivalents. We start with the overall sum.

\begin{verbatim}
t <- torch_outer(torch_tensor(1:3), torch_tensor(1:6))
t$sum()
\end{verbatim}

\begin{verbatim}
torch_tensor
126
[ CPULongType{} ]
\end{verbatim}

It gets more interesting for the row and column sums. The \texttt{dim}
argument tells \texttt{torch} which dimension(s) to sum over. Passing in
\texttt{dim\ =\ 1}, we see:

\begin{verbatim}
t$sum(dim = 1)
\end{verbatim}

\begin{verbatim}
torch_tensor
  6
 12
 18
 24
 30
 36
[ CPULongType{6} ]
\end{verbatim}

Unexpectedly, these are the column sums! Before drawing conclusions,
let's check what happens with \texttt{dim\ =\ 2}:

\begin{verbatim}
t$sum(dim = 2)
\end{verbatim}

\begin{verbatim}
torch_tensor
 21
 42
 63
[ CPULongType{3} ]
\end{verbatim}

Now, we have sums over rows. Did we misunderstand something about how
\texttt{torch} orders dimensions? No, it's not that. In \texttt{torch},
when we're in two dimensions, we think rows first, columns second. (And
as you'll see in a minute, we start indexing with 1, just as in R in
general.)

Instead, the conceptual difference is specific to aggregating, or
``grouping'', operations. In R, \emph{grouping}, in fact, nicely
characterizes what we have in mind: We group by row (dimension 1) for
row summaries, by column (dimension 2) for column summaries. In
\texttt{torch}, the thinking is different: We \emph{collapse} the
columns (dimension 2) to compute row summaries, the rows (dimension 1)
for column summaries.

The same thinking applies in higher dimensions. Assume, for example,
that we been recording time series data for four individuals. There are
two features, and both of them have been measured at three times. If we
were planning to train a recurrent neural network (much more on that
later), we would arrange the measurements like so:

\begin{itemize}
\item
  Dimension 1: Runs over individuals.
\item
  Dimension 2: Runs over points in time.
\item
  Dimension 3: Runs over features.
\end{itemize}

The tensor then would look like this:

\begin{verbatim}
t <- torch_randn(4, 3, 2)
t
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,.,.) = 
 -1.3427  1.1303
  1.0430  0.8232
  0.7952 -0.2447

(2,.,.) = 
 -1.9929  0.1251
  0.4143  0.3523
  0.9819  0.3219

(3,.,.) = 
  0.6389 -0.2606
  2.4011  0.2656
 -0.1750 -0.2597

(4,.,.) = 
  1.4534  0.7229
  1.2503 -0.2975
  1.6749 -1.2154
[ CPUFloatType{4,3,2} ]
\end{verbatim}

To obtain feature averages, independently of subject and time, we would
collapse dimensions 1 and 2:

\begin{verbatim}
t$mean(dim = c(1, 2))
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.1600
 0.1363
[ CPUFloatType{2} ]
\end{verbatim}

If, on the other hand, we wanted feature averages, but individually per
person, we'd do:

\begin{verbatim}
t$mean(dim = 2)
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.6153  0.8290
 0.3961  0.2739
-0.0579  0.1966
-0.3628 -0.7544
[ CPUFloatType{4,2} ]
\end{verbatim}

Here, the single feature ``collapsed'' is the time step.

\hypertarget{accessing-parts-of-a-tensor}{%
\section{\texorpdfstring{Accessing parts of a
tensor\index{tensors!index into}
\index{tensors!slice}}{Accessing parts of a tensor }}\label{accessing-parts-of-a-tensor}}

Often, when working with tensors, some computational step is meant to
operate on just part of its input tensor. When that part is a single
entity (value, row, column \ldots), we commonly refer to this as
\emph{indexing}; when it's a range of such entities, it is called
\emph{slicing}.

\hypertarget{think-r}{%
\subsection{``Think R''}\label{think-r}}

Both indexing and slicing work essentially as in R. There are a few
syntactic extensions, and I'll present these in the subsequent section.
But overall you should find the behavior intuitive.

This is because just as in R, indexing in \texttt{torch} is one-based.
And just as in R, singleton dimensions are dropped.

In the below example, we ask for the first column of a two-dimensional
tensor; the result is one-dimensional, i.e., a vector:

\begin{verbatim}
t <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
t[1, ]
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 2
 3
[ CPULongType{3} ]
\end{verbatim}

If we specify \texttt{drop\ =\ FALSE,} though, dimensionality is
preserved:

\begin{verbatim}
t[1, , drop = FALSE]
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  2  3
[ CPULongType{1,3} ]
\end{verbatim}

When slicing, there are no singleton dimensions -- and thus, no
additional considerations to be taken into account:

\begin{verbatim}
t <- torch_rand(3, 3, 3)
t[1:2, 2:3, c(1, 3)]
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,.,.) = 
  0.5273  0.3781
  0.5303  0.9537

(2,.,.) = 
  0.2966  0.7160
  0.5421  0.4284
[ CPUFloatType{2,2,2} ]
\end{verbatim}

In sum, thus, indexing and slicing work very much like in R. Now, let's
look at the aforementioned extensions that further enhance usability.

\hypertarget{beyond-r}{%
\subsubsection{Beyond R}\label{beyond-r}}

One of these extensions concerns accessing the last element in a tensor.
Conveniently, in \texttt{torch}, we can use \texttt{-1} to accomplish
that:

\begin{verbatim}
t <- torch_tensor(matrix(1:4, ncol = 2, byrow = TRUE))
t[-1, -1]
\end{verbatim}

\begin{verbatim}
torch_tensor
4
[ CPULongType{} ]
\end{verbatim}

Note how in R, negative indices have a quite different effect, causing
elements at respective positions to be removed.

Another useful feature extends slicing syntax to allow for a step
pattern, to be specified after a second colon. Here, we request values
from every second column between columns one and eight:

\begin{verbatim}
t <- torch_tensor(matrix(1:20, ncol = 10, byrow = TRUE))
t[ , 1:8:2]
\end{verbatim}

\begin{verbatim}
torch_tensor
  1   3   5   7
 11  13  15  17
[ CPULongType{2,4} ]
\end{verbatim}

Finally, sometimes the same code should be able to work with tensors of
different dimensionalities. In this case, we can use \texttt{..} to
collectively designate any existing dimensions not explicitly
referenced.

For example, say we want to index into the first dimension of whatever
tensor is passed, be it a matrix, an array, or some higher-dimensional
structure. The following

\begin{verbatim}
t[1, ..]
\end{verbatim}

will work for all:

\begin{verbatim}
t1 <- torch_randn(2, 2)
t2 <- torch_randn(2, 2, 2)
t3 <- torch_randn(2, 2, 2, 2)
t1[1, ..]
t2[1, ..]
t3[1, ..]
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.6179
-1.4769
[ CPUFloatType{2} ]


torch_tensor
 1.0602 -0.9028
 0.2942  0.4611
[ CPUFloatType{2,2} ]


torch_tensor
(1,.,.) = 
  1.3304 -0.6018
  0.0825  0.1221

(2,.,.) = 
  1.7129  1.2932
  0.2371  0.9041
[ CPUFloatType{2,2,2} ]
\end{verbatim}

If we wanted to index into the last dimension instead, we'd write
\texttt{t{[}..,\ 1{]}}. We can even combine both:

\begin{verbatim}
t3[1, .., 2]
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.6018  0.1221
 1.2932  0.9041
[ CPUFloatType{2,2} ]
\end{verbatim}

Now, a topic just as important as indexing and slicing is reshaping of
tensors.

\hypertarget{reshaping-tensors}{%
\section{\texorpdfstring{Reshaping
tensors\index{tensors!reshape}}{Reshaping tensors}}\label{reshaping-tensors}}

Say you have a tensor with twenty-four elements. What is its shape? It
could be any of the following:

\begin{itemize}
\item
  a vector of length 24
\item
  a matrix of shape 24 x 1, or 12 x 2, or 6 x 4, or \ldots{}
\item
  a three-dimensional array of size 24 x 1 x 1, or 12 x 2 x 1, or
  \ldots{}
\item
  and so on (in fact, it could even have shape 24 x 1 x 1 x 1 x 1)
\end{itemize}

We can modify a tensor's shape, without juggling around its values,
using the \texttt{view()} method. Here is the initial tensor, a vector
of length 24:

\begin{verbatim}
t <- torch_zeros(24)
print(t, n = 3)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0
 0
 0
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{24} ]
\end{verbatim}

Here is that same vector, reshaped to a wide matrix:

\begin{verbatim}
t2 <- t$view(c(2, 12))
t2
\end{verbatim}

\begin{verbatim}
torch_tensor
 0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0
[ CPUFloatType{2,12} ]
\end{verbatim}

So we have a new tensor, \texttt{t2}, but interestingly (and
importantly, performance-wise), \texttt{torch} did not have to allocate
any new storage for its values. This we can verify for ourselves. Both
tensors store their data in the same location:

\begin{verbatim}
t$storage()$data_ptr()
t2$storage()$data_ptr()
\end{verbatim}

\begin{verbatim}
[1] "0x55cd15789180"
[1] "0x55cd15789180"
\end{verbatim}

Let's talk a bit about how this is possible.

\hypertarget{zero-copy-reshaping-vs.-reshaping-with-copy}{%
\subsection{Zero-copy reshaping vs.~reshaping with
copy}\label{zero-copy-reshaping-vs.-reshaping-with-copy}}

Whenever we ask \texttt{torch} to perform an operation that changes the
shape of a tensor, it tries to fulfill the request without allocating
new storage for the tensor's contents. This is possible because the same
data -- the same bytes, ultimately -- can be read in different ways. All
that is needed is storage for the \emph{metadata}.

How does \texttt{torch} do it? Let's see a concrete example. We start
with a 3 x 5 matrix.

\begin{verbatim}
t <- torch_tensor(matrix(1:15, nrow = 3, byrow = TRUE))
t
\end{verbatim}

\begin{verbatim}
 torch_tensor
  1   2   3   4   5
  6   7   8   9  10
 11  12  13  14  15
[ CPULongType{3,5} ]
\end{verbatim}

Tensors have a \texttt{stride()} method that tracks, \emph{for every
dimension}, how many elements have to be traversed to arrive at its next
element. For the above tensor \texttt{t}, to go to the next row, we have
to skip over five elements, while to go to the next column, we need to
skip just one:

\begin{verbatim}
t$stride()
\end{verbatim}

\begin{verbatim}
[1] 5 1
\end{verbatim}

Now we reshape the tensor so it has five rows and three columns instead.
Remember, the data themselves do not change.

\begin{verbatim}
t2 <- t$view(c(5, 3))
t2
\end{verbatim}

\begin{verbatim}
torch_tensor
  1   2   3
  4   5   6
  7   8   9
 10  11  12
 13  14  15
[ CPULongType{5,3} ]
\end{verbatim}

This time, to arrive at the next row, we just skip three elements
instead of five. To get to the next column, we still just ``jump over''
a single element only:

\begin{verbatim}
t2$stride()
\end{verbatim}

\begin{verbatim}
[1] 3 1
\end{verbatim}

Now you may be thinking, what if the order of the elements also has to
change? For example, in matrix transposition. Is that still doable with
the metadata-only approach?

\begin{verbatim}
t3 <- t$t()
t3
\end{verbatim}

\begin{verbatim}
torch_tensor
  1   6  11
  2   7  12
  3   8  13
  4   9  14
  5  10  15
[ CPULongType{5,3} ]
\end{verbatim}

In fact, it must be, as both the original tensor and its transpose point
to the same place in memory:

\begin{verbatim}
t$storage()$data_ptr()
t3$storage()$data_ptr()
\end{verbatim}

\begin{verbatim}
[1] "0x55cd1cd4a840"
[1] "0x55cd1cd4a840"
\end{verbatim}

And it makes sense: This will work if we know that to arrive at the next
row, we just skip a single element, while to arrive at the next column,
that's five to skip over now. Let's verify:

\begin{verbatim}
t3$stride()
\end{verbatim}

\begin{verbatim}
[1] 1 5
\end{verbatim}

Exactly.

Whenever possible, \texttt{torch} will try to handle shape-changing
operations in this way.

Another such \emph{zero-copy} operation (and one we'll see a lot) is
\texttt{squeeze()}, together with its antagonist, \texttt{unsqueeze()}.
The latter adds a singleton dimension at the requested position, the
former removes it. For example:

\begin{verbatim}
t <- torch_randn(3)
t

t$unsqueeze(1)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.2291
-0.9454
 1.6630
[ CPUFloatType{3} ]

torch_tensor
 0.2291 -0.9454  1.6630
[ CPUFloatType{1,3} ]
\end{verbatim}

Here we added a singleton dimension in front. Alternatively, we could
have used \texttt{t\$unsqueeze(2)} to add it at the end.

Now, will that zero-copy technique ever fail? Here is an example where
it does:

\begin{verbatim}
t <- torch_randn(3, 3)
t$t()$view(9)
\end{verbatim}

\begin{verbatim}
 Error in (function (self, size)  : 
  view size is not compatible with input tensor's size and
  stride (at least one dimension spans across two contiguous
  subspaces). Use .reshape(...) instead. [...]
\end{verbatim}

When two operations that change the stride are executed in sequence, the
second is pretty likely to fail. There is a way to exactly determine
whether it will fail or not; but the easiest way is to just use a
different method instead of \texttt{view()}: \texttt{reshape()}. The
latter will ``automagically'' work metadata-only if that is possible,
but make a copy if not:

\begin{verbatim}
t <- torch_randn(3, 3)
t2 <- t$t()$reshape(9)

t$storage()$data_ptr()
t2$storage()$data_ptr()
\end{verbatim}

\begin{verbatim}
[1] "0x55cd1622a000"
[1] "0x55cd19d31e40"
\end{verbatim}

As expected, both tensors are now stored in different locations.

Finally, we are going to end this long chapter with a feature that may
seem overwhelming at first, but is of tremendous importance
performance-wise. Like with so many things, it takes time to get
accustomed to, but rest assured: You'll encounter it again and again, in
this book and in many projects using \texttt{torch}. It is called
\emph{broadcasting}.

\hypertarget{broadcasting}{%
\section{\texorpdfstring{Broadcasting\index{tensors!broadcasting}}{Broadcasting}}\label{broadcasting}}

We often have to perform operations on tensors with shapes that don't
match exactly.

Of course, we wouldn't probably try to add, say, a length-two vector to
a length-five vector. But there are things we \emph{may} want to do: for
example, multiply every element by a scalar. This works:

\begin{verbatim}
t1 <- torch_randn(3, 5)
t1 * 0.5
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.4845  0.3092 -0.3710  0.3558 -0.2126
-0.3419  0.1160  0.1800 -0.0094 -0.0189
-0.0468 -0.4030 -0.3172 -0.1558 -0.6247
[ CPUFloatType{3,5} ]
\end{verbatim}

That was probably a bit underwhelming. We're used to that; from R. But
the following does not work in R. The intention here would be to add the
same vector to every row in a matrix:

\begin{verbatim}
m <- matrix(1:15, ncol = 5, byrow = TRUE)
m2 <- matrix(1:5, ncol = 5, byrow = TRUE)

m + m2
\end{verbatim}

\begin{verbatim}
Error in m + m2 : non-conformable arrays
\end{verbatim}

Neither does it help if we make \texttt{m2} a vector.

\begin{verbatim}
m3 <- 1:5

m + m3
\end{verbatim}

\begin{verbatim}
     [,1] [,2] [,3] [,4] [,5]
[1,]    2    6    5    9    8
[2,]    8   12   11   10   14
[3,]   14   13   17   16   20
\end{verbatim}

Syntactically this worked, but semantics-wise this is not what we
intended.

Now, we try both of the above with \texttt{torch}. First, again, the
scenario where both tensors are two-dimensional (even though,
conceptually, one of them is a row vector):

\begin{verbatim}
t <- torch_tensor(m)
t2 <- torch_tensor(m2)

t$shape
t2$shape

t$add(t2)
\end{verbatim}

{[}1{]} 3 5

{[}1{]} 1 5

\begin{verbatim}
torch_tensor
  2   4   6   8  10
  7   9  11  13  15
 12  14  16  18  20
[ CPULongType{3,5} ]
\end{verbatim}

And now, with the thing to be added a one-dimensional tensor:

\begin{verbatim}
t3 <- torch_tensor(m3)

t3$shape

t$add(t3)
\end{verbatim}

\begin{verbatim}
[1] 5
torch_tensor
  2   4   6   8  10
  7   9  11  13  15
 12  14  16  18  20
[ CPULongType{3,5} ]
\end{verbatim}

In \texttt{torch}, both ways worked as intended. Let's see why.

Above, I've printed the tensor shapes for a reason. To a tensor of shape
3 x 5, we were able to add both a tensor of shape 3 and a tensor of
shape 1 x 5. Together, these illustrate how broadcasting works. In a
nutshell, this is what happens:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The 1 x 5 tensor, when used as an addend, is virtually expanded, that
  is, treated as if it contained the same row three times. This kind of
  expansion can only be performed if the non-matching dimension is a
  singleton, and if it is located on the left.
\item
  The same thing happens to the shape-3 tensor, but there is one
  additional step that takes place first: A leading dimension of size 1
  is -- virtually -- appended on the left. This puts us in exactly the
  same state we were in in (1), and we continue from there.
\end{enumerate}

Importantly, no physical expansions take place.

Let's systematize these rules.

\hypertarget{broadcasting-rules}{%
\subsection{Broadcasting rules}\label{broadcasting-rules}}

The rules are the following. The first, unspectactular though it may
look, is the basis for everything else.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  We align tensor shapes, \emph{starting from the right}.
\end{enumerate}

Say we have two tensors, one of size 3 x 7 x 1, the other of size 1 x 5.
Here they are, right-aligned:

\begin{verbatim}
# t1, shape:        3  7  1
# t2, shape:           1  5
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \emph{Starting from the right}, the sizes along aligned axes either
  have to match exactly, or one of them has to be equal to 1. In the
  latter case, the singleton-dimension tensor is \emph{broadcast} to the
  non-singleton one.
\end{enumerate}

In the above example, broadcasting happens twice -- once for each
tensor. This (virtually) yields

\begin{verbatim}
# t1, shape:        3  7  5
# t2, shape:           7  5
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  If, on the left, one of the tensors has an additional axis (or more
  than one), the other is virtually expanded to have a dimension of size
  1 in that place, in which case broadcasting will occur as stated in
  (2).
\end{enumerate}

In our example, this happens to the second tensor. First, there is a
virtual expansion

\begin{verbatim}
# t1, shape:        3  7  5
# t2, shape:        1  7  5
\end{verbatim}

and then, broadcasting takes place:

\begin{verbatim}
# t1, shape:        3  7  5
# t2, shape:        3  7  5
\end{verbatim}

In this example, we see that broadcasting can act on both tensors at the
same time. The thing to keep in mind, though, is that we always start
looking from the right. For example, no broadcasting in the world could
make \emph{this} work:

\begin{verbatim}
torch_zeros(4, 3, 2, 1)$add(torch_ones(4, 3, 2)) # error!
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Now, that was one of the longest, and least applied-seeming, perhaps,
chapters in the book. But feeling comfortable with tensors is, I dare
say, a precondition for being fluent in \texttt{torch}. The same goes
for the topic covered in the next chapter, automatic differentiation.
But the difference is, there \texttt{torch} does \emph{all} the heavy
lifting for us. We just need to understand what it's doing.

\hypertarget{sec:autograd}{%
\chapter{Autograd}\label{sec:autograd}}

In the last chapter, we have seen how to manipulate tensors, and
encountered a sample of mathematical operations one can perform on them.
If those operations, numerous though they may be, were all there was to
core \texttt{torch}, you would not be reading this book. Frameworks like
\texttt{torch} are so popular because of what you can do with them: deep
learning, machine learning, optimization, large-scale scientific
computation in general. Most of these application areas involve
minimizing some \emph{loss function}. This, in turn, entails computing
function \emph{derivatives}. Now imagine that as a user, you had to
specify the functional form of each derivative yourself. Especially with
neural networks, this could get cumbersome pretty fast!

In fact, \texttt{torch} does not work out, nor store, functional
representations of derivatives either. Instead, it implements what is
called \emph{automatic
differentiation}\index{automatic differentiation}. In automatic
differentiation, and more specifically, its often-used
\emph{reverse-mode} variant, derivatives are computed and combined on a
\emph{backward pass} through the graph of tensor operations. We'll look
at an example for this in a minute. But first, let's quickly backtrack
and talk about \emph{why} we would want to compute derivatives.

\hypertarget{why-compute-derivatives}{%
\section{Why compute derivatives?}\label{why-compute-derivatives}}

In supervised machine learning, we have at our disposal a \emph{training
set}, where the variable we're hoping to predict is known. This is the
target, or \emph{ground truth}. We now develop and train a prediction
algorithm, based on a set of input variables, the \emph{predictors}.
This training, or learning, process, is based on comparing the
algorithm's predictions with the ground truth, a comparison that leads
to a number capturing how good or bad the current predictions are. To
provide this number is the job of the \emph{loss
function}\index{loss function}.

Once it is aware of the current loss, an algorithm can adjust its
parameters -- the \emph{weights}, in a neural network -- in order to
deliver better predictions. It just has to know in which direction to
adjust them. This information is made available by the
\emph{gradient}\index{gradient}, the vector of
derivatives\index{derivative(s)}.

As an example, we imagine a loss function that looks like this
(fig.~\ref{fig-autograd-paraboloid}):

\begin{figure}[H]

{\centering \includegraphics{images/autograd-paraboloid.png}

}

\caption{\label{fig-autograd-paraboloid}Hypothetical loss function (a
paraboloid).}

\end{figure}

This is a quadratic function of two variables:
\(f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5\). It has its minimum at
\texttt{(0,0)}, and this is the point we'd like to be at. As humans,
standing at the location designated by the white dot, and looking at the
landscape, we have a pretty clear idea how to go downhill fast (assuming
we're not scared by the slope). To find the best direction
computationally, however, we compute the gradient.

Take the \(x_1\) direction. The derivative of the function with respect
to \(x_1\) indicates how its value varies as \(x_1\) varies. As we know
the function in closed form, we can compute that:
\(\frac{\partial f}{\partial x_1} = 0.4 x_1\). This tells us that as
\(x_1\) increases, loss increases, and how fast. But we want loss to
\emph{decrease}, so we have to go in the opposite direction.

The same holds for the \(x_2\)-axis. We compute the derivative
(\(\frac{\partial f}{\partial x_2} = 0.4 x_2\)). Again, we want to take
the direction opposite to where the derivative points. Overall, this
yields a descent direction of
\(\begin{bmatrix}-0.4x_1\\-0.4x_2 \end{bmatrix}\).

Descriptively, this strategy is called \emph{steepest
descent}\index{descent!steepest}. Commonly referred to as \emph{gradient
descent}\index{descent!gradient}, it is the most basic optimization
algorithm in deep learning. Perhaps unintuitively, it is not always the
most efficient way. And there's another question: Can we assume that
this direction, computed at the starting point, will remain optimal as
we continue descending? Maybe we'd better regularly recompute directions
instead? Questions like this will be addressed in later chapters.

\hypertarget{automatic-differentiation-example}{%
\section{Automatic differentiation
example}\label{automatic-differentiation-example}}

Now that we know why we need derivatives, let's see how automatic
differentiation (AD) would compute them.

This (fig.~\ref{fig-autograd-compgraph}) is how our above function could
be represented in a computational graph. \texttt{x1} and \texttt{x2} are
input nodes, corresponding to function parameters \(x_1\) and \(x_2\).
\texttt{x7} is the function's output; all other nodes are intermediate
ones, necessary to ensure correct order of execution. (We could have
given the constants, \texttt{-5} , \texttt{0.2}, and \texttt{2}, their
own nodes as well; but as they're remaining, well, constant anyway,
we're not too interested in them and prefer having a simpler graph.)

\begin{figure}[H]

{\centering \includegraphics{images/autograd-compgraph.png}

}

\caption{\label{fig-autograd-compgraph}Example of a computational
graph.}

\end{figure}

In reverse-mode AD, the flavor of automatic differentiation implemented
by \texttt{torch}, the first thing that happens is to calculate the
function's output value. This corresponds to a forward pass through the
graph. Then, a backward pass is performed to calculate the gradient of
the output with respect to both inputs, \texttt{x1} and \texttt{x2}. In
this process, information becomes available, and is built up, from the
right:

\begin{itemize}
\item
  At \texttt{x7}, we calculate partial derivatives with respect to
  \texttt{x5} and \texttt{x6}. Basically, the equation to differentiate
  looks like this: \(f(x_5, x_6) = x_5 + x_6 - 5\). Thus, both partial
  derivatives are 1.
\item
  From \texttt{x5}, we move to the left to see how it depends on
  \texttt{x3}. We find that \(\frac{\partial x_5}{\partial x_3} = 0.2\).
  At this point, applying the chain rule of calculus, we already know
  how the output depends on \texttt{x3}:
  \(\frac{\partial f}{\partial x_3} = 0.2 * 1 = 0.2\).
\item
  From \texttt{x3}, we take the final step to \texttt{x}. We learn that
  \(\frac{\partial x_3}{\partial x_1} = 2 x_1\). Now, we again apply the
  chain rule, and are able to formulate how the function depends on its
  first input:
  \(\frac{\partial f}{\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1\).
\item
  Analogously, we determine the second partial derivative, and thus,
  already have the gradient available:
  \(\nabla f = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} = 0.4 x_1 + 0.4 x_2\).
\end{itemize}

That is the principle. In practice, different frameworks implement
reverse-mode automatic differentiation differently. We'll catch a
glimpse of how \texttt{torch} does it in the next section.

\hypertarget{automatic-differentiation-with-torch-autograd}{%
\section{\texorpdfstring{Automatic differentiation with \texttt{torch}
\emph{autograd}}{Automatic differentiation with torch autograd}}\label{automatic-differentiation-with-torch-autograd}}

First, a quick note on terminology. In \texttt{torch}, the AD engine is
usually referred to as \emph{autograd}, and that is the way you'll see
it denoted in most of the rest of this book. Now, back to the task.

To construct the above computational graph with \texttt{torch}, we
create ``source'' tensors \texttt{x1} and \texttt{x2}. These will mimic
the parameters whose impact we're interested in. However, if we just
proceed ``as usual'', creating the tensors the way we've been doing so
far, \texttt{torch} will not prepare for AD. Instead, we need to pass in
\texttt{requires\_grad\ =\ TRUE} when instantiating those tensors:

\begin{verbatim}
library(torch)

x1 <- torch_tensor(2, requires_grad = TRUE)
x2 <- torch_tensor(2, requires_grad = TRUE)
\end{verbatim}

(By the way, the value \texttt{2} for both tensors was chosen completely
arbitrarily.)

Now, to create ``invisible'' nodes \texttt{x3} to \texttt{x6} , we
square and multiply accordingly. Then \texttt{x7} stores the final
result.

\begin{verbatim}
x3 <- x1$square()
x5 <- x3 * 0.2

x4 <- x2$square()
x6 <- x4 * 0.2

x7 <- x5 + x6 - 5
x7
\end{verbatim}

\begin{verbatim}
torch_tensor
-3.4000
[ CPUFloatType{1} ][ grad_fn = <SubBackward1> ]
\end{verbatim}

Note that we have to add \texttt{requires\_grad\ =\ TRUE} when creating
the ``source'' tensors only. All dependent nodes in the graph inherit
this property. For example:

\begin{verbatim}
x7$requires_grad
\end{verbatim}

\begin{verbatim}
[1] TRUE
\end{verbatim}

Now, all prerequisites are fulfilled to see automatic differentiation at
work. All we need to do to determine how \texttt{x7} depends on
\texttt{x1} and \texttt{x2} is call \texttt{backward()}:

\begin{verbatim}
x7$backward()
\end{verbatim}

Due to this call, the \texttt{\$grad} fields have been populated in
\texttt{x1} and \texttt{x2}:

\begin{verbatim}
x1$grad
x2$grad
\end{verbatim}

\begin{verbatim}
 0.8000
[ CPUFloatType{1} ]
torch_tensor
 0.8000
[ CPUFloatType{1} ]
\end{verbatim}

These are the partial derivatives of \texttt{x7} with respect to
\texttt{x1} and \texttt{x2}, respectively. Conforming to our manual
calculations above, both amount to 0.8, that is, 0.4 times the tensor
values 2 and 2.

How about the accumulation process we said was needed to build up those
end-to-end derivatives? Can we ``follow'' the end-to-end derivative as
it's being built up? For example, can we see how the final output
depends on \texttt{x3}?

\begin{verbatim}
x3$grad
\end{verbatim}

\begin{verbatim}
[W TensorBody.h:470] Warning: The .grad attribute of a Tensor
that is not a leaf Tensor is being accessed. Its .grad attribute
won't be populated during autograd.backward().
If you indeed want the .grad field to be populated for a 
non-leaf Tensor, use .retain_grad() on the non-leaf Tensor.[...]

torch_tensor
[ Tensor (undefined) ]
\end{verbatim}

The field does not seem to be populated. In fact, while it \emph{has} to
compute them, \texttt{torch} throws away the intermediate aggregates
once they are no longer needed, to save memory. We can, however, ask it
to keep them, using \texttt{retain\_grad\ =\ TRUE}:

\begin{verbatim}
x3 <- x1$square()
x3$retain_grad()

x5 <- x3 * 0.2
x5$retain_grad()

x4 <- x2$square()
x4$retain_grad()

x6 <- x4 * 0.2
x6$retain_grad()

x7 <- x5 + x6 - 5
x7$backward()
\end{verbatim}

Now, we find that \texttt{x3}'s \texttt{grad} field \emph{is} populated:

\begin{verbatim}
x3$grad
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.2000
[ CPUFloatType{1} ]
\end{verbatim}

The same goes for \texttt{x4}, \texttt{x5}, and \texttt{x6}:

\begin{verbatim}
x4$grad
x5$grad
x6$grad
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.2000
[ CPUFloatType{1} ]
torch_tensor
 1
[ CPUFloatType{1} ]
torch_tensor
 1
[ CPUFloatType{1} ]
\end{verbatim}

There is one remaining thing we might be curious about. We've managed to
catch a glimpse of the gradient-accumulation process from the ``running
gradient'' point of view, in a sense; but how about the individual
derivatives that need to be taken in order to proceed with accumulation?
For example, what \texttt{x3\$grad} tells us is how the output depends
on the intermediate state at \texttt{x3}; how do we get from there to
\texttt{x1}, the actual input node?

It turns out that of that aspect, too, we can get an idea. During the
forward pass, \texttt{torch} already takes a note on what it will have
to do, later, to calculate the individual derivatives. This ``recipe''
is stored in a tensor's \texttt{grad\_fn} field. For \texttt{x3}, this
adds the ``missing link'' to \texttt{x1}:

\begin{verbatim}
x3$grad_fn
\end{verbatim}

\begin{verbatim}
PowBackward0
\end{verbatim}

The same works for \texttt{x4}, \texttt{x5}, and \texttt{x6}:

\begin{verbatim}
x4$grad_fn
x5$grad_fn
x6$grad_fn
\end{verbatim}

\begin{verbatim}
PowBackward0
MulBackward1
MulBackward1
\end{verbatim}

And there we are! We've seen how \texttt{torch} computes derivatives for
us, and we've even caught a glimpse of how it does it. Now, we are ready
to play around with our first two applied tasks.

\hypertarget{sec:optim-1}{%
\chapter{\texorpdfstring{Function minimization with
\emph{autograd}}{Function minimization with autograd}}\label{sec:optim-1}}

In the last two chapters, we've learned about tensors and automatic
differentiation. In the upcoming two, we take a break from studying
\texttt{torch} mechanics and, instead, find out what we're able to do
with what we already have. Using nothing but tensors, and supported by
nothing but \emph{autograd}, we can already do two things:

\begin{itemize}
\item
  minimize a function (i.e., perform numerical optimization), and
\item
  build and train a neural network.
\end{itemize}

In this chapter, we start with minimization, and leave the network to
the next one.

\hypertarget{an-optimization-classic}{%
\section{An optimization classic}\label{an-optimization-classic}}

In optimization\index{optimization} research, the \emph{Rosenbrock
function} is a classic. It is a function of two variables; its minimum
is at \texttt{(1,1)}. If you take a look at its contours, you see that
the minimum lies inside a stretched-out, narrow valley
(fig.~\ref{fig-optim-1-rosenbrock}):

\begin{figure}[H]

{\centering \includegraphics{images/optim-1-rosenbrock.png}

}

\caption{\label{fig-optim-1-rosenbrock}Rosenbrock function.}

\end{figure}

Here is the function definition. \texttt{a} and \texttt{b} are
parameters that can be freely chosen; the values we use here are a
frequent choice.

\begin{verbatim}
a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
\end{verbatim}

\hypertarget{minimization-from-scratch}{%
\section{Minimization from scratch}\label{minimization-from-scratch}}

The scenario is the following. We start at some given point
\texttt{(x1,x2)}, and set out to find the location where the Rosenbrock
function has its minimum.

We follow the strategy outlined in the previous chapter: compute the
function's gradient at our current position, and use it to go the
opposite way. We don't know how far to go; if we take too big a big step
we may easily overshoot. (If you look back at the contour plot, you see
that if you were standing at one of the steep cliffs east or west of the
minimum, this could happen very fast.)

Thus, it is best to proceed iteratively, taking moderate steps and
re-evaluating the gradient every time.

In a nutshell, the optimization procedure then looks somewhat like this:

\begin{verbatim}
library(torch)

# attention: this is not the correct procedure yet!

for (i in 1:num_iterations) {

  # call function, passing in current parameter value
  value <- rosenbrock(x)

  # compute gradient of value w.r.t. parameter
  value$backward()

  # manually update parameter, subtracting a fraction
  # of the gradient
  # this is not quite correct yet!
  x$sub_(lr * x$grad)
}
\end{verbatim}

As written, this code snippet demonstrates our intentions, but it's not
quite correct (yet). It is also missing a few prerequisites: Neither the
tensor \texttt{x} nor the variables \texttt{lr} and
\texttt{num\_iterations} have been defined. Let's make sure we have
those ready first. \texttt{lr}, for learning rate, is the fraction of
the gradient to subtract on every step, and \texttt{num\_iterations} is
the number of steps to take. Both are a matter of experimentation.

\begin{verbatim}
lr <- 0.01

num_iterations <- 1000
\end{verbatim}

\texttt{x} is the parameter to optimize, that is, it is the function
input that hopefully, at the end of the process, will yield the minimum
possible function value. This makes it the tensor \emph{with respect to
which} we want to compute the function value's derivative. And that, in
turn, means we need to create it with \texttt{requires\_grad\ =\ TRUE}:

\begin{verbatim}
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
\end{verbatim}

The starting point, \texttt{(-1,1)}, here has been chosen arbitrarily.

Now, all that remains to be done is apply a small fix to the
optimization loop. With \emph{autograd} enabled on \texttt{x},
\texttt{torch} will record all operations performed on that tensor,
meaning that whenever we call \texttt{backward()}, it will compute all
required derivatives. However, when we subtract a fraction of the
gradient, this is not something we want a derivative to be calculated
for! We need to tell \texttt{torch} not to record this action, and that
we can do by wrapping it in \texttt{with\_no\_grad()}.

There's one other thing we have to tell it. By default, \texttt{torch}
accumulates the gradients stored in \texttt{grad} fields. We need to
zero them out for every new calculation, using \texttt{grad\$zero\_()}.

Taking into account these considerations, the parameter update should
look like this:

\begin{verbatim}
with_no_grad({
  x$sub_(lr * x$grad)
  x$grad$zero_()
})
\end{verbatim}

Here is the complete code, enhanced with logging statements that make it
easier to see what is going on.

\begin{verbatim}
num_iterations <- 1000

lr <- 0.01

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")

  value <- rosenbrock(x)
  if (i %% 100 == 0) {
    cat("Value is: ", as.numeric(value), "\n")
  }

  value$backward()
  if (i %% 100 == 0) {
    cat("Gradient is: ", as.matrix(x$grad), "\n")
  }

  with_no_grad({
    x$sub_(lr * x$grad)
    x$grad$zero_()
  })
}
\end{verbatim}

\begin{verbatim}
Iteration:  100 
Value is:  0.3502924 
Gradient is:  -0.667685 -0.5771312 

Iteration:  200 
Value is:  0.07398106 
Gradient is:  -0.1603189 -0.2532476 

Iteration:  300 
Value is:  0.02483024 
Gradient is:  -0.07679074 -0.1373911 

Iteration:  400 
Value is:  0.009619333 
Gradient is:  -0.04347242 -0.08254051 

Iteration:  500 
Value is:  0.003990697 
Gradient is:  -0.02652063 -0.05206227 

Iteration:  600 
Value is:  0.001719962 
Gradient is:  -0.01683905 -0.03373682 

Iteration:  700 
Value is:  0.0007584976 
Gradient is:  -0.01095017 -0.02221584 

Iteration:  800 
Value is:  0.0003393509 
Gradient is:  -0.007221781 -0.01477957

Iteration:  900 
Value is:  0.0001532408 
Gradient is:  -0.004811743 -0.009894371 

Iteration:  1000 
Value is:  6.962555e-05 
Gradient is:  -0.003222887 -0.006653666 
\end{verbatim}

After thousand iterations, we have reached a function value lower than
0.0001. What is the corresponding \texttt{(x1,x2)}-position?

\begin{verbatim}
x
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.9918
 0.9830
[ CPUFloatType{2} ]
\end{verbatim}

This is rather close to the true minimum of \texttt{(1,1)}. If you feel
like, play around a little, and try to find out what kind of difference
the learning rate makes. For example, try 0.001 and 0.1, respectively.

In the next chapter, we will build a neural network from scratch. There,
the function we minimize will be a \emph{loss function}, namely, the
mean squared error arising from a regression problem.

\hypertarget{sec:network-1}{%
\chapter{A neural network from scratch}\label{sec:network-1}}

In this chapter, we are going to solve a regression\index{regression}
task. But wait -- not the \texttt{lm} way. We'll be building a real
neural network, making use of tensors only (\texttt{autograd}-enabled
ones, it goes without saying). Of course, this is not how you'll be
using \texttt{torch}, later; but this does not make it a useless
endeavor. On the contrary. Having seen the raw mechanics, you'll be able
to appreciate even more the hard work that \texttt{torch} saves you.
What's more, understanding the basics will be an efficient antidote
against the surprisingly common temptation to think of deep learning as
some kind of ``magic''. It's all just matrix computations; one has to
learn how to orchestrate them though.

Let's start with what we need for a network that can perform regression.

\hypertarget{idea}{%
\section{Idea}\label{idea}}

In a nutshell, a network is a \emph{function} from inputs to outputs. A
suitable function, thus, is what we're looking for.

To find it, let's first think of regression as \emph{linear} regression.
What linear regression does is multiply and add. For each independent
variable, there is a \emph{coefficient} that multiplies it. On top of
that, there is a so-called \emph{bias} term that gets added at the end.
(In two dimensions, regression coefficient and bias correspond to slope
and x-intercept of the regression line.)

Thinking about it, multiplication and addition are things we can do with
tensors -- one could even say they are made for exactly that. Let's take
an example where the input data consist of a hundred observations, with
three features each. For example:

\begin{verbatim}
library(torch)

x <- torch_randn(100, 3)
x$size()
\end{verbatim}

\begin{verbatim}
[1] 100   3
\end{verbatim}

To store the per-feature coefficients that should multiply \texttt{x},
we need a column vector of length 3, the number of features.
Alternatively, preparing for a modification we're going to make very
soon, this can be a matrix whose columns are of length three, that is, a
matrix with three rows. How many columns should it have? Let's say we
want to predict a single output feature. In that case, the matrix should
be of size 3 x 1.

Here comes a suitable candidate, initialized randomly. Note how the
tensor is created with \texttt{requires\_grad\ =\ TRUE}, as it
represents a parameter we'll want the network to \emph{learn}.

\begin{verbatim}
w <- torch_randn(3, 1, requires_grad = TRUE)
\end{verbatim}

The bias tensor then has to be of size 1 x 1:

\begin{verbatim}
b <- torch_zeros(1, 1, requires_grad = TRUE)
\end{verbatim}

Now, we can get a ``prediction'' by multiplying the data with the
weight\index{weight} matrix \texttt{w} and adding the bias\index{bias}
\texttt{b}:

\begin{verbatim}
y <- x$matmul(w) + b
print(y, n = 10)
\end{verbatim}

\begin{verbatim}
torch_tensor
-2.1600
-3.3244
 0.6046
 0.4472
-0.4971
-0.0530
 5.1259
-1.1595
-0.5960
-1.4584
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{100,1} ][ grad_fn = <AddBackward0> ]
\end{verbatim}

In math notation, what we've done here is implement the function:

\[
f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b}
\]

How does this relate to neural networks?

\hypertarget{layers}{%
\section{Layers}\label{layers}}

Circling back to neural-network terminology, what we've done here is
prototype the action of a network that has a \emph{single}
\emph{layer}\index{layer}: the output layer. However, a single-layer
network is hardly the type you'd be interested in building -- why would
you, when you could simply do linear regression instead? In fact, one of
the defining features of neural networks is their ability to chain an
unlimited (in theory) number of layers. Of these, all but the
output\index{layer!output} layer may be referred to as ``hidden''
layers, although from the point of view of someone who uses a deep
learning framework such as \texttt{torch}, they are not that
\emph{hidden} after all.

Let's say we want our network to have one hidden
layer\index{layer!hidden}. Its size, meaning, the number of \emph{units}
it has, will be an important factor in determining the network's power.
This number is reflected in the weight matrix we create: A layer with
eight units will need a weight matrix with eight columns.

\begin{verbatim}
w1 <- torch_randn(3, 8, requires_grad = TRUE)
\end{verbatim}

Each unit has its own value for bias, too.

\begin{verbatim}
b1 <- torch_zeros(1, 8, requires_grad = TRUE)
\end{verbatim}

Just like we saw before, the hidden layer will multiply the input it
receives by the weights and add the bias. That is, it applies the
function \(f\) displayed above. Then, another function is applied. This
function receives its input from the hidden layer and produces the final
output. In a nutshell, what is happening here is function composition:
Calling the second function \(g\), the overall transformation is
\(g(f(\mathbf{X})\), or \(g \circ f\).

For \(g\) to yield an output analogous to the single-layer architecture
above, its weight matrix has to take the eight-column hidden layer to a
single column. That is, \texttt{w2} looks like this:

\begin{verbatim}
w2 <- torch_randn(8, 1, requires_grad = TRUE)
\end{verbatim}

The bias, \texttt{b2}, is a single value, like \texttt{b1}:

\begin{verbatim}
b2 <- torch_randn(1, 1, requires_grad = TRUE)
\end{verbatim}

Of course, there is no reason to stop at \emph{one} hidden layer, and
once we've built up the complete apparatus, please feel invited to
experiment with the code. But first, we need to add in a few other types
of components. For one, with our most recent architecture, what we're
doing is chain, or compose, functions -- which is good. But all these
functions are doing is add and multiply, implying that they are linear.
The power of neural networks, however, is usually associated with
\emph{nonlinearity}\index{nonlinearity}. Why?

\hypertarget{activation-functions}{%
\section{\texorpdfstring{Activation
functions\index{activation function}}{Activation functions}}\label{activation-functions}}

Imagine, for a moment, that we had a network with three layers, and all
each layer did was multiply its input by its weight matrix. (Having a
bias term doesn't really change anything. But it makes the example more
complex, so we're ``abstracting it out''.)

This gives us a chain of matrix multiplications:
\(f(\mathbf{X}) = ((\mathbf{X} \mathbf{W}_1)\mathbf{W}_2)\mathbf{W}_3\).
Now, this can be rearranged so that all the weight matrices are
multiplied together before application to \(\mathbf{X}\):
\(f(\mathbf{X}) = \mathbf{X} (\mathbf{W}_1\mathbf{W}_2\mathbf{W}_3)\).
Thus, this three-layer network can be simplified to a single-layer one,
where \(f(\mathbf{X}) = \mathbf{X} \mathbf{W}_4\). And now, we have lost
all advantages associated with deep neural networks.

This is where activation functions, sometimes called ``nonlinearities'',
come in. They introduce non-linear operations that cannot be modeled by
matrix multiplication. Historically, the prototypical activation
function has been the \emph{sigmoid}\index{activation!sigmoid}, and it's
still extremely important today. Its constitutive action is to squish
its input between zero and one, yielding a value that can be interpreted
as a probability. But in regression, this is not usually what we want,
and neither would it be for most hidden layers.

Instead, the most-used activation function inside a network is the
so-called \emph{ReLU}\index{activation!ReLU}, or Rectified Linear Unit.
This is a long name for something rather straightforward: All negative
values are set to zero. In \texttt{torch}, this can be accomplished
using the \texttt{relu()} function:

\begin{verbatim}
t <- torch_tensor(c(-2, 1, 5, -7))
t$relu()
\end{verbatim}

\begin{verbatim}
torch_tensor
 0
 1
 5
 0
[ CPUFloatType{4} ]
\end{verbatim}

Why would this be nonlinear? One criterion for a linear function is that
when you have two inputs, it doesn't matter if you first add them and
then, apply the transformation, or if you start by applying the
transformation independently to both inputs and then, go ahead and add
them. But with ReLU, this does not work:

\begin{verbatim}
t1 <- torch_tensor(c(1, 2, 3))
t2 <- torch_tensor(c(1, -2, 3))

t1$add(t2)$relu()
\end{verbatim}

\begin{verbatim}
torch_tensor
 2
 0
 6
[ CPUFloatType{3} ]
\end{verbatim}

\begin{verbatim}
t1_clamped <- t1$relu()
t2_clamped <- t2$relu()

t1_clamped$add(t2_clamped)
\end{verbatim}

\begin{verbatim}
torch_tensor
 2
 2
 6
[ CPUFloatType{3} ]
\end{verbatim}

The results are not the same.

Wrapping up so far, we've talked about how to code layers and activation
functions. There is just one further concept to discuss before we can
build the complete network. This is the loss function.

\hypertarget{loss-functions}{%
\section{Loss functions}\label{loss-functions}}

Put abstractly, the loss is a measure of how far away we are from our
goal. When minimizing a function, like we did in the previous chapter,
this is the difference between the current function value and the
smallest value it can take. With neural networks, we are free to choose
a suitable loss function as we like, provided it matches our task. For
regression-type tasks, this often will be mean squared error (MSE),
although it doesn't have to be. For example, there could be reasons to
use mean absolute error instead.

In \texttt{torch}, computation of mean squared error is a one-liner:

\begin{verbatim}
y <- torch_randn(5)
y_pred <- y + 0.01

loss <- (y_pred - y)$pow(2)$mean()

loss
\end{verbatim}

\begin{verbatim}
torch_tensor
9.99999e-05
[ CPUFloatType{} ]
\end{verbatim}

As soon as we have the loss, we'll be able to update the weights,
subtracting a fraction of its gradient. We've already seen how to do
this in the last chapter, and will see it again shortly.

We now take the pieces discussed and put them together.

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

We split this into three parts. This way, when later we refactor
individual components to make use of higher-level \texttt{torch}
functionality, it will be easier to see the areas where encapsulation
and modularization are occurring.

\hypertarget{generate-random-data}{%
\subsection{Generate random data}\label{generate-random-data}}

Our example data consist of one hundred observations. The input,
\texttt{x}, has three features; the target, \texttt{y}, just one.
\texttt{y} is generated from \texttt{x}, but with some noise added.

\begin{verbatim}
library(torch)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)
\end{verbatim}

Next, the network.

\hypertarget{build-the-network}{%
\subsection{\texorpdfstring{Build the
network\index{network}}{Build the network}}\label{build-the-network}}

The network has two layers: a hidden layer and the output layer. This
means that we need two weight matrices and two bias tensors. For no
special reason, the hidden layer here has thirty-two units:

\begin{verbatim}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

# weights connecting input to hidden layer
w1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 <- torch_zeros(1, d_out, requires_grad = TRUE)
\end{verbatim}

With their current values -- results of random initialization -- those
weights and biases won't be of much use. Time to train the network.

\hypertarget{train-the-network}{%
\subsection{Train the network}\label{train-the-network}}

Training the network means passing the input through its layers,
calculating the loss, and adjusting the parameters\index{parameters}
(weights and biases) in a way that predictions improve. These activities
we keep repeating until performance seems sufficient (which, in
real-life applications, would have to be defined very carefully).
Technically, each repeated application of these steps is called an
\emph{epoch}\index{epoch}.

Just like with function minimization, deciding on a suitable learning
rate (the fraction of the gradient to subtract) needs some
experimentation.

Looking at the below training loop, you see that, logically, it consists
of four parts:

\begin{itemize}
\item
  do a forward pass\index{forward pass}, yielding the network's
  predictions (if you dislike the one-liner, feel free to split it up);
\item
  compute the loss (this, too, being a one-liner -- we merely added some
  logging);
\item
  have \emph{autograd} calculate the gradient of the loss with respect
  to the parameters; and
\item
  update the parameters accordingly (again, taking care to wrap the
  whole action in \texttt{with\_no\_grad()}, and zeroing the
  \texttt{grad} fields on every iteration).
\end{itemize}

\begin{verbatim}
learning_rate <- 1e-4

### training loop ----------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  
  y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)
  
  ### -------- Compute loss -------- 
  loss <- (y_pred - y)$pow(2)$mean()
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with
  # requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we don't 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 <- w1$sub_(learning_rate * w1$grad)
     w2 <- w2$sub_(learning_rate * w2$grad)
     b1 <- b1$sub_(learning_rate * b1$grad)
     b2 <- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they'd
     # accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}
\end{verbatim}

\begin{verbatim}
Epoch: 10 Loss: 24.92771
Epoch: 20 Loss: 23.56143
Epoch: 30 Loss: 22.3069
Epoch: 40 Loss: 21.14102
Epoch: 50 Loss: 20.05027
Epoch: 60 Loss: 19.02925
Epoch: 70 Loss: 18.07328
Epoch: 80 Loss: 17.16819
Epoch: 90 Loss: 16.31367
Epoch: 100 Loss: 15.51261
Epoch: 110 Loss: 14.76012
Epoch: 120 Loss: 14.05348
Epoch: 130 Loss: 13.38944
Epoch: 140 Loss: 12.77219
Epoch: 150 Loss: 12.19302
Epoch: 160 Loss: 11.64823
Epoch: 170 Loss: 11.13535
Epoch: 180 Loss: 10.65219
Epoch: 190 Loss: 10.19666
Epoch: 200 Loss: 9.766989
\end{verbatim}

The loss decreases quickly at first, and then, not so rapidly anymore.
But this example was not created to exhibit magnificent performance; the
idea was to show how few lines of code are needed to build a ``real''
neural network.

Now, the layers, the loss, the parameter updates -- all that is still
pretty ``raw'': It's (literally) \emph{just tensors}. For such a small
network this works fine, but it would get cumbersome pretty fast for
more complex designs. The following two chapters, thus, will show how to
abstract away weights and biases into neural network \emph{modules},
swap self-made loss functions with built-in ones, and get rid of the
verbose parameter update routine.

\hypertarget{sec:modules}{%
\chapter{Modules}\label{sec:modules}}

In the last chapter, we built a neural network for a regression task.
There were two distinct types of operations: linear and non-linear.

In the non-linear category, we had ReLU activation, expressed as a
straightforward function call: \texttt{nnf\_relu()}. Activation
functions are \emph{functions}: Given input \(\mathbf{x}\), they return
output \(\mathbf{y}\) every time. In other words, they are
deterministic. It's different with the linear part, though.

The linear part in the regression network was implemented as
multiplication by a matrix -- the weight matrix -- and addition of a
vector (the bias vector). With operations like that, results inevitably
depend on the actual values stored in the respective tensors. Put
differently, the operation is \emph{stateful}.

Whenever there is state involved, it helps to encapsulate it in an
object, freeing the user from manual management. This is what
\texttt{torch}'s \emph{modules} do.

Note that term, \emph{modules}\index{module (terminology)}. In
\texttt{torch}, a module can be of any complexity, ranging from basic
\emph{layers} -- like the \texttt{nn\_linear()} we are going to
introduce in a minute -- to complete \emph{models} consisting of many
such layers. Code-wise, there is no difference between
``layers''\index{layer (terminology)} and
``models''\index{model (terminology)}. This is why in some texts, you'll
see ``module'' used throughout. In this book, I'll mostly stay with the
common terminology of layers and models, as it maps more closely to how
things appear conceptually.

Back to the \emph{why} of modules. In addition to encapsulation, there
is another reason for providing layer objects: Not all often-used layers
are as light-weight as \texttt{nn\_linear()} is. We'll quickly mention a
few others at the end of the next section, reserving a complete
introduction to later chapters of this book.

\hypertarget{built-in-nn_modules}{%
\section{\texorpdfstring{Built-in
\texttt{nn\_module()}s}{Built-in nn\_module()s}}\label{built-in-nn_modules}}

In \texttt{torch}, a linear layer is created using
\texttt{nn\_linear()}. \texttt{nn\_linear()} expects (at least) two
arguments: \texttt{in\_features} and \texttt{out\_features}. Let's say
your input data has fifty observations with five features each; that is,
it is of size 50 x 5. You want to build a hidden layer with sixteen
units. Then \texttt{in\_features} is 5, and \texttt{out\_features} is
16. (The same 5 and 16 would constitute the number of rows/columns in
the weight matrix if you built one yourself.)

\begin{verbatim}
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
\end{verbatim}

Once created, the module readily informs you about its parameters:

\begin{verbatim}
l
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 96 parameters.
Parameters
 weight: Float [1:16, 1:5]
 bias: Float [1:16]
\end{verbatim}

Encapsulation doesn't keep us from inspecting the weight and bias
tensors:

\begin{verbatim}
l$weight
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.2079 -0.1920  0.2926  0.0036 -0.0897
 0.3658  0.0076 -0.0671  0.3981 -0.4215
 0.2568  0.3648 -0.0374 -0.2778 -0.1662
 0.4444  0.3851 -0.1225  0.1678 -0.3443
-0.3998  0.0207 -0.0767  0.4323  0.1653
 0.3997  0.0647 -0.2823 -0.1639 -0.0225
 0.0479  0.0207 -0.3426 -0.1567  0.2830
 0.0925 -0.4324  0.0448 -0.0039  0.1531
-0.2924 -0.0009 -0.1841  0.2028  0.1586
-0.3064 -0.4006 -0.0553 -0.0067  0.2575
-0.0472  0.1238 -0.3583  0.4426 -0.0269
-0.0275 -0.0295 -0.2687  0.2236  0.3787
-0.2617 -0.2221  0.1503 -0.0627  0.1094
 0.0122  0.2041  0.4466  0.4112  0.4168
-0.4362 -0.3390  0.3679 -0.3045  0.1358
 0.2979  0.0023  0.0695 -0.1906 -0.1526
[ CPUFloatType{16,5} ]
\end{verbatim}

\begin{verbatim}
l$bias
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.2314
 0.2942
 0.0567
-0.1728
-0.3220
-0.1553
-0.4149
-0.2103
-0.1769
 0.4219
-0.3368
 0.0689
 0.3625
-0.1391
-0.1411
-0.2014
[ CPUFloatType{16} ]
\end{verbatim}

At this point, I need to ask for your indulgence. You've probably
noticed that \texttt{torch} reports the weight matrix as being of size
16 x 5, not 5 x 16, like we said you'd create it when coding from
scratch. This is due to an implementation detail inherited from the
underlying C++ implementation, \texttt{libtorch}. For performance
reasons, \texttt{libtorch}'s linear module stores the weight and bias
tensors in \emph{transposed} form. On the R side, all we can do is
explicitly point you to it and thereby, hopefully, alleviate the
confusion.

Let's go on. To apply this module to input data, just ``call'' it like a
function:

\begin{verbatim}
x <- torch_randn(50, 5)
output <- l(x)
output$size()
\end{verbatim}

\begin{verbatim}
[1] 50 16
\end{verbatim}

So that's the forward pass. How about gradient computation? Previously,
when creating a tensor we wanted to figure as a ``source'' in gradient
computation, we had to let \texttt{torch} know explicitly, passing
\texttt{requires\_grad\ =\ TRUE}. No such thing is required for built-in
\texttt{nn\_module()}s. We can immediately check that \texttt{output}
knows what to do on \texttt{backward()}:

\begin{verbatim}
output$grad_fn
\end{verbatim}

\begin{verbatim}
AddmmBackward0
\end{verbatim}

To be sure though, let's calculate some ``dummy'' loss based on
\texttt{output}, and call \texttt{backward()}. We see that now, the
linear module's \texttt{weight} tensor has its \texttt{grad} field
populated:

\begin{verbatim}
loss <- output$mean()
loss$backward()
l$weight$grad
\end{verbatim}

\begin{verbatim}
torch_tensor
0.01 *
-0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
 -0.3064  2.4118 -0.6095  0.3419 -1.6131
[ CPUFloatType{16,5} ]
\end{verbatim}

Thus, once you work with \texttt{nn\_module}s, \texttt{torch}
automatically assumes that you'll want gradients computed.

\texttt{nn\_linear()}, straightforward though it may be, is an essential
building block encountered in most every model architecture. Others
include:

\begin{itemize}
\item
  \texttt{nn\_conv1d()}, \texttt{nn\_conv2d(),\ and\ nn\_conv3d()}, the
  so-called \emph{convolutional} layers that apply filters to input data
  of varying dimensionality,
\item
  \texttt{nn\_lstm()} and \texttt{nn\_gru()} , the \emph{recurrent}
  layers that carry through a state,
\item
  \texttt{nn\_embedding()} that is used to embed categorical data in
  high-dimensional space,
\item
  and more.
\end{itemize}

\hypertarget{building-up-a-model}{%
\section{Building up a model}\label{building-up-a-model}}

The built-in \texttt{nn\_module()}s give us \emph{layers}, in usual
speak. How do we combine those into \emph{models}? Using the ``factory
function'' \texttt{nn\_module()}, we can define models of arbitrary
complexity. But we may not always need to go that way.

\hypertarget{models-as-sequences-of-layers-nn_sequentialindexnn_sequential}{%
\subsection{\texorpdfstring{Models as sequences of layers:
\texttt{nn\_sequential()}index\{\texttt{nn\_sequential()}\}}{Models as sequences of layers: nn\_sequential()index\{nn\_sequential()\}}}\label{models-as-sequences-of-layers-nn_sequentialindexnn_sequential}}

If all our model should do is propagate straight through the layers, we
can use \texttt{nn\_sequential()} to build it. Models consisting of all
linear layers are known as \emph{Multi-Layer
Perceptrons}index\{Multi-Layer Perceptron (MLP)\} (MLPs). Here is one:

\begin{verbatim}
mlp <- nn_sequential(
  nn_linear(10, 32),
  nn_relu(),
  nn_linear(32, 64),
  nn_relu(),
  nn_linear(64, 1)
)
\end{verbatim}

Take a close look at the layers involved. We've already seen
\texttt{nnf\_relu()}, the \emph{function} that implements ReLU
activation. (The \texttt{f} in \texttt{nnf\_} stands for functional.)
Below, \texttt{nn\_relu}, like \texttt{nn\_linear()}, is a module, that
is, an object. This is because \texttt{nn\_sequential()} expects all its
arguments to be modules.

Just like the built-in modules, you can apply this model to data by just
\emph{calling} it:

\begin{verbatim}
mlp(torch_randn(5, 10))
\end{verbatim}

\begin{verbatim}
torch_tensor
0.01 *
-7.8097
 -9.0363
 -38.3282
  5.3959
 -16.4837
[ CPUFloatType{5,1} ][ grad_fn = <AddmmBackward0> ]
\end{verbatim}

The single call triggered a complete forward pass through the network.
Analogously, calling \texttt{backward()} will
back-propagate\index{backpropagation} through all the layers.

What if you need the model to chain execution steps in a non-sequential
way?

\hypertarget{models-with-custom-logic}{%
\subsection{Models with custom logic}\label{models-with-custom-logic}}

As already hinted at, this is where you use \texttt{nn\_module()}.

\texttt{nn\_module()} creates constructors for custom-made R6 objects.
Below, \texttt{my\_linear()} is such a constructor. When called, it will
return a linear module similar to the built-in \texttt{nn\_linear()}.

Two methods should be implemented in defining a constructor:
\texttt{initialize()} and \texttt{forward()}. \texttt{initialize()}
creates the module object's fields, that is, the objects or values it
``owns'' and can access from inside any of its methods.
\texttt{forward()} defines what should happen when the module is called
on the input:

\begin{verbatim}
my_linear <- nn_module(
  initialize = function(in_features, out_features) {
    self$w <- nn_parameter(torch_randn(
      in_features, out_features
    ))
    self$b <- nn_parameter(torch_zeros(out_features))
  },
  forward = function(input) {
    input$mm(self$w) + self$b
  }
)
\end{verbatim}

Note the use of \texttt{nn\_parameter()}. \texttt{nn\_parameter()} makes
sure that the passed-in tensor is registered as a module
\emph{parameter}, and thus, is subject to backpropagation by default.

To instantiate the newly-defined module, call its constructor:

\begin{verbatim}
l <- my_linear(7, 1)
l
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 8 parameters.

Parameters 
 w: Float [1:7, 1:1]
 b: Float [1:1]
\end{verbatim}

Granted, in this example, there really is no \emph{custom logic} we
needed to define our own module for. But here, you have a template
applicable to any use case. Later, we'll see definitions of
\texttt{initialize()} and \texttt{forward()} that are more complex, and
we'll encounter additional methods defined on modules. But the basic
mechanism will remain the same.

At this point, you may feel like you'd like to rewrite last chapter's
neural network using modules. Feel free to do so! Or maybe wait until,
in the next chapter, we'll have learned about \emph{optimizer}s, and
built-in loss functions. Once we're done, we'll return to our two
examples, function minimization and the regression network. Then, we'll
be removing all do-it-yourself pieces rendered superfluous by
\texttt{torch}.

\hypertarget{sec:optimizers}{%
\chapter{Optimizers}\label{sec:optimizers}}

By now, we've gone into quite some detail on tensors, automatic
differentiation, and modules. In this chapter, we look into the final
major concept present in core \texttt{torch}:
\emph{optimizers}\index{optimizers}. Where modules encapsulate layer and
model logic, optimizers do the same for optimization strategies.

Let's start by pondering why having optimizer objects is so useful.

\hypertarget{why-optimizers}{%
\section{Why optimizers?}\label{why-optimizers}}

To this question, there are two main types of answer. First, the
technical one.

If you look back at how we coded our first neural network, you'll see
that we proceeded like this:

\begin{itemize}
\item
  compute predictions (forward pass),
\item
  calculate the loss,
\item
  have \emph{autograd} compute partial derivatives (calling
  \texttt{loss\$backward()}), and
\item
  update the parameters, subtracting from each some fraction of the
  gradient.
\end{itemize}

Here is how that last part looked:

\begin{verbatim}
library(torch)

# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
loss$backward()
  
### -------- Update weights -------- 
  
# Wrap in with_no_grad() because this is a part we don't 
# want to record for automatic gradient computation
with_no_grad({
  w1 <- w1$sub_(learning_rate * w1$grad)
  w2 <- w2$sub_(learning_rate * w2$grad)
  b1 <- b1$sub_(learning_rate * b1$grad)
  b2 <- b2$sub_(learning_rate * b2$grad)  
     
  # Zero gradients after every pass, as they'd accumulate
  # otherwise
  w1$grad$zero_()
  w2$grad$zero_()
  b1$grad$zero_()
  b2$grad$zero_()  
})
\end{verbatim}

Now this was a small network -- imagine having to code such logic for
architectures with tens or hundreds of layers! Surely this can't be what
developers of a deep learning framework want their users to do.
Accordingly, weight updates are taken care of by specialized objects --
the optimizers in question.

Thus, the technical type of answer concerns usability and convenience.
But more is involved. With the above approach, there's hardly a way to
find a good learning rate other than by trial and error. And most
probably, there is not even an optimal learning rate that would be
constant over the whole training process. Fortunately, a rich tradition
of research has turned up at set of proven update strategies. These
strategies commonly involve a \emph{state} kept between operations. This
is another reason why, just like modules, optimizers are objects in
\texttt{torch}.

Before we look deeper at these strategies, let's see how we'd replace
the above manual weight-updating process with a version that uses an
optimizer.

\hypertarget{using-built-in-torch-optimizers}{%
\section{\texorpdfstring{Using built-in \texttt{torch}
optimizers}{Using built-in torch optimizers}}\label{using-built-in-torch-optimizers}}

An optimizer needs to know what it's supposed to optimize. In the
context of a neural network model, this will be the network's
parameters. With no real difference between ``model modules'' and
``layer modules'', however, we can demonstrate how it works using a
single built-in module such as \texttt{nn\_linear()}.

Here we instantiate a gradient descent
optimizer\index{gradient descent (optimizer)} designed to work on some
linear module's parameters:

\begin{verbatim}
l <- nn_linear(10, 2)

opt <- optim_sgd(l$parameters, lr = 0.1)
\end{verbatim}

In addition to the always-required reference to what tensors should be
optimized, \texttt{optim\_sgd()} has just a single non-optional
parameter: \texttt{lr}, the learning rate.

Once we have an optimizer object, parameter updates are triggered by
calling its \texttt{step()} method. One thing remains unchanged, though.
We still need to make sure gradients are not accumulated over training
iterations. This means we still call \texttt{zero\_grad()} -- but this
time, on the optimizer object.

This is the complete code replacing the above manual procedure:

\begin{verbatim}
# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
# no change here
loss$backward()

# Still need to zero out gradients before the backward pass,
# only this time, on the optimizer object
optimizer$zero_grad()

# use the optimizer to update model parameters
optimizer$step()
\end{verbatim}

I'm sure you'll agree that usability-wise, this is an enormous
improvement. Now, let's get back to our original question -- why
optimizers? -- and talk more about the second, strategic part of the
answer.

\hypertarget{parameter-update-strategies}{%
\section{Parameter update
strategies}\label{parameter-update-strategies}}

Searching for a good learning rate by trial and error is costly. And the
learning rate isn't even the only thing we're uncertain about. All it
does is specify how big of a step to take. However, that's not the only
unresolved question.

So far, we've always assumed that the direction of steepest descent, as
given by the gradient, is the best way to go. This is not always the
case, though. So we are left with uncertainties regarding both magnitude
and direction of parameter updates.

Fortunately, over the last decade, there has been significant progress
in research related to weight updating in neural networks. Here, we take
a look at major considerations involved, and situate in context some of
the most popular optimizers provided by \texttt{torch}.

The baseline to compare against is \emph{gradient descent}, or
\emph{steepest descent}, the algorithm we've been using in our manual
implementations of function minimization and neural-network training.
Let's quickly recall the guiding principle behind it.

\hypertarget{gradient-descent-a.k.a.-steepest-descent-a.k.a.-stochastic-gradient-descent-sgd}{%
\subsection{\texorpdfstring{Gradient descent (a.k.a. steepest descent,
a.k.a. stochastic gradient descent
(SGD))\index{stochastic gradient descent (optimizer)}}{Gradient descent (a.k.a. steepest descent, a.k.a. stochastic gradient descent (SGD))}}\label{gradient-descent-a.k.a.-steepest-descent-a.k.a.-stochastic-gradient-descent-sgd}}

The gradient -- the vector of partial derivatives, one for each input
feature -- indicates the direction in which a function increases most.
Going in the opposite direction means we descend the fastest way
possible. Or does it?

Unfortunately, it is not that simple. It depends on the landscape that
surrounds us, or put more technically, the contours of the function we
want to minimize. To illustrate, compare two situations.

The first is the one we encountered when first learning about automatic
differentiation. The example there was a quadratic function in two
dimensions. We didn't make a great deal out of it at the time, but an
important point about this specific function was that the slope was the
same in both dimensions. Under such conditions, steepest descent is
optimal.

Let's verify that. The function was :
\(f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5\), and its gradient,
\(\begin{bmatrix}0.4\\0.4 \end{bmatrix}\). Now say we're at point
\((x1, x2) = (6,6)\). For each coordinate, we subtract 0.4 times its
current value. Or rather, that would be if we had to use a learning rate
of 1. But we don't have to. If we pick a learning rate of 2.5, we can
arrive at the minimum in a single step:
\((x_1, x_2) = (6 - 2.5*0.4*6, 6 - 2.5*0.4*6) = (0,0)\). See below for
an illustration of what happens in each case
(fig.~\ref{fig-optimizers-steepest-descent-symmetric}).

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-steepest-descent-symmetric.png}

}

\caption{\label{fig-optimizers-steepest-descent-symmetric}Steepest
descent on an isotropic paraboloid, using different learning rates.}

\end{figure}

In a nutshell, thus, with a isotropic function like this -- the variance
being the same in both directions -- it is ``just'' a matter of getting
the learning rate right.

Now compare this to what happens if slopes in both directions are
decidedly distinct.

This time, the coefficient for \(x_2\) is ten times as big as that for
\(x_1\): We have \(f(x_1, x_2) = 0.2 {x_1}^2 + 2 {x_2}^2 - 5\). This
means that as we progress in the \(x_2\) direction, the function value
increases sharply, while in the \(x_1\) direction, it rises much more
slowly. Thus, during gradient descent, we make far greater progress in
one direction than the other.

Again, we investigate what happens for different learning
rates\index{learning rate}. Below, we contrast three different settings.
With the lowest learning rate, the process eventually reaches the
minimum, but a lot more slowly than in the symmetric case. With a
learning rate just slightly higher, descent gets lost in endless
zig-zagging, oscillating between positive and negative values of the
more influential variable, \(x_2\). Finally, a learning rate that,
again, is just minimally higher, has a catastrophic effect: The function
value explodes, zig-zagging up right to infinity
(fig.~\ref{fig-optimizers-steepest-descent-elliptic}).

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-steepest-descent-elliptic.png}

}

\caption{\label{fig-optimizers-steepest-descent-elliptic}Steepest
descent on a non-isotropic paraboloid, using (minimally!) different
learning rates.}

\end{figure}

This should be pretty convincing -- even with a pretty conventional
function of just two variables, steepest descent is far from being a
panacea! And in deep learning, loss functions will be a \emph{lot} less
well-behaved. This is where the need for more sophisticated algorithms
arises: Enter -- again -- optimizers.

\hypertarget{things-that-matter}{%
\subsection{Things that matter}\label{things-that-matter}}

Viewed conceptually, major modifications to steepest descent can be
categorized by the considerations that drive them, or equivalently, by
the problems they're trying to solve. Here, we focus on three such
considerations.

First, instead of starting in a completely new direction every time we
re-compute the gradient, we might want to keep a bit of the old
direction -- keep momentum, to use the technical term. This should help
avoiding the inefficient zig-zagging seen in the example above.

Second, looking back at just that example of minimizing a non-symmetric
function \ldots{} Why, really, should we be constrained to using the
same learning rate for all variables? When it's evident that all
variables don't vary to the same degree, why don't we update them in
individually appropriate ways?

Third -- and this is a fix for problems that only arise once you've
taken actions to reduce the learning rate for overly-impactful features
-- you also want to make sure that learning still progresses, that
parameters still get updated.

These considerations are nicely illustrated by a few classics among the
optimization algorithms.

\hypertarget{staying-on-track-gradient-descent-with-momentum}{%
\subsection{\texorpdfstring{Staying on track: Gradient descent with
momentum\index{momentum (optimizer)}}{Staying on track: Gradient descent with momentum}}\label{staying-on-track-gradient-descent-with-momentum}}

In gradient descent with momentum, we don't \emph{directly} use the
gradient to update the weights. Instead, you can picture weight updates
as particles moving on a trajectory: They want to keep going in whatever
direction they're going -- keep their \emph{momentum}, in physics speak
-- but get continually deflected by collisions. These ``collisions'' are
friendly nudges to, please, keep into account the gradient at the
\emph{now current} position. These dynamics result in a two-step update
logic.

In the below formulas, the choice of symbols reflects the physical
analogy. \(\mathbf{x}\) is the position, ``where we're at'' in parameter
space -- or more simply, the current values of the parameters. Time
evolution is captured by superscripts, with \(\mathbf{y}^{(k)}\)
representing the state of variable \(\mathbf{y}\) at the current time,
\(k\). The instantaneous velocity at time \(k\) is just what is measured
by the gradient, \(\mathbf{g}^{(k)}\). But in updating position, we
won't directly make use of it. Instead, at each iteration, the update
velocity is a combination of old velocity -- weighted by \emph{momentum}
parameter \(m\) -- and the freshly-computed gradient (weighted by the
learning rate). Step one of the two-step logic captures this strategy:

\begin{equation}\protect\hypertarget{eq-optimizers-1}{}{
\mathbf{v}^{(k+1)} = m \ \mathbf{v}^{(k)} + lr \ \mathbf{g}^{(k)} 
}\label{eq-optimizers-1}\end{equation}

The second step then is the update of \(\mathbf{x}\) due to this
``compromise'' velocity \(\mathbf{v}\).

\begin{equation}\protect\hypertarget{eq-optimizers-2}{}{
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \mathbf{v}^{(k+1)}
}\label{eq-optimizers-2}\end{equation}

Besides the physics analogy, there is another one you may find useful,
one that makes use of a concept prominent in time series analysis. If we
choose \(m\) and \(lr\) such that they add up to 1, the result is an
\emph{exponentially weighted moving average}. (While this
conceptualization, I think, helps understanding, in practice there is no
necessity to have \(m\) and \(lr\) summing to 1, though).

Now, let's return to the non-isotropic paraboloid, and compare SGD with
and without momentum. For the latter (bright curve), I'm using a
combination of \(lr = 0.5\) and \(mu = 0.1\). For SGD -- dark curve --
the learning rate is the ``good one'' from the figure above.Definitely,
SGD with momentum requires far fewer steps to reach the minimum
(fig.~\ref{fig-optimizers-momentum}).

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-momentum.png}

}

\caption{\label{fig-optimizers-momentum}SGD with momentum (white),
compared with vanilla SGD (gray).}

\end{figure}

\hypertarget{adagrad}{%
\subsection{\texorpdfstring{Adagrad\index{Adagrad (optimizer)}}{Adagrad}}\label{adagrad}}

Can we do better yet? Now, we know that in our running example, it is
really the fact that one feature changes much faster than the other that
slows down optimization. Having separate learning rates per parameter
thus clearly seems like a thing we want. In fact, most of the optimizers
popular in deep learning have per-parameter learning rates. But how
would you actually determine those?

This is where different algorithms differ. Adagrad, for example, divides
each parameter update by the cumulative sum of its partial derivatives
(squared, to be precise), where ``cumulative'' means we're keeping track
of them since the very first iteration. If we call that ``accumulator
variable'' \(s\), refer to the parameter in question by \(i\), and count
iterations using \(k\), this gives us the following formula for keeping
\(s\) updated:

\begin{equation}\protect\hypertarget{eq-optimizers-3}{}{
s_i^{(k)} = \sum_{j=1}^k (g_i^{(j)})^2
}\label{eq-optimizers-3}\end{equation}

(By the way, feel free to skip over the formulas if you don't like them.
I'm doing my best to communicate what they do in words, so you shouldn't
miss out on essential information.)

Now, the update rule for each parameter subtracts a portion of the
gradient, as did vanilla steepest descent -- but this time, that portion
is determined not just by the (global) learning rate, but also, by the
aforementioned cumulative sum of squared partials. The bigger that sum
-- that is, the bigger the gradients have been during training -- the
smaller the adjustment:\footnote{Here \(\epsilon\) is just a tiny value
  added to avoid division by zero.}

\begin{equation}\protect\hypertarget{eq-optimizers-4}{}{
x_i^{(k+1)} = x_i^{(k)} - \frac{lr}{\epsilon + \sqrt{s_i^{(k)}}}\ g_i^{(k)}\\
}\label{eq-optimizers-4}\end{equation}

The net effect of this strategy is that, if a parameter has consistently
high gradients, its influence is played down. Parameters with,
habitually, tiny gradients, on the other hand, can be sure to receive a
lot of attention once that changes.

With this algorithm, the global learning rate, \(lr\), is of lesser
importance. In our running example, it turns out that for best results,
we can (and should) use a very high learning rate: 3.7! Here
(fig.~\ref{fig-optimizers-adagrad}) is the result, again comparing with
vanilla gradient descent (gray curve):

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-adagrad.png}

}

\caption{\label{fig-optimizers-adagrad}Adagrad (white), compared with
vanilla SGD (gray).}

\end{figure}

In our example, thus, Adagrad performs excellently. But in training a
neural network, we tend to run \emph{a lot} of iterations. Then, with
the way gradients are accumulated, the effective learning rate decreases
more and more, and a dead end is reached.

Are there other ways to have individual, per-parameter learning rates?

\hypertarget{rmsprop}{%
\subsection{\texorpdfstring{RMSProp\index{RMSProp (optimizer)}}{RMSProp}}\label{rmsprop}}

RMSProp replaces the cumulative-gradient strategy found in Adagrad with
a weighted-average one. At each point, the ``bookkeeping'',
per-parameter variable \(s_i\) is a weighted average of its previous
value and the previous (squared) gradient:

\begin{equation}\protect\hypertarget{eq-optimizers-5}{}{
s_i^{(k+1)} = \gamma \ s_i^{(k)} + (1-\gamma) \ (g_i^{(k)})^2
}\label{eq-optimizers-5}\end{equation}

The update then looks as with Adagrad:

\begin{equation}\protect\hypertarget{eq-optimizers-6}{}{
x_i^{(k+1)} = x_i^{(k)} - \frac{lr}{\epsilon + \sqrt{s_i^{(k)}}}\ g_i^{(k)}\\
}\label{eq-optimizers-6}\end{equation}

In this way, each parameter update gets weighted appropriately, without
learning slowing down overall.

Here is the result, again compared against the SGD baseline
(fig.~\ref{fig-optimizers-rmsprop}):

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-rmsprop.png}

}

\caption{\label{fig-optimizers-rmsprop}RMSProp (white), compared with
vanilla SGD (gray).}

\end{figure}

As of today, RMSProp is one of the most-often used optimizers in deep
learning, with probably just Adam - to be introduced next -- being more
popular.

\hypertarget{adam}{%
\subsection{\texorpdfstring{Adam\index{Adam (optimizer)}}{Adam}}\label{adam}}

Adam combines two concepts we've already seen: momentum -- to keep ``on
track'' -- and parameter-dependent updates, to avoid excessive
dependence on fast-changing parameters. The logic is like
this.\footnote{Actual implementations usually contain an additional
  step, but there is no need to go into details here.}

For one, just like in SGD with momentum, we keep an exponentially
weighted average of gradients. Here the weighting coefficient,
\(\gamma_v\), is usually set to 0.9.

\begin{equation}\protect\hypertarget{eq-optimizers-7}{}{
v_i^{(k+1)} = \gamma_v \ v_i^{(k)} + (1-\gamma_v) \ g_i^{(k)}
}\label{eq-optimizers-7}\end{equation}

Also, like in RMSProp, there is an exponentially weighted average of
squared gradients, with weighting coefficient \(\gamma_s\) usually set
to 0.999.

\begin{equation}\protect\hypertarget{eq-optimizers-8}{}{
s_i^{(k+1)} = \gamma_s \ s_i^{(k)} + (1-\gamma_s) \ (g_i^{(k)})^2
}\label{eq-optimizers-8}\end{equation}

The parameter updates now make use of that information in the following
way. The velocity determines the direction of the update, while both
velocity and magnitude of gradients (together with the learning rate,
\(lr\)) determine its size:

\begin{equation}\protect\hypertarget{eq-optimizers-9}{}{
x_i^{(k+1)} = x_i^{(k)} - \frac{lr \ 
v_i^{(k+1)}}{\epsilon + \sqrt{s_i^{(k+1)}}}\ \\
}\label{eq-optimizers-9}\end{equation}

Let's conclude this chapter by testing Adam on our running example
(fig.~\ref{fig-optimizers-adam}).

\begin{figure}[H]

{\centering \includegraphics{images/optimizers-adam.png}

}

\caption{\label{fig-optimizers-adam}Adam (white), compared with vanilla
SGD (gray).}

\end{figure}

Next, we head on to loss functions, the last building block to look at
before we re-factor the regression network and function minimization
examples to benefit from \texttt{torch} modules and optimizers.

\hypertarget{sec:loss-functions}{%
\chapter{Loss functions}\label{sec:loss-functions}}

The concept of a loss function is essential to machine learning. At any
iteration, the current loss value indicates how far the estimate is from
the target. It is then used to update the parameters in a direction that
will decrease the loss.

In our applied example, we already have made use of a loss function:
mean squared error, computed manually as

\begin{verbatim}
library(torch)

loss <- (y_pred - y)$pow(2)$sum()
\end{verbatim}

As you might expect, here is another area where this kind of manual
effort is not needed.

In this final conceptual chapter before we re-factor our running
examples, we want to talk about two things: First, how to make use of
\texttt{torch}'s built-in loss
functions\index{loss functions (built into torch)}. And second, what
function to choose.

\hypertarget{torch-loss-functions}{%
\section{\texorpdfstring{\texttt{torch} loss
functions}{torch loss functions}}\label{torch-loss-functions}}

In \texttt{torch}, loss functions start with \texttt{nn\_} or
\texttt{nnf\_}.

Using \texttt{nnf\_}, you directly \emph{call a function}.
Correspondingly, its arguments (estimate and target) both are tensors.
For example, here is \texttt{nnf\_mse\_loss()}, the built-in analog to
what we coded manually:

\begin{verbatim}
nnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.81
[ CPUFloatType{} ]
\end{verbatim}

With \texttt{nn\_}, in contrast, you create an object:

\begin{verbatim}
l <- nn_mse_loss()
\end{verbatim}

This object can then be called on tensors to yield the desired loss:

\begin{verbatim}
l(torch_ones(2, 2),torch_zeros(2, 2) + 0.1)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.81
[ CPUFloatType{} ]
\end{verbatim}

Whether to choose object or function is mainly a matter of preference
and context. In larger models, you may end up combining several loss
functions, and then, creating loss objects can result in more modular,
and more maintainable code. In this book, I'll mainly use the first way,
unless there are compelling reasons to do otherwise.

On to the second question.

\hypertarget{what-loss-function-should-i-choose}{%
\section{What loss function should I
choose?}\label{what-loss-function-should-i-choose}}

In deep learning, or machine learning overall, most applications aim to
do one (or both) of two things: predict a numerical value, or estimate a
probability. The regression task of our running example does the former;
real-world applications might forecast temperatures, infer employee
churn, or predict sales. In the second group, the prototypical task is
\emph{classification}. To categorize, say, an image according to its
most salient content, we really compute the respective probabilities.
Then, when the probability for ``dog'' is 0.7, while that for ``cat'' is
0.3, we say it's a dog.

\hypertarget{maximum-likelihood}{%
\subsection{Maximum likelihood}\label{maximum-likelihood}}

In both classification and regression, the mostly used loss functions
are built on the \emph{maximum likelihood} principle. Maximum likelihood
means: We want to choose model parameters in a way that the \emph{data},
the things we have observed or could have observed, are maximally
likely. This principle is not ``just'' fundamental, it is also
intuitively appealing. Imagine a simple example.

Say we have the values 7.1, 22.14, and 11.3, and we know that the
underlying process follows a normal distribution. Then it is much more
likely that these data have been generated by a distribution with mean
14 and standard deviation 7 than by one with mean 20 and standard
deviation 1.

\hypertarget{regression}{%
\subsection{Regression}\label{regression}}

In regression (that implicitly assumes the target distribution to be
normal\footnote{For cases where that assumption seems unlikely,
  distribution-adequate loss functions are provided (e.g., Poisson
  negative log likelihood, available as
  \texttt{nnf\_poisson\_nll\_loss()} .}), to maximize likelihood, we
just keep using mean squared error -- the loss we've been computing all
along. Maximum likelihood estimators have all kinds of desirable
statistical properties. However, in concrete applications, there may be
reasons to use different ones.

For example, say a dataset has outliers where, for some reason,
prediction and target are found to be deviating substantially. Mean
squared error will allocate high importance to these outliers. In such
cases, possible alternatives are mean absolute error
(\texttt{nnf\_l1\_loss()}) and smooth L1 loss
(\texttt{nn\_smooth\_l1\_loss()}). The latter is a mixture type that, by
default, computes the absolute (L1) error, but switches to squared (L2)
error whenever the absolute errors get very small.

\hypertarget{classification}{%
\subsection{Classification}\label{classification}}

In classification, we are comparing two \emph{distributions}. The
estimate is a probability by design, and the target can be viewed as
one, too. In that light, maximum likelihood estimation is equivalent to
minimizing the Kullback-Leibler divergence (KL divergence).

KL divergence is a measure of how two distributions differ. It depends
on two things: the likelihood of the data, as determined by some
data-generating process, and the likelihood of the data under the model.
In the machine learning scenario, however, we are concerned only with
the latter. In that case, the criterion to be minimized reduces to the
\emph{cross-entropy}\index{cross entropy} between the two distributions.
And cross-entropy loss is exactly what is commonly used in
classification tasks.

In \texttt{torch}, there are several variants of loss functions that
calculate cross-entropy. With this topic, it's nice to have a quick
reference around; so here is a quick lookup table
(tbl.~\ref{tbl-loss-funcs-features} abbreviates the -- rather long-ish
-- function names; see tbl.~\ref{tbl-loss-abbrevs} for the mapping):

\hypertarget{tbl-loss-funcs-features}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1159}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1449}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1884}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1739}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1594}}@{}}
\caption{\label{tbl-loss-funcs-features}Loss functions, by type of data
they work on (binary vs.~multi-class) and expected input (raw scores,
probabilities, or log probabilities).}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
& \textbf{Data} & & \textbf{Input} & & \\
& binary & multi-class & raw scores & probabilities & log probs \\
\emph{BCeL} & Y & & Y & & \\
\emph{Ce} & & Y & Y & & \\
\emph{BCe} & Y & & & Y & \\
\emph{Nll} & & Y & & & Y \\
\end{longtable}

\hypertarget{tbl-loss-abbrevs}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-loss-abbrevs}Abbreviations used to refer to
\texttt{torch} loss functions.}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{BCeL} & \texttt{nnf\_binary\_cross\_entropy\_with\_logits()} \\
\emph{Ce} & \texttt{nnf\_cross\_entropy()} \\
\emph{BCe} & \texttt{nnf\_binary\_cross\_entropy()} \\
\emph{Nll} & \texttt{nnf\_nll\_loss()} \\
\end{longtable}

To pick the function applicable to your use case, there are two things
to consider.

First, are there just two possible classes (``dog vs.~cat'', ``person
present / person absent'', etc.), or are there several?

And second, what is the type of the estimated values? Are they raw
scores (in theory, any value between plus and minus infinity)? Are they
probabilities (values between 0 and 1)? Or (finally) are they log
probabilities, that is, probabilities to which a logarithm has been
applied? (In the final case, all values should be either negative or
equal to zero.)

\hypertarget{binary-data}{%
\subsubsection{\texorpdfstring{Binary
data\index{cross entropy!binary}}{Binary data}}\label{binary-data}}

Starting with binary data, our example classification vector is a
sequence of zeros and ones. When thinking in terms of probabilities, it
is most intuitive to imagine the ones standing for presence, the zeros
for absence of one of the classes in question -- cat or no cat, say.

\begin{verbatim}
target <- torch_tensor(c(1, 0, 0, 1, 1))
\end{verbatim}

The raw scores could be anything. For example:

\begin{verbatim}
unnormalized_estimate <-
  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))
\end{verbatim}

To turn these into probabilities, all we need to do is pass them to
\texttt{nnf\_sigmoid()}. \texttt{nnf\_sigmoid()} squishes its argument
to values between zero and one:

\begin{verbatim}
probability_estimate <- nnf_sigmoid(unnormalized_estimate)
probability_estimate
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.9526
 0.9370
 0.2315
 0.9995
 0.8699
[ CPUFloatType{5} ]
\end{verbatim}

From the above table, we see that given \texttt{unnormalized\_estimate}
and \texttt{probability\_estimate}, we can use both as inputs to a loss
function -- but we have to choose the appropriate one. Provided we do
that, the output has to be the same in both cases.

Let's see (raw scores first):

\begin{verbatim}
nnf_binary_cross_entropy_with_logits(
  unnormalized_estimate, target
)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.643351
[ CPUFloatType{} ]
\end{verbatim}

And now,
probabilities:\index{\texttt{nnf{\textunderscore}binary{\textunderscore}cross{\textunderscore}entropy()}}

\begin{verbatim}
nnf_binary_cross_entropy(probability_estimate, target)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.643351
[ CPUFloatType{} ]
\end{verbatim}

That worked as expected. What does this mean in practice? It means that
when we build a model for binary classification, and the final layer
computes an un-normalized score, we don't need to attach a sigmoid layer
to obtain probabilities. We can just call
\texttt{nnf\_binary\_cross\_entropy\_with\_logits()} when training the
network. In fact, doing so is the preferred way, also due to reasons of
numerical stability.

\hypertarget{multi-class-data}{%
\subsubsection{\texorpdfstring{Multi-class
data\index{cross entropy!multi-class}}{Multi-class data}}\label{multi-class-data}}

Moving on to multi-class data, the most intuitive framing now really is
in terms of (several) \emph{classes}, not presence or absence of a
single class. Think of classes as class indices (maybe indexing into
some look-up table). Being indices, technically, classes start at 1:

\begin{verbatim}
target <- torch_tensor(c(2, 1, 3, 1, 3), dtype = torch_long())
\end{verbatim}

In the multi-class scenario, raw scores are a two-dimensional tensor.
Each row contains the scores for one observation, and each column
corresponds to one of the classes. Here's how the raw estimates could
look:

\begin{verbatim}
unnormalized_estimate <- torch_tensor(
  rbind(c(1.2, 7.7, -1),
    c(1.2, -2.1, -1),
    c(0.2, -0.7, 2.5),
    c(0, -0.3, -1),
    c(1.2, 0.1, 3.2)
  )
)
\end{verbatim}

As per the above table, given this estimate, we should be calling
\texttt{nnf\_cross\_entropy()} (and we will, when below we compare
results).

So that's the first option, and it works exactly as with binary data.
For the second, there is an additional step.

First, we again turn raw scores into probabilities, using
\texttt{nnf\_softmax()}. For most practical purposes,
\texttt{nnf\_softmax()} can be seen as the multi-class equivalent of
\texttt{nnf\_sigmoid()}. Strictly though, their effects are not the
same. In a nutshell, \texttt{nnf\_sigmoid()} treats low-score and
high-score values equivalently, while \texttt{nnf\_softmax()}
exacerbates the distances between the top score and the remaining ones
(``winner takes all'').

\begin{verbatim}
probability_estimate <- nnf_softmax(unnormalized_estimate,
  dim = 2
)
probability_estimate
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.0015  0.9983  0.0002
 0.8713  0.0321  0.0965
 0.0879  0.0357  0.8764
 0.4742  0.3513  0.1745
 0.1147  0.0382  0.8472
[ CPUFloatType{5,3} ]
\end{verbatim}

The second step, the one that was not required in the binary case,
consists in transforming the probabilities to log probabilities. In our
example, this could be accomplished by calling \texttt{torch\_log()} on
the \texttt{probability\_estimate} we just computed. Alternatively, both
steps together are taken care of by \texttt{nnf\_log\_softmax()}:

\begin{verbatim}
logprob_estimate <- nnf_log_softmax(unnormalized_estimate,
  dim = 2
)
logprob_estimate
\end{verbatim}

\begin{verbatim}
torch_tensor
-6.5017 -0.0017 -8.7017
-0.1377 -3.4377 -2.3377
-2.4319 -3.3319 -0.1319
-0.7461 -1.0461 -1.7461
-2.1658 -3.2658 -0.1658
[ CPUFloatType{5,3} ]
\end{verbatim}

Now that we have estimates in both possible forms, we can again compare
results from applicable loss functions. First,
\texttt{nnf\_cross\_entropy()} on the raw
scores:\index{\texttt{nnf{\textunderscore}cross{\textunderscore}entropy()}}

\begin{verbatim}
nnf_cross_entropy(unnormalized_estimate, target)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.23665
[ CPUFloatType{} ]
\end{verbatim}

And second, \texttt{nnf\_nll\_loss()} on the log
probabilities:\index{\texttt{nnf{\textunderscore}nll{\textunderscore}loss()}}

\begin{verbatim}
nnf_nll_loss(logprob_estimate, target)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.23665
[ CPUFloatType{} ]
\end{verbatim}

Application-wise, what was said for the binary case applies here as
well: In a multi-class classification network, there is no need to have
a softmax layer at the end.

Before we end this chapter, let's address a question that might have
come to mind. Is not binary classification a sub-type of the multi-class
setup? Should we not, in that case, arrive at the same result, whatever
the method chosen?

\hypertarget{check-binary-data-multi-class-method}{%
\subsubsection{Check: Binary data, multi-class
method}\label{check-binary-data-multi-class-method}}

Let's see. We re-use the binary-classification scenario employed above.
Here it is again:

\begin{verbatim}
target <- torch_tensor(c(1, 0, 0, 1, 1))

unnormalized_estimate <- 
  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))

probability_estimate <- nnf_sigmoid(unnormalized_estimate)

nnf_binary_cross_entropy(probability_estimate, target)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.64335
[ CPUFloatType{} ]
\end{verbatim}

We hope to get the same value doing things the multi-class way. We
already have the probabilities (namely, \texttt{probability\_estimate});
we just need to put them into the ``observation by class'' format
expected by \texttt{nnf\_nll\_loss()}:

\begin{verbatim}
# logits
multiclass_probability <- torch_tensor(rbind(
  c(1 - 0.9526, 0.9526),
  c(1 - 0.9370, 0.9370),
  c(1 - 0.2315, 0.2315),
  c(1 - 0.9995, 0.9995),
  c(1 - 0.8699, 0.8699)
))
\end{verbatim}

Now, we still want to apply the logarithm. And there is one other thing
to be taken care of: In the binary setup, classes were coded as
probabilities (either 0 or 1); now, we're dealing with indices. This
means we add 1 to the \texttt{target} tensor:

\begin{verbatim}
target <- target + 1
\end{verbatim}

Finally, we can call \texttt{nnf\_nll\_loss()}:

\begin{verbatim}
nnf_nll_loss(
  torch_log(multiclass_probability),
  target$to(dtype = torch_long())
)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.643275
[ CPUFloatType{} ]
\end{verbatim}

There we go. The results are indeed the same.

\hypertarget{sec:optim-2}{%
\chapter{Function minimization with L-BFGS}\label{sec:optim-2}}

Now that we've become acquainted with \texttt{torch} modules and
optimizers, we can go back to the two tasks we already approached
without either: function minimization, and training a neural network.
Again, we start with minimization, and leave the network to the next
chapter.

Thinking back to what we did when minimizing the Rosenbrock function, in
essence it was this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define a tensor to hold the parameter to be optimized, namely, the
  \(\mathbf{x}\)-position where the function attains its minimum.
\item
  Iteratively update the parameter, subtracting a fraction of the
  current gradient.
\end{enumerate}

While as a strategy, this was straightforward, a problem remained: How
big a fraction of the gradient should we subtract? It's exactly here
that optimizers come in useful.

\hypertarget{meet-l-bfgs}{%
\section{\texorpdfstring{Meet
L-BFGS\index{L-BFGS (optimizer)}}{Meet L-BFGS}}\label{meet-l-bfgs}}

So far, we've only talked about the kinds of optimizers often used in
deep learning -- stochastic gradient descent (SGD), SGD with momentum,
and a few classics from the \emph{adaptive} \emph{learning rate} family:
RMSProp, Adadelta, Adagrad, Adam. All these have in common one thing:
They only make use of the \emph{gradient}, that is, the vector of first
derivatives. Accordingly, they are all \emph{first-order} algorithms.
This means, however, that they are missing out on helpful information
provided by the \emph{Hessian}, the matrix of second derivatives.

\hypertarget{changing-slopes}{%
\subsection{Changing slopes}\label{changing-slopes}}

First derivatives tell us about the \emph{slope} of the landscape: Does
it go up? Does it go down? How much so? Going a step further, second
derivatives encode how much that slope \emph{changes}.

Why should that be important?

Assume we're at point \(\mathbf{x}_n\), and have just decided on a
suitable descent direction. We take a step, of length determined by some
pre-chosen learning rate, all set to arrive at point
\(\mathbf{x}_{n+1}\). What we don't know is how the slope will have
changed by the time we'll have gotten there. Maybe it's become much
flatter in the meantime: In this case, we'll have gone way too far,
overshooting and winding up in a far-off area where anything could have
happened in-between (including the slope going \emph{up} again!).

We can illustrate this on a function of a single variable. Take a
parabola, such as

\[
y = 10x^2
\]

Its derivative is \(\frac{dy}{dx} = 20x\). If our current \(x\) is, say,
\(3\), and we work with a learning rate of \(0.1\), we'll subtract
\(20 * 3 * 0.1= 6\), winding up at \(-3\).

But say we had slowed down at \(2\) and inspected the current slope.
We'd have seen that there, the slope was less steep; in fact, when at
that point, we should just have subtracted \(20 * 2 * 0.1= 4\).

By sheer luck, this ``close-your-eyes-and-jump'' strategy can still work
out -- \emph{if} we happen to be using just the right learning rate for
the function in question. (At the chosen learning rate, this would have
been the case for a different parabola, \(y = 5x^2\), for example.) But
wouldn't it make sense to include second derivatives in the decision
from the outset?

Algorithms that do this form the family of Newton methods. First, we
look at their ``purest'' specimen, which best illustrates the principle
but seldom is feasible in practice.

\hypertarget{exact-newton-method}{%
\subsection{Exact Newton method}\label{exact-newton-method}}

In higher dimensions, the exact Newton method multiplies the gradient by
the inverse of the Hessian, thus scaling the descent direction
coordinate-by-coordinate. Our current example has just a single
independent variable; so this means for us: take the first derivative,
and divide by the second.

We now have a scaled gradient -- but what portion of it should we
subtract? In its original version, the exact Newton method does not make
use of a learning rate, thus freeing us of the familiar trial-and-error
game. Let's see, then: In our example, the second derivative is \(20\),
meaning that at \(x=3\) we have to subtract \((20 * 3)/20=3\). Voil, we
end up at \(0\), the location of the minimum, in a single step.

Seeing how that turned out just great, why don't we do it all the time?
For one, it will work perfectly only with quadratic functions, like the
one we chose for the demonstration. In other cases, it, too, will
normally need some ``tuning'', for example, by using a learning rate
here as well.

But the main reason is another one. In more realistic applications, and
certainly in the areas of machine learning and deep learning, computing
the inverse of the Hessian at every step is way too costly. (It may, in
fact, not even be possible.) This is where \emph{approximate}, a.k.a.
\emph{Quasi-Newton}, methods come in.

\hypertarget{approximate-newton-bfgs-and-l-bfgs}{%
\subsection{Approximate Newton: BFGS and
L-BFGS}\label{approximate-newton-bfgs-and-l-bfgs}}

Among approximate Newton methods, probably the most-used is the
\emph{Broyden-Goldfarb-Fletcher-Shanno} algorithm, or \emph{BFGS}.
Instead of continually computing the exact inverse of the Hessian, it
keeps an iteratively-updated approximation of that inverse. BFGS is
often implemented in a more memory-friendly version, referred to as
\emph{Limited-Memory BFGS} (\emph{L-BFGS}). This is the one provided as
part of the core \texttt{torch} optimizers.

Before we get there, though, there is one last conceptual thing to
discuss.

\hypertarget{line-search}{%
\subsection{Line search}\label{line-search}}

Like their exact counterpart, approximate Newton methods can work
without a learning rate. In that case, they compute a descent direction
and follow the scaled gradient as-is. We already talked about how,
depending on the function in question, this can work more or less well.
When it does not, there are two things one could do: Firstly, take small
steps, or put differently, introduce a learning rate. And secondly, do a
\emph{line search}.

With line search, we spend some time evaluating how far to follow the
descent direction. There are two principal ways of doing this.

The first, \emph{exact} line search, involves yet another optimization
problem: Take the current point, compute the descent direction, and
hard-code them as givens in a \emph{second} function that depends on the
learning rate only. Then, differentiate this function to find \emph{its}
minimum. The solution will be the learning rate that optimizes the step
length taken.

The alternative strategy is to do an approximate search. By now, you're
probably not surprised: Just as approximate Newton is more
realistically-feasible than exact Newton, approximate line search is
more practicable than exact line search.

For line search, approximating the best solution means following a set
of proven heuristics. Essentially, we look for something that is
\emph{just} \emph{good enough}. Among the most established heuristics
are the \emph{Strong Wolfe conditions}, and this is the strategy
implemented in \texttt{torch}'s \texttt{optim\_lbfgs()}. In the next
section, we'll see how to use \texttt{optim\_lbfgs()} to minimize the
Rosenbrock function, both with and without line search.

\hypertarget{minimizing-the-rosenbrock-function-with-optim_lbfgs}{%
\section{\texorpdfstring{Minimizing the Rosenbrock function with
\texttt{optim\_lbfgs()}}{Minimizing the Rosenbrock function with optim\_lbfgs()}}\label{minimizing-the-rosenbrock-function-with-optim_lbfgs}}

Here is the Rosenbrock function again:

\begin{verbatim}
library(torch)

a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
\end{verbatim}

In our manual minimization efforts, the procedure was the following. A
one-time action, we first defined the parameter tensor destined to hold
the current \(\mathbf{x}\):

\begin{verbatim}
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
\end{verbatim}

Then, we iteratively executed the following operations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate the function value at the current \(\mathbf{x}\).
\item
  Compute the gradient of that value at the position in question.
\item
  Subtract a fraction of the gradient from the current \(\mathbf{x}\).
\end{enumerate}

How, if so, does that blueprint change?

The first step remains unchanged. We still have

\begin{verbatim}
value <- rosenbrock(x)
\end{verbatim}

The second step stays the same, as well. We still call
\texttt{backward()} directly on the output tensor:

\begin{verbatim}
value$backward()
\end{verbatim}

This is because an optimizer does not \emph{compute} gradients; it
\emph{decides what to do with the gradient} once it's been computed.

What changes, thus, is the third step, the one that also was the most
cumbersome. Now, it is the optimizer that applies the update. To be able
to do that, there is a prerequisite: Prior to starting the loop, the
optimizer will need to be told which parameter it is supposed to work
on. In fact, this is so important that you can't even create an
optimizer without passing it that parameter:

\begin{verbatim}
opt <- optim_lbfgs(x)
\end{verbatim}

In the loop, we now call the \texttt{step()} method on the optimizer
object to update the parameter. There is just one part from our manual
procedure that needs to get carried over to the new way: We still need
to zero out the gradient on each iteration. Just this time, not on the
parameter tensor, \texttt{x}, but the optimizer object itself. \emph{In
principle}, this then yields the following actions to be performed on
each iteration:

\begin{verbatim}
value <- rosenbrock(x)

opt$zero_grad()
value$backward()

opt$step()
\end{verbatim}

Why ``in principle''? In fact, this is what we'd write for every
optimizer \emph{but} \texttt{optim\_lbfgs()}.

For \texttt{optim\_lbfgs()}, \texttt{step()} needs to be called passing
in an anonymous function, a closure. Zeroing of previous gradients,
function call, and gradient calculation, all these happen inside the
closure:

\begin{verbatim}
calc_loss <- function() {
  optimizer$zero_grad()
  value <- rosenbrock(x_star)
  value$backward()
  value
}
\end{verbatim}

Having executed those actions, the closure returns the function value.
Here is how it is called by \texttt{step()}:

\begin{verbatim}
for (i in 1:num_iterations) {
  optimizer$step(calc_loss)
}
\end{verbatim}

Now we put it all together, add some logging output, and compare what
happens with and without line search.

\hypertarget{optim_lbfgs-default-behavior}{%
\subsection{\texorpdfstring{\texttt{optim\_lbfgs()} default
behavior}{optim\_lbfgs() default behavior}}\label{optim_lbfgs-default-behavior}}

As a baseline, we first run without line search. Two iterations are
enough. In the below output, you can see that in each iteration, the
closure is evaluated several times. This is the technical reason we had
to create it in the first place.

\begin{verbatim}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x)

calc_loss <- function() {
  optimizer$zero_grad()

  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
\end{verbatim}

\begin{verbatim}
Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  318.0431 
Value is:  5.146369 
Value is:  4.443705 
Value is:  0.8787204 
Value is:  0.8543001 
Value is:  2.001667 
Value is:  0.5656172 
Value is:  0.400589 
Value is:  7.726219 
Value is:  0.3388008 
Value is:  0.2861604 
Value is:  1.951176 
Value is:  0.2071857 
Value is:  0.150776 
Value is:  0.411357 
Value is:  0.08056168 
Value is:  0.04880721 
Value is:  0.0302862 

Iteration:  2 
Value is:  0.01697086 
Value is:  0.01124081 
Value is:  0.0006622815 
Value is:  3.300996e-05 
Value is:  1.35731e-07 
Value is:  1.111701e-09 
Value is:  4.547474e-12 
\end{verbatim}

To make sure we really have found the minimum, we check \texttt{x}:

\begin{verbatim}
x
\end{verbatim}

\begin{verbatim}
torch_tensor
 1.0000
 1.0000
[ CPUFloatType{2} ]
\end{verbatim}

Can this still be improved upon?

\hypertarget{optim_lbfgs-with-line-search}{%
\subsection{\texorpdfstring{\texttt{optim\_lbfgs()} with line
search}{optim\_lbfgs() with line search}}\label{optim_lbfgs-with-line-search}}

Let's see. Below, the only line that's changed is the one where we
construct the optimizer.

\begin{verbatim}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x, line_search_fn = "strong_wolfe")

calc_loss <- function() {
  optimizer$zero_grad()

  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
\end{verbatim}

\begin{verbatim}
Iteration:  1 
Value is:  4 
Value is:  6 
Value is:  3.802412 
Value is:  3.680712 
Value is:  2.883048 
Value is:  2.5165 
Value is:  2.064779 
Value is:  1.38384 
Value is:  1.073063 
Value is:  0.8844351 
Value is:  0.5554555 
Value is:  0.2501077 
Value is:  0.8948895 
Value is:  0.1619074 
Value is:  0.06823064 
Value is:  0.01653575 
Value is:  0.004060207 
Value is:  0.00353789 
Value is:  0.000391416 
Value is:  4.303527e-06 
Value is:  2.036851e-08 
Value is:  6.870948e-12 

Iteration:  2 
Value is:  6.870948e-12 
\end{verbatim}

With line search, a single iteration is sufficient to reach the minimum.
Inspecting the individual losses, we also see that the algorithm reduces
the loss nearly every time it probes the function, which without line
search, had not been the case.

\hypertarget{sec:network-2}{%
\chapter{Modularizing the neural network}\label{sec:network-2}}

Let's recall the network we built a few chapters ago. Its purpose was
regression, but its method was not \emph{linear}. Instead, an activation
function (ReLU, for ``rectified linear unit'') introduced a
nonlinearity, located between the single hidden layer and the output
layer. The ``layers'', in this original implementation, were just
tensors: weights and biases. You won't be surprised to hear that these
will be replaced by \emph{modules}.

How will the training process change? Conceptually, we can distinguish
four phases: the forward pass, loss computation, backpropagation of
gradients, and weight updating. Let's think about where our new tools
will fit in:

\begin{itemize}
\item
  The forward pass, instead of calling functions on tensors, will call
  the model.
\item
  In computing the loss, we now make use of \texttt{torch}'s
  \texttt{nnf\_mse\_loss()}.
\item
  Backpropagation of gradients is, in fact, the only operation that
  remains unchanged.
\item
  Weight updating is taken care of by the optimizer.
\end{itemize}

Once we've made those changes, the code will be more modular, and a lot
more readable.

\hypertarget{data}{%
\section{Data}\label{data}}

As a prerequisite, we generate the data, same as last time.

\begin{verbatim}
# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)
\end{verbatim}

\hypertarget{network}{%
\section{Network}\label{network}}

With two linear layers connected via ReLU activation, the easiest choice
is a sequential module, very similar to the one we saw in the
introduction to modules:

\begin{verbatim}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)
\end{verbatim}

\hypertarget{training}{%
\section{Training}\label{training}}

Here is the updated training process. We use the Adam optimizer, a
popular choice.

\begin{verbatim}
opt <- optim_adam(net$parameters)

### training loop --------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  y_pred <- net(x)
  
  ### -------- Compute loss -------- 
  loss <- nnf_mse_loss(y_pred, y)
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  opt$zero_grad()
  loss$backward()
  
  ### -------- Update weights -------- 
  opt$step()

}
\end{verbatim}

\begin{verbatim}
Epoch:  10    Loss:  2.549933 
Epoch:  20    Loss:  2.422556 
Epoch:  30    Loss:  2.298053 
Epoch:  40    Loss:  2.173909 
Epoch:  50    Loss:  2.0489 
Epoch:  60    Loss:  1.924003 
Epoch:  70    Loss:  1.800404 
Epoch:  80    Loss:  1.678221 
Epoch:  90    Loss:  1.56143 
Epoch:  100    Loss:  1.453637 
Epoch:  110    Loss:  1.355832 
Epoch:  120    Loss:  1.269234 
Epoch:  130    Loss:  1.195116 
Epoch:  140    Loss:  1.134008 
Epoch:  150    Loss:  1.085828 
Epoch:  160    Loss:  1.048921 
Epoch:  170    Loss:  1.021384 
Epoch:  180    Loss:  1.0011 
Epoch:  190    Loss:  0.9857832 
Epoch:  200    Loss:  0.973796 
\end{verbatim}

In addition to shortening and streamlining the code, our changes have
made a big difference performance-wise.

\hypertarget{whats-to-come}{%
\section{What's to come}\label{whats-to-come}}

You now know a lot about how \texttt{torch} works, and how to use it to
minimize a cost function in various settings: for example, to train a
neural network. But for real-world applications, there is a lot more
\texttt{torch} has to offer. The next -- and most voluminous -- part of
the book focuses on deep learning.

\part{Deep learning with torch}

\hypertarget{sec:dl-overview}{%
\chapter{Overview}\label{sec:dl-overview}}

This part of the book is completely dedicated to applications of deep
learning. There will be two categories of things to dive into: topics
workflow-related, and topics related to domain adaptation.

Regarding workflow, we'll see how to:

\begin{itemize}
\tightlist
\item
  prepare the input data in a form the model can work with;
\item
  effectively and efficiently train a model, monitoring progress and
  adjusting hyper-parameters on the fly;
\item
  save and load models;
\item
  making models generalize beyond the training data;
\item
  speed up training;
\item
  and more.
\end{itemize}

Secondly -- beyond an efficient workflow -- the task in question
matters. Compositions of linear layers, of the type we used to learn
\texttt{torch} in the first part, will not suffice when our goal is to
model images or time series. Successful use of deep learning means
tailoring model architecture to the domain in question. To that end, we
start from concrete tasks, and present applicable architectures directly
by example.

Concretely, the plan is the following. The upcoming two chapters will
introduce you to workflow-related techniques that are indispensable in
practice. You'll encounter another package, \texttt{luz}, that endows
\texttt{torch} with an important layer of abstraction, and significantly
streamlines the workflow. Once you know how to use it, we're all set to
look at a first application: image classification. To improve on our
initial results, we then back up and explore two more advanced
workflow-related topics: how to improve generalization, and how to speed
up training. Equipped with that knowledge, we first return to images,
before extending our domain-related skills to tabular data, time series,
and audio.

\hypertarget{sec:data}{%
\chapter{Loading data}\label{sec:data}}

Our toy example, which we'll see a third (and last) version of in the
next chapter, had the model train on a tiny set of data -- small enough
to pass all observations to the model in one go. What if that wasn't the
case? Say we had 10,000 items instead, and every item was an RGB image
of size 256 x 256 pixels. Even on very powerful hardware, we could not
possibly train a model on the complete data all at once.

For that reason, deep-learning frameworks like \texttt{torch} include an
input pipeline that lets you pass data to the model in \emph{batches} --
that is, subsets of observations. Involved in this process are two
classes: \texttt{dataset()} and \texttt{dataloader()}. Before we look at
how to construct instances of these, let's characterize them by what
they're \emph{for}.

\hypertarget{data-vs.-dataset-vs.-dataloader-whats-the-difference}{%
\section{\texorpdfstring{Data
vs.~\texttt{dataset()}\index{\texttt{dataset()}}
vs.~\texttt{dataloader()}\index{\texttt{dataloader()}} -- what's the
difference?}{Data vs.~dataset() vs.~dataloader() -- what's the difference?}}\label{data-vs.-dataset-vs.-dataloader-whats-the-difference}}

In this book, ``dataset'' (variable-width font, no parentheses), or just
``the data'', usually refers to things like R matrices,
\texttt{data.frame}s, and what's contained therein. A \texttt{dataset()}
(fixed-width font, parentheses), however, is a \texttt{torch} object
that knows how to do one thing: \emph{deliver to the caller a}
\emph{single item.} That item, usually, will be a list, consisting of
one input and one target tensor. (It could be anything, though --
whatever makes sense for the task. For example, it could be a single
tensor, if input and target are the same. Or more than two tensors, in
case different inputs should be passed to different modules.)

As long as it fulfills the above-stated contract, a \texttt{dataset()}
is free to do whatever needs to be done. It could, for example, download
data from the internet, store them in some temporary location, do some
pre-processing, and when asked, return bite-sized chunks of data in just
the shape expected by a certain class of models. No matter what it does
in the background, all its caller cares about is that it return a single
item. Its caller, that's the \texttt{dataloader()}.

A \texttt{dataloader()}'s role is to feed input to the model in
\emph{batches}. One immediate reason is computer memory: Most
\texttt{dataset()}s will be far too large to pass them to the model in
one go. But there are additional benefits to batching. Since gradients
are computed (and model weights updated) once per \emph{batch}, there is
an inherent stochasticity to the process, a stochasticity that helps
with model training. We'll talk more about that in an upcoming chapter.

\hypertarget{using-datasets}{%
\section{\texorpdfstring{Using
\texttt{dataset()}s}{Using dataset()s}}\label{using-datasets}}

\texttt{dataset()}s come in all flavors, from ready-to-use -- and
brought to you by some package, \texttt{torchvision} or
\texttt{torchdatasets}, say, or any package that chooses to provide
access to data in \texttt{torch}-ready form -- to fully customized (made
by you, that is). Creating \texttt{dataset()}s is straightforward, since
they are R6 objects, and there's just three methods to be implemented.
These methods are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{initialize(...)}. Parameters to \texttt{initialize()} are
  passed when a \texttt{dataset()} is instantiated. Possibilities
  include, but are not limited to, references to R \texttt{data.frame}s,
  filesystem paths, download URLs, and any configurations and
  parameterizations expected by the \texttt{dataset()}.
\item
  \texttt{.getitem(i)}. This is the method responsible for fulfilling
  the contract. Whatever it returns counts as a single item. The
  parameter, \texttt{i}, is an index that, in many cases, will be used
  to determine the starting position in the underlying data structure (a
  \texttt{data.frame} of file system paths, for example). However, the
  \texttt{dataset()} is not \emph{obliged} to actually make use of that
  parameter. With extremely huge \texttt{dataset()}s, for example, or
  given serious class imbalance, it could instead decide to return items
  based on \emph{sampling}.
\item
  \texttt{.length()}. This, usually, is a one-liner, its only purpose
  being to inform about the number of available items in a
  \texttt{dataset()}.
\end{enumerate}

Here is a blueprint for creating a \texttt{dataset()}:

\begin{verbatim}
ds <- dataset()(
  initialize = function(...) {
    ...
  },
  .getitem = function(index) {
    ...
  },
  .length = function() {
    ...
  }
)
\end{verbatim}

That said, let's compare three ways of obtaining a \texttt{dataset()} to
work with, from tailor-made to maximally effortless.

\hypertarget{a-self-built-dataset}{%
\subsection{\texorpdfstring{A self-built
\texttt{dataset()}}{A self-built dataset()}}\label{a-self-built-dataset}}

Let's say we wanted to build a classifier based on the popular
\texttt{iris} alternative, \texttt{palmerpenguins}.

\begin{verbatim}
library(torch)
library(palmerpenguins)
library(dplyr)

penguins %>% glimpse()
\end{verbatim}

\begin{verbatim}
$ species           <fct> Adelie, Adelie, Adelie, Adelie,...
$ island            <fct> Torgersen, Torgersen, Torgersen,...
$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3,...
$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6,...
$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181,...
$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650,...
$ sex               <fct> male, female, female, NA, female,...
$ year              <int> 2007, 2007, 2007, 2007, 2007,...
\end{verbatim}

In predicting \texttt{species}, we want to make use of just a subset of
columns: \texttt{bill\_length\_mm}, \texttt{bill\_depth\_mm},
\texttt{flipper\_length\_mm}, and \texttt{body\_mass\_g}. We build a
\texttt{dataset()} that returns exactly what is needed:

\begin{verbatim}
penguins_dataset <- dataset(
  name = "penguins_dataset()",
  initialize = function(df) {
    df <- na.omit(df)
    self$x <- as.matrix(df[, 3:6]) %>% torch_tensor()
    self$y <- torch_tensor(
      as.numeric(df$species)
    )$to(torch_long())
  },
  .getitem = function(i) {
    list(x = self$x[i, ], y = self$y[i])
  },
  .length = function() {
    dim(self$x)[1]
  }
)
\end{verbatim}

Once we've instantiated a \texttt{penguins\_dataset()}, we should
immediately perform some checks. First, does it have the expected
length?

\begin{verbatim}
ds <- penguins_dataset(penguins)
length(ds)
\end{verbatim}

\begin{verbatim}
[1] 333
\end{verbatim}

And second, do individual elements have the expected shape and data
type? Conveniently, we can access \texttt{dataset()} items like tensor
values, through indexing:

\begin{verbatim}
ds[1]
\end{verbatim}

\begin{verbatim}
$x
torch_tensor
   39.1000
   18.7000
  181.0000
 3750.0000
[ CPUFloatType{4} ]

$y
torch_tensor
1
[ CPULongType{} ]
\end{verbatim}

This also works for items ``further down'' in the \texttt{dataset()} --
it has to: When indexing into a \texttt{dataset()}, what happens in the
background is a call to \texttt{.getitem(i)}, passing along the desired
position \texttt{i}.

Truth be told, in this case we didn't really have to build our own
\texttt{dataset()}. With so little pre-processing to be done, there is
an alternative: \texttt{tensor\_dataset()}.

\hypertarget{tensor_dataset}{%
\subsection{\texorpdfstring{\texttt{tensor\_dataset()}}{tensor\_dataset()}}\label{tensor_dataset}}

When you already have a tensor around, or something that's readily
converted to one, you can make use of a built-in \texttt{dataset()}
generator: \texttt{tensor\_dataset()}. This function can be passed any
number of tensors; each batch item then is a list of tensor values:

\begin{verbatim}
three <- tensor_dataset(
  torch_randn(10), torch_randn(10), torch_randn(10)
)
three[1]
\end{verbatim}

\begin{verbatim}
[[1]]
torch_tensor
0.522735
[ CPUFloatType{} ]

[[2]]
torch_tensor
-0.976477
[ CPUFloatType{} ]

[[3]]
torch_tensor
-1.14685
[ CPUFloatType{} ]
\end{verbatim}

In our \texttt{penguins} scenario, we end up with two lines of code:

\begin{verbatim}
penguins <- na.omit(penguins)
ds <- tensor_dataset(
  torch_tensor(as.matrix(penguins[, 3:6])),
  torch_tensor(
    as.numeric(penguins$species)
  )$to(torch_long())
)

ds[1]
\end{verbatim}

Admittedly though, we have not made use of all the dataset's columns.
The more pre-processing you need a \texttt{dataset()} to do, the more
likely you are to want to code your own.

Thirdly and finally, here is the most effortless possible way.

\hypertarget{torchvisionmnist_dataset}{%
\subsection{\texorpdfstring{\texttt{torchvision::mnist\_dataset()}}{torchvision::mnist\_dataset()}}\label{torchvisionmnist_dataset}}

When you're working with packages in the \texttt{torch} ecosystem,
chances are that they already include some \texttt{dataset()}s, be it
for demonstration purposes or for the sake of the data themselves.
\texttt{torchvision}, for example, packages a number of classic image
datasets -- among those, that archetype of archetypes, MNIST.

Since we're going to talk about image processing in a later chapter, I
won't comment on the arguments to \texttt{mnist\_dataset()} here; we do,
however, include a quick check that the data delivered conform to what
we'd expect:

\begin{verbatim}
library(torchvision)

dir <- "~/.torch-datasets"

ds <- mnist_dataset(
  root = dir,
  train = TRUE, # default
  download = TRUE,
  transform = function(x) {
    x %>% transform_to_tensor() 
  }
)

first <- ds[1]
cat("Image shape: ", first$x$shape, " Label: ", first$y, "\n")
\end{verbatim}

\begin{verbatim}
Image shape:  1 28 28  Label:  6 
\end{verbatim}

At this point, that is all we need to know about \texttt{dataset()}s --
we'll encounter plenty of them in the course of this book. Now, we move
on from the one to the many.

\hypertarget{using-dataloaders}{%
\section{\texorpdfstring{Using
\texttt{dataloader()}s}{Using dataloader()s}}\label{using-dataloaders}}

Continuing to work with the newly created MNIST \texttt{dataset()}, we
instantiate a \texttt{dataloader()} for it. The \texttt{dataloader()}
will deliver pairs of images and labels in batches: thirty-two at a
time. In every epoch, it will return them in different order
(\texttt{shuffle\ =\ TRUE}):

\begin{verbatim}
dl <- dataloader(ds, batch_size = 32, shuffle = TRUE)
\end{verbatim}

Just like \texttt{dataset()}s, \texttt{dataloader()}s can be queried
about their length:

\begin{verbatim}
length(dl)
\end{verbatim}

\begin{verbatim}
[1] 1875
\end{verbatim}

This time, though, the returned value is not the number of items; it is
the number of batches.

To loop over batches, we first obtain an iterator, an object that knows
how to traverse the elements in this \texttt{dataloader()}. Calling
\texttt{dataloader\_next()}, we can then access successive batches, one
by one:

\begin{verbatim}
first_batch <- dl %>%
  # obtain an iterator for this dataloader
  dataloader_make_iter() %>% 
  dataloader_next()

dim(first_batch$x)
dim(first_batch$y)
\end{verbatim}

\begin{verbatim}
[1] 32  1 28 28
[1] 32
\end{verbatim}

If you compare the batch shape of \texttt{x} -- the image part -- with
the shape of an individual image (as inspected above), you see that now,
there is an additional dimension in front, reflecting the number of
images in a batch.

The next step is passing the batches to a model. This -- in fact, this
as well as the complete, end-to-end deep-learning workflow -- is what
the next chapter is about.

\hypertarget{sec:luz}{%
\chapter{Training with luz}\label{sec:luz}}

At this point in the book, you know how to train a neural network. Truth
be told, though, there's some cognitive effort involved in having to
remember the right execution order of steps like
\texttt{optimizer\$zero\_grad()}, \texttt{loss\$backward()}, and
\texttt{optimizer\$step()}. Also, in more complex scenarios than our
running example, the list of things to actively remember gets longer.

One thing we haven't talked about yet, for example, is how to handle the
usual three stages of machine learning: training, validation, and
testing. Another is the question of data flow between \emph{devices}
(CPU and GPU, if you have one). Both topics necessitate additional code
to be introduced to the training loop. Writing this code can be tedious,
and creates a potential for mistakes.

You can see exactly what I'm referring to in the appendix at the end of
this chapter. But now, I want to focus on the remedy: a high-level,
easy-to-use, concise way of organizing and instrumenting the training
process, contributed by a package built on top of \texttt{torch}:
\texttt{luz}.

\hypertarget{que-haya-luz---que-haja-luz---let-there-be-light}{%
\section{Que haya luz - Que haja luz - Let there be
light}\label{que-haya-luz---que-haja-luz---let-there-be-light}}

A \emph{torch} already brings some light, but sometimes in life, there
is no \emph{too bright}. \texttt{luz} was designed to make deep learning
with \texttt{torch} as effortless as possible, while at the same time
allowing for easy customization. In this chapter, we focus on the
overall process; examples of customization will appear in later
chapters.

For ease of comparison, we take our running example, and add a third
version, now using \texttt{luz}. First, we ``just'' directly port the
example; then, we adapt it to a more realistic scenario. In that
scenario, we

\begin{itemize}
\item
  make use of separate training, validation, and test sets;
\item
  have \texttt{luz} compute \emph{metrics} during training/validation;
\item
  illustrate the use of \emph{callbacks} to perform custom actions or
  dynamically change hyper-parameters during training; and
\item
  explain what is going on with the aforementioned \emph{devices}.
\end{itemize}

\hypertarget{porting-the-toy-example}{%
\section{Porting the toy example}\label{porting-the-toy-example}}

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

\texttt{luz} does not just substantially transform the code required to
train a neural network; it also adds flexibility on the data side of
things. In addition to a reference to a \texttt{dataloader()}, its
\texttt{fit()} method accepts \texttt{dataset()}s, tensors, and even R
objects, as we'll be able to verify soon.

We start by generating an R matrix and a vector, as before. This time
though, we also wrap them in a \texttt{tensor\_dataset()}, and
instantiate a \texttt{dataloader()}. Instead of just 100, we now
generate 1000 observations.

\begin{verbatim}
library(torch)
library(luz)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)
\end{verbatim}

\hypertarget{model}{%
\subsection{Model}\label{model}}

To use \texttt{luz}, no changes are needed to the model definition.
Note, though, that we just \emph{define} the model architecture; we
never actually \emph{instantiate} a model object ourselves.

\begin{verbatim}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
\end{verbatim}

\hypertarget{training-1}{%
\subsection{Training}\label{training-1}}

To train the model, we don't write loops anymore. \texttt{luz} replaces
the familiar \emph{iterative} style by a \emph{declarative} one: You
tell \texttt{luz} what you want to happen, and like a docile sorcerer's
apprentice, it sets in motion the machinery.

Concretely, instruction happens in two -- required -- calls.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \texttt{setup()}\index{\texttt{setup()} (luz)}, you specify the
  loss function and the optimizer to use.
\item
  In \texttt{fit()}\index{\texttt{fit()} (luz)}, you pass reference(s)
  to the training (and optionally, validation) data, as well as the
  number of epochs to train for.
\end{enumerate}

If the model is configurable -- meaning, it accepts arguments to
\texttt{initialize()} -- a third method comes into play:
\texttt{set\_hparams()}\index{\texttt{set{\textunderscore}hparams()} (luz)},
to be called in-between the other two. (That's \texttt{hparams} for
hyper-parameters.) Using this mechanism, you can easily experiment with,
for example, different layer sizes, or other factors suspected to affect
performance.

\begin{verbatim}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(dl, epochs = 200)
\end{verbatim}

Running this code, you should see output approximately like this:

\begin{verbatim}
Epoch 1/200
Train metrics: Loss: 3.0343                                                                               
Epoch 2/200
Train metrics: Loss: 2.5387                                                                               
Epoch 3/200
Train metrics: Loss: 2.2758                                                                               
...
...
Epoch 198/200
Train metrics: Loss: 0.891                                                                                
Epoch 199/200
Train metrics: Loss: 0.8879                                                                               
Epoch 200/200
Train metrics: Loss: 0.9036 
\end{verbatim}

Above, what we passed to \texttt{fit()} was the \texttt{dataloader()}.
Let's check that referencing the \texttt{dataset()} would have been just
as fine:

\begin{verbatim}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(ds, epochs = 200)
\end{verbatim}

Or even, \texttt{torch} tensors:

\begin{verbatim}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(x, y), epochs = 200)
\end{verbatim}

And finally, R objects, which can be convenient when we aren't already
working with tensors.

\begin{verbatim}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(as.matrix(x), as.matrix(y)), epochs = 200)
\end{verbatim}

In the following sections, we'll always be working with
\texttt{dataloader()}s; but in some cases those ``shortcuts'' may come
in handy.

Next, we extend the toy example, illustrating how to address more
complex requirements.

\hypertarget{a-more-realistic-scenario}{%
\section{A more realistic scenario}\label{a-more-realistic-scenario}}

\hypertarget{integrating-training-validation-and-test}{%
\subsection{Integrating training, validation, and
test}\label{integrating-training-validation-and-test}}

In deep learning, training and validation phases are interleaved. Every
epoch of training is followed by an epoch of validation. Importantly,
the data used in both phases have to be strictly disjoint.

In each training phase, gradients are computed and weights are changed;
during validation, none of that happens. Why have a validation set,
then? If, for each epoch, we compute task-relevant metrics for both
partitions, we can see if we are \emph{overfitting} to the training
data: that is, drawing conclusions based on training sample specifics
not descriptive of the overall population we want to model. All we have
to do is two things: instruct \texttt{luz} to compute a suitable metric,
and pass it an additional \texttt{dataloader} pointing to the validation
data.

The former is done in \texttt{setup()}, and for a regression task,
common choices are mean squared or mean absolute error (MSE or MAE,
resp.). As we're already using MSE as our loss, let's choose MAE for a
metric:

\begin{verbatim}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  fit(...)
\end{verbatim}

The validation \texttt{dataloader} is passed in \texttt{fit()} -- but to
be able to reference it, we need to construct it first! So now
(anticipating we'll want to have a test set, too), we split up the
original 1000 observations into three partitions, creating a
\texttt{dataset} and a \texttt{dataloader} for each of them.

\begin{verbatim}
train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(
  setdiff(1:length(ds), train_ids),
  size = 0.2 * length(ds)
)
test_ids <- setdiff(
  1:length(ds),
  union(train_ids, valid_ids)
)

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100, shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)
\end{verbatim}

Now, we are ready to start the enhanced workflow:

\begin{verbatim}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 200, valid_data = valid_dl)
\end{verbatim}

\begin{verbatim}
Epoch 1/200
Train metrics: Loss: 2.5863 - MAE: 1.2832                                       
Valid metrics: Loss: 2.487 - MAE: 1.2365
Epoch 2/200
Train metrics: Loss: 2.4943 - MAE: 1.26                                          
Valid metrics: Loss: 2.4049 - MAE: 1.2161
Epoch 3/200
Train metrics: Loss: 2.4036 - MAE: 1.236                                         
Valid metrics: Loss: 2.3261 - MAE: 1.1962
...
...
Epoch 198/200
Train metrics: Loss: 0.8947 - MAE: 0.7504
Valid metrics: Loss: 1.0572 - MAE: 0.8287
Epoch 199/200
Train metrics: Loss: 0.8948 - MAE: 0.7503
Valid metrics: Loss: 1.0569 - MAE: 0.8286
Epoch 200/200
Train metrics: Loss: 0.8944 - MAE: 0.75
Valid metrics: Loss: 1.0579 - MAE: 0.8292
\end{verbatim}

Even though both training and validation sets come from the exact same
distribution, we do see a bit of overfitting. This is a topic we'll talk
about more in the next chapter.

Once training has finished, the \texttt{fitted} object above holds a
history of epoch-wise metrics, as well as references to a number of
important objects involved in the training process. Among the latter is
the fitted model itself -- which enables an easy way to obtain
predictions on the test set:\index{\texttt{predict()} (luz)}

\begin{verbatim}
fitted %>% predict(test_dl)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0.7799
 1.7839
-1.1294
-1.3002
-1.8169
-1.6762
-0.7548
-1.2041
 2.9613
-0.9551
 0.7714
-0.8265
 1.1334
-2.8406
-1.1679
 0.8350
 2.0134
 2.1083
 1.4093
 0.6962
-0.3669
-0.5292
 2.0310
-0.5814
 2.7494
 0.7855
-0.5263
-1.1257
-3.3117
 0.6157
... [the output was truncated (use n=-1 to disable)]
[ CPUFloatType{200,1} ]
\end{verbatim}

We also want to evaluate performance on the test
set:\index{\texttt{evaluate()} (luz)}

\begin{verbatim}
fitted %>% evaluate(test_dl)
\end{verbatim}

\begin{verbatim}
A `luz_module_evaluation`
 Results 
loss: 0.9271
mae: 0.7348
\end{verbatim}

This workflow of: training and validation in lock-step, then checking
and extracting predictions on the test set is something we'll encounter
times and again in this book.

\hypertarget{using-callbacks-to-hook-into-the-training-process}{%
\subsection{\texorpdfstring{Using callbacks to ``hook'' into the
training
process\index{callbacks (luz)}}{Using callbacks to ``hook'' into the training process}}\label{using-callbacks-to-hook-into-the-training-process}}

At this point, you may feel that what we've gained in code efficiency,
we may have lost in flexibility. Coding the training loop yourself, you
can arrange for all kinds of things to happen: save model weights,
adjust the learning rate \ldots{} whatever you need.

In reality, no flexibility is lost. Instead, \texttt{luz} offers a
standardized way to achieve the same goals: callbacks. Callbacks are
objects that can execute arbitrary R code, at any of the following
points in time:

\begin{itemize}
\item
  when the overall training process starts or ends
  (\texttt{on\_fit\_begin()} / \texttt{on\_fit\_end()});
\item
  when an epoch (comprising training and validation) starts or ends
  (\texttt{on\_epoch\_begin()} / \texttt{on\_epoch\_end()});
\item
  when during an epoch, the training (validation, resp.) phase starts or
  ends (\texttt{on\_train\_begin()} / \texttt{on\_train\_end()};
  \texttt{on\_valid\_begin()} / \texttt{on\_valid\_end()});
\item
  when during training (validation, resp.), a new batch is either about
  to be or has been processed (\texttt{on\_train\_batch\_begin()} /
  \texttt{on\_train\_batch\_end()}; \texttt{on\_valid\_batch\_begin()} /
  \texttt{on\_valid\_batch\_end()});
\item
  and even at specific landmarks inside the ``innermost'' training /
  validation logic, such as ``after loss computation'', ``after
  \texttt{backward()}'' or ``after \texttt{step()}''.
\end{itemize}

While you can implement any logic you wish using callbacks (and we'll
see how to do this in a later chapter), \texttt{luz} already comes
equipped with a very useful set. For example:

\begin{itemize}
\item
  \texttt{luz\_callback\_model\_checkpoint()} saves model weights after
  every epoch (or just in case of improvements, if so instructed).
\item
  \texttt{luz\_callback\_lr\_scheduler()} activates one of
  \texttt{torch}'s \emph{learning rate schedulers}. Different scheduler
  objects exist, each following their own logic in dynamically updating
  the learning rate.
\item
  \texttt{luz\_callback\_early\_stopping()} terminates training once
  model performance stops to improve. What exactly ``stops to improve''
  should mean is configurable by the user.
\end{itemize}

Callbacks are passed to the \texttt{fit()} method in a list. For
example, augmenting our most recent workflow:

\begin{verbatim}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(d_in = d_in,
              d_hidden = d_hidden,
              d_out = d_out) %>%
  fit(
    train_dl,
    epochs = 200,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_model_checkpoint(path = "./models/",
                                    save_best_only = TRUE),
      luz_callback_early_stopping(patience = 10)
    )
  )
\end{verbatim}

With this configuration, weights will be saved, but only if validation
loss decreases. Training will halt if there is no improvement (again, in
validation loss) for ten epochs. With both callbacks, you can pick any
other metric to base the decision on, and the metric in question may
also refer to the training set.

Here, we see early stopping happening after 111 epochs:

\begin{verbatim}
Epoch 1/200
Train metrics: Loss: 2.5803 - MAE: 1.2547
Valid metrics: Loss: 3.3763 - MAE: 1.4232
Epoch 2/200
Train metrics: Loss: 2.4767 - MAE: 1.229
Valid metrics: Loss: 3.2334 - MAE: 1.3909
...
...
Epoch 110/200
Train metrics: Loss: 1.011 - MAE: 0.8034
Valid metrics: Loss: 1.1673 - MAE: 0.8578
Epoch 111/200
Train metrics: Loss: 1.0108 - MAE: 0.8032
Valid metrics: Loss: 1.167 - MAE: 0.8578
Early stopping at epoch 111 of 200
\end{verbatim}

\hypertarget{how-luz-helps-with-devices}{%
\subsection{\texorpdfstring{How \texttt{luz} helps with
devices\index{device handling (luz)}}{How luz helps with devices}}\label{how-luz-helps-with-devices}}

Finally, let's quickly mention how \texttt{luz} helps with device
placement. Devices, in a usual environment, are the CPU and perhaps, if
available, a GPU. For training, data and model weights need to be
located on the same device. This can introduce complexities, and -- at
the very least -- necessitates additional code to keep all pieces in
sync.

With \texttt{luz}, related actions happen transparently to the user.
Let's take the prediction step from above:

\begin{verbatim}
fitted %>% predict(test_dl)
\end{verbatim}

In case this code was executed on a machine that has a GPU, \texttt{luz}
will have detected that, and the model's weight tensors will already
have been moved there. Now, for the above call to \texttt{predict()},
what happened ``under the hood'' was the following:

\begin{itemize}
\tightlist
\item
  \texttt{luz} put the model in evaluation mode, making sure that
  weights are not updated.
\item
  \texttt{luz} moved the test data to the GPU, batch by batch, and
  obtained model predictions.
\item
  These predictions were then moved back to the CPU, in anticipation of
  the caller wanting to process them further with R. (Conversion
  functions like \texttt{as.numeric()}, \texttt{as.matrix()} etc. can
  only act on CPU-resident tensors.)
\end{itemize}

In the below appendix, you find a complete walk-through of how to
implement the train-validate-test workflow by hand. You'll likely find
this a lot more complex than what we did above -- and it does not even
bring into play metrics, or any of the functionality afforded by
\texttt{luz} callbacks.

In the next chapter, we discuss essential ingredients of modern deep
learning we haven't yet touched upon; and following that, we look at
specific architectures destined to specifically handle different tasks
and domains.

\hypertarget{appendix-a-train-validate-test-workflow-implemented-by-hand}{%
\section{Appendix: A train-validate-test workflow implemented by
hand}\label{appendix-a-train-validate-test-workflow-implemented-by-hand}}

For clarity, we repeat here the two things that do \emph{not} depend on
whether you're using \texttt{luz} or not: \texttt{dataloader()}
preparation and model definition.

\begin{verbatim}
# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)

train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(setdiff(
  1:length(ds),
  train_ids
), size = 0.2 * length(ds))
test_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)

# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
\end{verbatim}

Recall that with \texttt{luz}, now all that separates you from watching
how training and validation losses evolve is a snippet like this:

\begin{verbatim}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 200, valid_data = valid_dl)
\end{verbatim}

Without \texttt{luz}, however, things to be taken care of fall into
three distinct categories.

First, instantiate the network, and, if CUDA is installed, move its
weights to the GPU.

\begin{verbatim}
device <- torch_device(if
(cuda_is_available()) {
  "cuda"
} else {
  "cpu"
})

model <- net(d_in = d_in, d_hidden = d_hidden, d_out = d_out)
model <- model$to(device = device)
\end{verbatim}

Second, create an optimizer.

\begin{verbatim}
optimizer <- optim_adam(model$parameters)
\end{verbatim}

And third, the biggest chunk: In each epoch, iterate over training
batches as well as validation batches, performing backpropagation when
working on the former, while just passively reporting losses when
processing the latter.

For clarity, we pack training logic and validation logic each into their
own functions. \texttt{train\_batch()} and \texttt{valid\_batch()} will
be called from inside loops over the respective batches. Those loops, in
turn, will be executed for every epoch.

While \texttt{train\_batch()} and \texttt{valid\_batch()}, per se,
trigger the usual actions in the usual order, note the device placement
calls: For the model to be able to take in the data, they have to live
on the same device. Then, for mean-squared-error computation to be
possible, the target tensors need to live there as well.

\begin{verbatim}
train_batch <- function(b) {
  optimizer$zero_grad()
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)

  loss <- nn_mse_loss(output, target)
  loss$backward()
  optimizer$step()

  loss$item()
}

valid_batch <- function(b) {
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)

  loss <- nn_mse_loss(output, target)
  loss$item()
}
\end{verbatim}

The loop over epochs contains two lines that deserve special attention:
\texttt{model\$train()} and \texttt{model\$eval()}. The former instructs
\texttt{torch} to put the model in training mode; the latter does the
opposite. With the simple model we're using here, it wouldn't be a
problem if you forgot those calls; however, when later we'll be using
regularization layers like \texttt{nn\_dropout()} and
\texttt{nn\_batch\_norm2d()}, calling these methods in the correct
places is essential. This is because these layers behave differently
during evaluation and training.

\begin{verbatim}
num_epochs <- 200

for (epoch in 1:num_epochs) {
  model$train()
  train_loss <- c()

  # use coro::loop() for stability and performance
  coro::loop(for (b in train_dl) {
    loss <- train_batch(b)
    train_loss <- c(train_loss, loss)
  })

  cat(sprintf(
    "\nEpoch %d, training: loss: %3.5f \n",
    epoch, mean(train_loss)
  ))

  model$eval()
  valid_loss <- c()

  # disable gradient tracking to reduce memory usage
  with_no_grad({ 
    coro::loop(for (b in valid_dl) {
      loss <- valid_batch(b)
      valid_loss <- c(valid_loss, loss)
    })  
  })
  
  cat(sprintf(
    "\nEpoch %d, validation: loss: %3.5f \n",
    epoch, mean(valid_loss)
  ))
}
\end{verbatim}

This completes our walk-through of manual training, and should have made
more concrete my assertion that using \texttt{luz} significantly reduces
the potential for casual (e.g., copy-paste) errors.

\hypertarget{sec:image-classification-1}{%
\chapter{A first go at image
classification}\label{sec:image-classification-1}}

\hypertarget{what-does-it-take-to-classify-an-image}{%
\section{What does it take to classify an
image?}\label{what-does-it-take-to-classify-an-image}}

Think about how we, as human beings, can say ``that's a cat'', or:
``this is a dog''. No conscious processing is required. (Usually, that
is.)

Why? The neuroscience, and cognitive psychology, involved are definitely
out of scope for this book; but on a high level, we can assume that
there are at least two prerequisites: First, that our visual system be
able to build up complex representations out of lower-level ones, and
second, that we have a set of concepts available we can map those
high-level representations to. Presumably, then, an algorithm expected
to do the same thing needs to be endowed with these same capabilities.

In the context of this chapter, dedicated to image classification, the
second prerequisite is satisfied gratuitously. Classification being a
variant of supervised machine learning, the concepts are given by means
of the targets. The first, however, is all-important. We can again
distinguish two components: the capability to detect low-level features,
and that to successively compose them into higher-level ones.

Take a simple example. What would be required to identify a rectangle? A
rectangle consists of edges: straight-ish borders of sort where
something in the visual impression (color, for example) changes. To
start with, then, the algorithm would have to be able to identify a
single edge. That ``edge extractor'', as we might call it, is going to
mark all four edges in the image. In this case, no further composition
of features is needed; we can directly infer the concept.

On the other hand, assume the image were showing a house built of
bricks. Then, there would be many rectangles, together forming a wall of
the house; another rectangle, the door; and a few further ones, the
windows. Maybe there'd be a different arrangement of edges,
triangle-shaped, the roof. Meaning, an edge detector is not enough: We
also need a ``rectangle detector'', a ``triangle detector'', a ``wall
detector'', a ``roof detector'' \ldots{} and so on. Evidently, these
detectors can't all be programmed up front. They'll have to be emergent
properties of the algorithm: the neural network, in our case.

\hypertarget{neural-networks-for-feature-detection-and-feature-emergence}{%
\section{\texorpdfstring{Neural networks for feature
detection\index{feature detection} and feature
emergence}{Neural networks for feature detection and feature emergence}}\label{neural-networks-for-feature-detection-and-feature-emergence}}

The way we've spelled out the requirements, a neural network for image
classification needs to (1) be able to detect features, and (2) build up
a hierarchy of such. Networks being networks, we can safely assume that
(1) will be taken care of by a specialized layer (module), while (2)
will be made possible by chaining several layers.

\hypertarget{detecting-low-level-features-with-cross-correlation}{%
\subsection{\texorpdfstring{Detecting low-level features with
cross-correlation\index{cross-correlation}}{Detecting low-level features with cross-correlation}}\label{detecting-low-level-features-with-cross-correlation}}

This chapter is about ``convolutional'' neural networks; the specialized
module in question is the ``convolutional'' one. Why, then, am I talking
about cross-correlation? It's because what neural-network people refer
to as \emph{convolution}\index{convolution}, technically is
\emph{cross-correlation}. (Don't worry -- I'll be making the distinction
just here, in the conceptual introduction; afterwards I'll be saying
``convolution'', just like everyone else.)

So why am I insisting? It is for two reasons. First, this book actually
\emph{has} a chapter on convolution -- the ``real one''; it figures in
part three right between matrix operations and the Discrete Fourier
Transform. Second, while in a formal sense the difference may be small,
semantically as well as in terms of mathematical status, convolution and
cross-correlation are decidedly distinct. In broad strokes:

Convolution may well be the most fundamental operation in all of signal
processing, fundamental in the way addition and multiplication are. It
can act as a \emph{filter}, a signal-space transformation intended to
achieve a desired result. For example, a moving average filter can be
programmed as a convolution. So can, however, something quite the
opposite: a filter that emphasizes differences. (An edge enhancer would
be an example of the latter.)

Cross-correlation, in contrast, is more specialized. It \emph{finds}
things, or put differently: It spots similarities. This is what is
needed in image recognition. To demonstrate how it works, we start in a
single dimension.

\hypertarget{cross-correlation-in-one-dimension}{%
\subsubsection{Cross-correlation in one
dimension}\label{cross-correlation-in-one-dimension}}

Assume we have a signal -- a univariate time series -- that looks like
this: \texttt{0,1,1,1,-1,0,-1,-1,1,1,1,-1}. We want to find locations
where a \emph{one} occurs three times in a row. To that end, we make use
of a filter that, too, has three ones in a row: \texttt{1,1,1}.

That filter, also called a \emph{kernel}, is going to slide over the
input sequence, producing an output value at every location. To be
precise: The output value in question will be mapped to the \emph{input
value co-located with the kernel's central value}. How, then, can we
obtain an output for the very first input value, which has no way of
being mapped to the center of the kernel? In order for this to work, the
input sequence is padded with zeroes: one in front, and one at the end.
The new signal looks like this: \texttt{0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0}
.

Now, we have the kernel sliding over the signal. Like so:

\begin{verbatim}
0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
   1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
      1,1, 1
\end{verbatim}

And so on.

At every position, products are computed between the mapped input and
kernel values, and then, those products are added up, to yield the
output value at the central position. For example, this is what gets
computed at the very first matching: \texttt{0*1\ +\ 0*1\ +\ 1*1\ =\ 1}.
Appending the outputs, we get a new sequence:
\texttt{1,2,3,1,0,-2,-2,-1,1,3,1,0} .

How does this help in finding three consecutive ones? Well, a three can
only result when the kernel has found such a location. Thus, with that
choice of kernel, we take every occurrence of \texttt{3} in the output
as the center of the target sequence we're looking for.

\hypertarget{cross-correlation-in-two-dimensions}{%
\subsubsection{Cross-correlation in two
dimensions}\label{cross-correlation-in-two-dimensions}}

This chapter is about images; how does that logic carry over to two
dimensions?

Everything works just the same; it's just that now, the input signal
extends over two dimensions, and the kernel is two-dimensional, as well.
Again, the input is padded; with a kernel of size 3 x 3, for example,
one row is added on top and bottom each, and one column, on the left and
the right. Again, the kernel slides over the image, row by row and
column by column. At each point it computes an aggregate that is the sum
of point-wise products. Mathematically, that's a \emph{dot product}.

To get a feel for how this works, we look at a bare-bones example: a
white square on black background (fig.~\ref{fig-images-square}).

\begin{figure}[H]

{\centering \includegraphics{images/images-square.png}

}

\caption{\label{fig-images-square}White square on black background.}

\end{figure}

Nicely, the open-source graphics program Gimp has a feature that allows
one to experiment with custom filters (``Filters'' -\textgreater{}
``Custom'' -\textgreater{} ``Convolution matrix''). We can construct
kernels and directly examine their effects.

Say we want to find the left edge of the square. We are looking for
locations where the color changes, horizontally, from black to white.
This can be achieved with a 3x3 kernel that looks like this
(fig.~\ref{fig-images-square-left}):

\begin{verbatim}
 0 0 0
-1 1 0
 0 0 0
\end{verbatim}

This kernel is \emph{similar} to the edge type we're interested in in
that it has, in the second row, a horizontal transition from -1 to 1.

Analogously, kernels can be constructed that extract the right
(fig.~\ref{fig-images-square-right}), top
(fig.~\ref{fig-images-square-top}), and bottom
(fig.~\ref{fig-images-square-bottom}) edges.

\begin{figure}[H]

{\centering \includegraphics{images/images-square-left.png}

}

\caption{\label{fig-images-square-left}Gimp convolution matrix that
detects the left edge.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{images/images-square-right.png}

}

\caption{\label{fig-images-square-right}Gimp convolution matrix that
detects the right edge.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{images/images-square-top.png}

}

\caption{\label{fig-images-square-top}Gimp convolution matrix that
detects the top edge.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{images/images-square-bottom.png}

}

\caption{\label{fig-images-square-bottom}Gimp convolution matrix that
detects the bottom edge.}

\end{figure}

To understand this numerically, we can simulate a tiny image
(fig.~\ref{fig-images-cross-correlation}, left). The numbers represent a
grayscale image with values ranging from 0 to 255. To its right, we have
the kernel; this is the one we used to detect the left edge. As a result
of having that kernel slide over the image, we obtain the ``image'' on
the right. \texttt{0} being the lowest possible value, negative pixels
end up black, and we obtain a white edge on black background, just like
we saw with Gimp.

\begin{figure}[H]

{\centering \includegraphics{images/images-cross-correlation.png}

}

\caption{\label{fig-images-cross-correlation}Input image, filter, and
result as pixel values. Negative pixel values being impossible, -255
will end up as 0.}

\end{figure}

Now, we've talked a lot about constructing kernels. Neural networks are
all about \emph{learning} feature detectors, not having them programmed
up-front. Naturally, then, learning a filter means having a layer type
whose weights embody this logic.

\hypertarget{convolutional-layers-in-torch}{%
\subsubsection{\texorpdfstring{Convolutional layers in
\texttt{torch}}{Convolutional layers in torch}}\label{convolutional-layers-in-torch}}

So far, the only layer type we've seen that learns weights is
\texttt{nn\_linear()}. \texttt{nn\_linear()} performs an affine
operation: It takes an input tensor, matrix-multiplies it by its weight
matrix \(\mathbf{W}\), and adds the bias vector \(\mathbf{b}\). While
there is just a single bias per layer, independently of the number of
neurons it has, this is not the case for the weights: There is a unique
connection between each feature in the input tensor and each of the
layer's neurons.

This is not true for \texttt{nn\_conv2d()}, \texttt{torch}'s
(two-dimensional) convolution\footnote{Like I said above, I'll be using
  the established term ``convolution'' from now on. Actually -- given
  that weights are \emph{learned --} it does not matter that much
  anyway.} layer.

Back to how convolutional layers differ from linear ones. We've already
seen what the layer's effect is supposed to be: A \emph{kernel} should
slide over its input, generating an output value at each location. Now
the kernel, for a convolutional layer, is exactly its weight matrix. The
kernel sliding over an input image means that weights are re-used every
time it shifts its position. Thus, the number of weights is determined
by the size of the kernel, not the size of the input. As a consequence,
a convolutional layer is way more economical than a linear one.

Another way to express this is the following.

Conceptually, we are looking for the same thing, wherever it appears in
the image. Take the most standard of standard image classification
benchmarks, MNIST. It is about classifying images of the Arabic numerals
0-9. Say we want to learn the shape for a 2. The 2 could be right in the
middle of the image, or it could be shifted to the left (say). An
algorithm should be able to recognize it no matter where. Additional
requirements depend on the task. If all we need to be able to do is say
``that's a 2'', we're good to use an algorithm that is
\emph{translation-invariant}\index{invariance (translational)}: It
outputs the same thing independently of any shifts that may have
occurred. For classification, that's just fine: A 2 is a 2 is a 2.

Another important task, though, is image segmentation (something we'll
look at in an upcoming chapter). In segmentation, we want to mark all
pixels in an image according to whether they are part of some object or
not. Think tumor cells, for example. The 2 is still a 2, but we do need
the information where in the image it is located. The algorithm to use
now has to be
\emph{translation-equivariant}\index{equivariance (translational)}: If a
shift has occurred, the target is still detected, but at a new location.
And thinking about the convolution algorithm, translation-equivariant is
exactly what it is.

So now, we have an idea how \texttt{torch} lets us detect individual
features in an image. This gives us the first in our list of
desiderates. The second is about combining feature detectors, that is,
building up a hierarchy, in order to discern more and more specialized
types of objects. This means that from a single layer, we move on to a
network of layers.

\hypertarget{build-up-feature-hierarchies}{%
\subsection{\texorpdfstring{Build up feature
hierarchies\index{feature hierarchies}}{Build up feature hierarchies}}\label{build-up-feature-hierarchies}}

A prototypical convolutional neural network for image classification
will chain blocks composed of three types of layers: convolutional ones
(\texttt{nn\_conv1d()}, \texttt{nn\_conv2d()}, or \texttt{nn\_conv3d()},
depending on the dimension we're in), activation layers (e.g.,
\texttt{nn\_relu()}), and pooling layers (e.g.,
\texttt{nn\_maxpool1d()}, \texttt{nn\_maxpool2d()},
\texttt{nn\_maxpool3d()}).

The only type we haven't talked about yet are the pooling\index{pooling}
layers. Just like activation layers, these don't have any parameters;
what they do is aggregate neighboring tensor values. The size of the
region to summarize is specified in the layer constructor's parameters.
Various types of aggregation are available:
\texttt{nn\_maxpool\textless{}n\textgreater{}d()} picks the highest
value, while \texttt{nn\_avg\_pool\textless{}n\textgreater{}d()}
computes the average.

Why would one want to perform these kinds of aggregation? Practically
speaking, one \emph{has to} if one wants to arrive at a per-image (as
opposed to per-pixel) output. But we can't just choose \emph{any} way of
aggregating spatially-arranged values. Picture, for example, an average
where the interior pixels of an image patch were weighted higher than
the exterior ones. Then, it would make a difference where in the patch
some object was located. But for classification, this should not be the
case. For classification, as opposed to segmentation, we want
translation \emph{invariance} -- not just \emph{equivariance}, the
property we just said convolution has. And translation-invariant is just
what layers like \texttt{nn\_maxpool2d()}, \texttt{nn\_avgpool2d()},
etc. are.

\hypertarget{a-prototypical-convnet}{%
\subsubsection{A prototypical convnet}\label{a-prototypical-convnet}}

A template for a convolutional network, called ``convnet'' from now on,
could thus look as below. To preempt any possible confusion: Even
though, above, I was talking about three types of \emph{layers}, there
really is just one type in the code: the convolutional one. For brevity,
both ReLU activation and max pooling are realized as functions instead.

Here is a possible template. It is not intended as a recommendation (as
to number of filters, kernel size, or other hyperparameters, for
example) -- just to illustrate the mechanics. More detailed comments
follow.

\begin{verbatim}
library(torch)

convnet <- nn_module(
  "convnet",
  
  initialize = function() {
    
    # nn_conv2d(in_channels, out_channels, kernel_size)
    self$conv1 <- nn_conv2d(1, 16, 3)
    self$conv2 <- nn_conv2d(16, 32, 3)
    self$conv3 <- nn_conv2d(32, 64, 3)
    
    self$output <- nn_linear(2304, 3)

  },
  
  forward = function(x) {
    
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv2() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv3() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$output()
      
  }
)

model <- convnet()
\end{verbatim}

To understand what is going on, we need to know how images are
represented in \texttt{torch}. By itself, an image is represented as a
three-dimensional tensor, with one dimension indexing into available
channels\index{channels (image)} (package luz)\} (one for gray-scale
images, three for RGB, possibly more for different kinds of imaging
outputs), and the other two, corresponding to the two spatial axes,
height (rows) and width (columns). In deep learning, we work with
batches; thus, there is an additional dimension\index{batch dimension}
-- the very first one -- that refers to batch number.

Let's look at an example image that may be used with the above template:

\begin{verbatim}
img <- torch_randn(1, 1, 64, 64)
\end{verbatim}

What we have here is an image, or more precisely, a batch containing a
single image, that has a single channel, and is of size 64 x 64.

That said, the above template assumes the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The input image has one channel. That's why the first argument to
  \texttt{nn\_conv2d()} is \texttt{1} when we construct the first of the
  conv layers. (No assumptions are made, on the other hand, about the
  size of the input image.)
\item
  We want to distinguish between three different target classes. This
  means that the output layer, a linear module, needs to have three
  output channels.
\end{enumerate}

To test the code, we can call the un-trained model on our example image:

\begin{verbatim}
model(img)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.01 *
 6.4821  3.4166 -5.6050
[ CPUFloatType{1,3} ][ grad_fn = <AddmmBackward0> ]
\end{verbatim}

One final note about that template. When you were reading the code
above, one line that might have stood out is the following:

\begin{verbatim}
self$output <- nn_linear(2304, 3)
\end{verbatim}

How did that 2304, the number of input connections to
\texttt{nn\_linear()}, come about? It is the result of (1) a number of
operations that each reduce spatial resolution, plus (2) a flattening
operation that removes all dimensional information besides the batch
dimension. This will make more sense once we've discussed the arguments
to the layers in question. But one thing needs to be said upfront:
\emph{If this sounds like magic, there is a simple means to make the
magic go away.} Namely, a simple way to find out about tensor shapes at
any stage in a network is to comment all subsequent actions in
\texttt{forward()}, and call the modified model. Naturally, this should
not replace understanding, but it's a great way not to lose one's nerves
when encountering shape errors.

Now, about layer arguments.

\hypertarget{arguments-to-nn_conv2d}{%
\subsubsection{\texorpdfstring{Arguments to
\texttt{nn\_conv2d()}}{Arguments to nn\_conv2d()}}\label{arguments-to-nn_conv2d}}

Above, we passed three arguments to \texttt{nn\_conv2d()}:
\texttt{in\_channels}, \texttt{out\_channels}, and
\texttt{kernel\_size}. This is not an exhaustive list of parameters,
though. The remaining ones all have default values, but it is important
to know about their existence. We're going to elaborate on three of
them, all of whom you're likely to play with applying the template to
some concrete task. All of them affect output size. So do two of the
three mandatory arguments, \texttt{out\_channels} and
\texttt{kernel\_size}:

\begin{itemize}
\item
  \texttt{out\_channels} refers to the number of kernels\index{kernel}
  (often called \emph{filters}\index{filter}, in this context) learned.
  Its value affects the second of the four dimensions of the output
  tensor; it does not affect spatial resolution, though. Learning more
  filters adds capacity to the network, as it increases the number of
  weights.
\item
  \texttt{kernel\_size}, on the other hand, \emph{does} alter spatial
  resolution -- unless its value is 1, in which case the kernel never
  exceeds image boundaries. Like \texttt{out\_channels}, it is a
  candidate for experimentation. In general, though, it is advisable to
  keep kernel size rather small, and chain a larger number of
  convolutional layers, instead of enlarging kernel size in a
  ``shallow'' network.
\end{itemize}

Now for the three non-mandatory arguments to explore.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{padding}\index{padding} is something we've encountered before.
  Any kernel that extends over more than a single pixel will move
  outside the valid region when sliding over an image; the more, the
  bigger the kernel. General options are to (1) either pad the image
  (with zeroes, for example), or (2) compute the dot product only where
  possible. In the latter case, spatial resolution will decrease. That
  need not in itself be a problem; like so many things, it's a matter of
  experimentation. By default, \texttt{torch} does not pad images;
  however by passing a value greater than \texttt{0} for
  \texttt{padding}, you can ensure that spatial resolution is preserved,
  whatever the kernel size. Compare
  fig.~\ref{fig-images-conv-arithmetic-padding}, reproduced from a nice
  compilation by Dumoulin and Visin (2016), to see the effect of
  padding.
\item
  \texttt{stride}\index{stride} refers to the way a kernel moves over
  the image. With a \texttt{stride} greater than \texttt{1}, it takes
  ``leaps'' of sorts -- see
  fig.~\ref{fig-images-conv-arithmetic-strides}. This results in fewer
  ``snapshots'' being taken. As a result, spatial resolution decreases.
\item
  A setting of \texttt{dilation}\index{dilation} greater than
  \texttt{1,} too, results in fewer snapshots, but for a different
  reason. Now, it's not that the kernel moves faster. Instead, the
  pixels it is applied to are not adjacent anymore. They're spread out
  -- how much, depends on the argument's value. See
  fig.~\ref{fig-images-conv-arithmetic-dilation}.
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics{images/images-conv-arithmetic-padding.png}

}

\caption{\label{fig-images-conv-arithmetic-padding}Convolution, and the
effect of padding. Copyright Dumoulin and Visin (2016), reproduced under
\href{https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE}{MIT
license}.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{images/images-conv-arithmetic-strides.png}

}

\caption{\label{fig-images-conv-arithmetic-strides}Convolution, and the
effect of \texttt{strides}. Copyright Dumoulin and Visin (2016),
reproduced under
\href{https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE}{MIT
license}.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{images/images-conv-arithmetic-dilation.png}

}

\caption{\label{fig-images-conv-arithmetic-dilation}Convolution, and the
effect of \texttt{dilation}. Copyright Dumoulin and Visin (2016),
reproduced under
\href{https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE}{MIT
license}.}

\end{figure}

For non-mandatory arguments \texttt{padding}, \texttt{stride}, and
\texttt{dilation}, tbl.~\ref{tbl-images-convargs} has a summary of
defaults and effects.

\hypertarget{tbl-images-convargs}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4795}}@{}}
\caption{\label{tbl-images-convargs}Arguments to \texttt{nn\_conv\_2d()}
you may want to experiment with -- default values and non-default
actions.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Argument
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Action (if non-default)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Argument
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Action (if non-default)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{padding} & 0 & virtual rows/columns added around the image \\
\texttt{stride} & 1 & kernel moves across image at bigger step size
(``jumps'' over pixels) \\
\texttt{dilation} & 1 & kernel is applied to spread-out image pixels
(``holes'' in kernel) \\
\end{longtable}

\hypertarget{arguments-to-pooling-layers}{%
\subsubsection{Arguments to pooling
layers}\label{arguments-to-pooling-layers}}

Pooling layers compute aggregates over neighboring pixels. The number of
pixels of aggregate over in every dimension is specified in the layer
constructor's first argument (alternatively, the corresponding
function's second argument). Slightly misleadingly, that argument is
called \texttt{kernel\_size}, although there are no weights involved:
For example, in the above template, we were unconditionally taking the
maximum pixel value over regions of size 2 x 2.

In analogy to convolution layers, pooling layers also accept arguments
\texttt{padding} and \texttt{stride}. However, they are seldom used.

\hypertarget{zooming-out}{%
\subsubsection{Zooming out}\label{zooming-out}}

We've talked a lot about layers and their arguments. Let's zoom out and
think back about the general template, and what it is supposed to
achieve.

We are chaining blocks that, each, perform a convolution, apply a
non-linearity, and spatially aggregate the result. Each block's weights
act as feature detectors, and every block but the first receives as
input something that already is the result of applying one or more
feature detectors. The magical thing that happens, and the reason behind
the success of convnets, is that by chaining layers, a \emph{hierarchy}
of features is built. Early layers detect edges and textures, later
ones, patterns of various complexity, and the final ones, objects and
parts of objects (see fig.~\ref{fig-images-feature-visualization}, a
beautiful visualization reproduced from Olah, Mordvintsev, and Schubert
(2017)).

\begin{figure}[H]

{\centering \includegraphics{images/images-feature-visualization.png}

}

\caption{\label{fig-images-feature-visualization}Feature visualization
on a subset of layers of GoogleNet. Figure from Olah, Mordvintsev, and
Schubert (2017), reproduced under
\href{https://creativecommons.org/licenses/by/4.0/}{Creative Commons
Attribution CC-BY 4.0} without modification.}

\end{figure}

We now know enough about coding convnets and how they work to explore a
real example.

\hypertarget{classification-on-tiny-imagenet}{%
\section{Classification on Tiny
Imagenet}\label{classification-on-tiny-imagenet}}

Before we start coding, let me anchor your expectations. In this
chapter, we design and train a basic convnet from scratch. As to data
pre-processing, we do what is needed, not more. In the next two
chapters, we'll learn about common techniques used to improve model
training, in terms of quality as well as speed. Once we've covered
those, we'll pick up right where this chapter ended, and apply a few of
those techniques to the present task. Therefore, what we're doing here
is build a baseline, to be used in comparison with more sophisticated
approaches. This is ``just'' a beginning.

\hypertarget{data-pre-processing}{%
\subsection{Data pre-processing}\label{data-pre-processing}}

In addition to \texttt{torch} and \texttt{luz}, we load a third package
from the \texttt{torch} ecosystem: \texttt{torchvision}.
\texttt{torchvision} provides operations on images, as well as a set of
pre-trained models and common benchmark datasets.

\begin{verbatim}
library(torch)
library(torchvision)
library(luz)
\end{verbatim}

The dataset we use is ``Tiny Imagenet''. Tiny Imagenet is a subset of
\href{https://image-net.org/index.php}{ImageNet}, a gigantic collection
of more than fourteen million images, initially made popular through the
``ImageNet Large Scale Visual Recognition Challenge'' that was run
between 2012 and 2017. In the challenge, the most popular task was
multi-class classification, with one thousand different classes to
choose from.

One thousand classes is a lot; and with images typically being processed
at a resolution of 256 x 256, training a model takes a lot of time, even
on luxurious hardware. For that reason, a more manageable version was
created as part of a popular Stanford class on deep learning for images,
\href{http://cs231n.stanford.edu/}{Convolutional Neural Networks for
Visual Recognition (CS231n)}. The condensed dataset has two hundred
classes, with five hundred training images per class. Two hundred
classes, that's still a lot! (Most introductory examples will do ``cats
vs.~dogs'', or some other binary problem.) Thus, it's not an easy task.

We start by downloading the data.

\begin{verbatim}
set.seed(777)
torch_manual_seed(777)

dir <- "~/.torch-datasets"

train_ds <- tiny_imagenet_dataset(
  dir,
  download = TRUE,
  transform = function(x) {
    x %>%
      transform_to_tensor() 
  }
)

valid_ds <- tiny_imagenet_dataset(
  dir,
  split = "val",
  transform = function(x) {
    x %>%
      transform_to_tensor()
  }
)
\end{verbatim}

Notice how \texttt{tiny\_imagenet\_dataset()} takes an argument called
\texttt{transform}\index{\texttt{transform} (torchvision)}. This is used
to specify operations to be performed as part of the input pipeline.
Here, not much is happening: We just convert images to something we can
work with, tensors. However, very soon we'll see this argument used to
specify sequences of transformations such as resizing, cropping,
rotation, and more.

What remains to be done is create the data loaders.

\begin{verbatim}
train_dl <- dataloader(train_ds,
  batch_size = 128,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 128)
\end{verbatim}

Images are RGB, and of size 64 x 64:

\begin{verbatim}
batch <- train_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()

dim(batch$x)
\end{verbatim}

\begin{verbatim}
[1] 128   3  64  64
\end{verbatim}

Classes are integers between 1 to 200:

\begin{verbatim}
batch$y
\end{verbatim}

\begin{verbatim}
torch_tensor
 172
  17
  76
  78
 111
  57
   8
 166
 146
 114
  41
  28
 138
  98
  57
  98
  25
 148
 166
 135
  31
 182
  48
 184
 160
 166
  40
 115
 161
  21
... [the output was truncated (use n=-1 to disable)]
[ CPULongType{128} ]
\end{verbatim}

Now we define a convnet, and train it with \texttt{luz}.

\hypertarget{image-classification-from-scratch}{%
\subsection{Image classification from
scratch}\label{image-classification-from-scratch}}

Here is a prototypical convnet, modeled after our template, but more
powerful.

In addition to what we've seen already, the code illustrates a way of
modularizing the code, arranging layers into three groups:

\begin{itemize}
\item
  a (large) feature detector that, as a whole, is shift-equivariant;
\item
  a shift-invariant pooling layer (\texttt{nn\_adaptive\_avg\_pool2d()})
  that allows us to specify a desired output resolution; and
\item
  a feed-forward neural network that takes the computed features and
  uses them to produce final scores: two hundred values, corresponding
  to two hundred classes, for each item in the batch.
\end{itemize}

\begin{verbatim}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(3, 64, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(64, 128, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(128, 256, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(256, 512, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(512, 1024, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1))
    )
    self$classifier <- nn_sequential(
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
\end{verbatim}

Now, we train the network. The classifier outputs raw logits, not
probabilities; this means we need to make use of
\texttt{nn\_cross\_entropy\_loss()}. We train for fifty epochs:

\begin{verbatim}
fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  fit(train_dl,
      epochs = 50,
      valid_data = valid_dl,
      verbose = TRUE
      )
\end{verbatim}

After fifty epochs, this resulted in accuracy values of 0.92 and 0.22,
on the training and test sets, respectively. This is quite a difference!
On the training set, this model is near-perfect; on the test set, it
only gets up to every fourth image correct.

\begin{verbatim}
Epoch 1/50
Train metrics: Loss: 5.0822 - Acc: 0.0146                                     
Valid metrics: Loss: 4.8564 - Acc: 0.0269
Epoch 2/50
Train metrics: Loss: 4.5545 - Acc: 0.0571                                     
Valid metrics: Loss: 4.2592 - Acc: 0.0904
Epoch 3/50
Train metrics: Loss: 4.0727 - Acc: 0.1122                                     
Valid metrics: Loss: 3.9097 - Acc: 0.1381
...
...
Epoch 48/50
Train metrics: Loss: 0.3033 - Acc: 0.9064                                     
Valid metrics: Loss: 10.2999 - Acc: 0.2188
Epoch 49/50
Train metrics: Loss: 0.2932 - Acc: 0.9098                                     
Valid metrics: Loss: 10.7348 - Acc: 0.222
Epoch 50/50
Train metrics: Loss: 0.2733 - Acc: 0.9152                                     
Valid metrics: Loss: 10.641 - Acc: 0.2204
\end{verbatim}

With two hundred options to choose from, ``every fourth'' does not even
seem so bad; however, looking at the enormous difference between both
metrics, something is not quite right. The model has severely
\emph{overfitted} to the training set -- memorized the training samples,
in other words. Overfitting is not specific to deep learning; it is the
nemesis of all of machine learning. We'll consecrate the whole next
chapter to this topic.

Before we end, though, let's see how we would use \texttt{luz} to obtain
predictions:

\begin{verbatim}
preds <- last %>% predict(valid_dl)
\end{verbatim}

\texttt{predict()} directly returns what is output by the model: two
hundred non-normalized scores for each item. That's because the model's
last layer is a linear module, with no activation applied. (Remember how
the loss function, \texttt{nn\_cross\_entropy\_loss()}, applies a
\emph{softmax} operation before calculating cross-entropy.)

Now, we could certainly call \texttt{nnf\_softmax()} ourselves,
converting outputs from \texttt{predict()} to probabilities:

\begin{verbatim}
preds <- nnf_softmax(preds, dim = 2)
\end{verbatim}

However, if we're just interested in determining the most likely class,
we can as well skip the normalization step, and directly pick the
highest value for each batch item:

\begin{verbatim}
torch_argmax(preds, dim = 2)
\end{verbatim}

\begin{verbatim}
torch_tensor
  55
   1
   1
   1
   1
   1
   1
  89
  45
   1
   1
  19
   1
 190
  14
   1
 185
   1
   1
 150
  77
  37
 131
 193
  80
   1
   1
  45
   1
 131
... [the output was truncated (use n=-1 to disable)]
[ CUDALongType{10000} ]
\end{verbatim}

We could now go on to compare predictions with actual classes, looking
for inspiration on what could be done better. But at this stage, there
is still a \emph{lot} that can be done better! We will return to this
application in due time, but first, we need to learn about overfitting,
and ways to speed up model training.

\hypertarget{sec:overfitting}{%
\chapter{Making models generalize}\label{sec:overfitting}}

In deep learning, as in machine learning in general, we face a problem
of generalization. The training set is limited, after all. There is no
guarantee that what a model has learned generalizes to the ``outside
world''.

How big of a problem is this? It depends. When we construct a toy
problem ourselves, we can actively make sure that training and test set
come from the very same distribution. On the other end of the spectrum
are configurations where significant structural disparities are known to
exist from the outset. In the latter case, the measures described in
this chapter will be useful, but not sufficient to resolve the problem.
Instead, expert domain knowledge and appropriate workflow (re-)design
will be required.

Between both extremes, there is a wide range of possibilities. For
example:

\begin{itemize}
\item
  An image dataset has had all pictures taken using excellent
  photographic equipment, and as viewed from a 180 degrees angle; but
  prediction will take place ``in the wild'', using cell phones.
\item
  A application designed to diagnose skin cancer has been trained on
  white males mostly, but is supposed to be used by people of any gender
  and/or skin color.
\end{itemize}

The second example should make clear that lack of generalization (a.k.a.
\emph{overfitting}\footnote{Meaning: ``overly fitting'' the model to the
  training data.}) -- is anything but an exclusively technical problem.
Insufficient representation, or even \emph{lack} of representation, is
one of the major manifestations of \emph{dataset bias}. Among the
measures intended to prevent overfitting we discuss below, just one --
the most important of all -- applies to the second scenario. (This book
is dedicated to technical topics; but as emphasized in the preface, we
all live in a ``real world'' that is not just extremely complex, but
also, fraught with inequality, inequity, and injustice. However strong
our technical focus, this is something we should never lose awareness
of.)

The counter-measures I'll be presenting can be ordered by stage in the
machine learning workflow they apply to: data collection, data
pre-processing, model definition, and model training. As you'll be
expecting by now, the most important measure, as hinted at above, is to
collect more representative data.

\hypertarget{the-royal-road-more-and-more-representative-data}{%
\section{The royal road: more -- and more representative! --
data}\label{the-royal-road-more-and-more-representative-data}}

Depending on your situation, you may be forced to work with pre-existing
datasets. In that case, you may still have the option of supplementing
external sources with data you've collected yourself, albeit of lesser
quantity.

In the next chapter, we'll encounter the technique of \emph{transfer
learning}. Transfer learning means making use of \emph{pre-trained}
models, employing them as feature extractors for your ``downstream''
task. Often, these models have been trained on huge datasets. In
consequence, they are not just ``very good at what they do'', but they
also generalize to unseen data -- \emph{provided} the new data are
similar to those they have been trained on. What if they're not? For
many tasks, it will still be possible to make use of a pre-trained
model: Take it the way it comes, and go on training it -- but now,
adding in the kinds of data you want it to generalize to.

Of course, being able to add in \emph{any} data of your own may still be
a dream. In that case, all you can do is think hard about how your
results will be biased, and be honest about it.

Now, let's say you've thought it through, and are confident that there
are no systematic deficiencies in your training data preventing
generalization to use cases in the wild. Or maybe you've restricted the
application domain of your model as required. Then, if you have a small
training set and want it to generalize as much as possible, what can you
do?

\hypertarget{pre-processing-stage-data-augmentation}{%
\section{\texorpdfstring{Pre-processing stage: Data
augmentation\index{data augmentation}}{Pre-processing stage: Data augmentation}}\label{pre-processing-stage-data-augmentation}}

Data augmentation means taking the data you have and modifying them, so
as to force the algorithm to abstract over some things. What things? It
depends on the domain operated upon. This should become clear by means
of a concrete example.

In this chapter, I'll introduce two popular variants of data
augmentation: one I'll refer to as ``classic'', the other one going by
the name of ``mixup''.

\hypertarget{classic-data-augmentation}{%
\subsection{Classic data augmentation}\label{classic-data-augmentation}}

Classically, when people talk about (image) data augmentation, it is the
following they're having in mind. You take an image, and apply some
random transformation to it. That transformation could be geometric,
such as when rotating, translating, or scaling the image. Alternatively,
instead of moving things around, the operation could affect the colors,
as when changing brightness or saturation. Other options include
blurring the image, or, quite the contrary, sharpening it. Technically,
you are free to implement whatever algorithm you desire -- you don't
have to use any of the (numerous!) transformations provided by
\texttt{torchvision}. In practice, though, you'll likely find much of
what you need among the transformations already available.

In our running example, we'll work with MNIST, the \emph{Hello World} of
image-classification datasets we've already made quick use of before. It
contains 70,000 images of the digits \texttt{0} to \texttt{9}, split
into training and test sets in a ratio of six to one. Like before, we
get the data from \texttt{torchvision}.

To see how the digits appear without any data augmentation, take a look
at the first thirty-two images in the test set
(fig.~\ref{fig-overfitting-mnist-test}):

\begin{verbatim}
library(torch)
library(torchvision)
library(luz)

dir <- "~/.torch-datasets"

valid_ds <- mnist_dataset(
  dir,
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)

valid_dl <- dataloader(valid_ds, batch_size = 128)

# a convenient way to obtain individual images without 
# manual iteration
test_images <- coro::collect(
  valid_dl, 1
)[[1]]$x[1:32, 1, , ] %>% as.array()

par(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))
test_images %>%
  purrr::array_tree(1) %>%
  purrr::map(as.raster) %>%
  purrr::iwalk(~ {
    plot(.x)
  })
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/overfitting-mnist-test.png}

}

\caption{\label{fig-overfitting-mnist-test}MNIST: The first thirty-two
images in the test set.}

\end{figure}

Now, we use the training set to experiment with data
augmentation\index{data augmentation!transforms (torchvision)}. Like the
\texttt{dogs\_vs\_cats\_dataset()} we made use of in the last chapter --
in fact, like all \texttt{torchvision} datasets --
\texttt{mnist\_dataset()} takes a \texttt{transform} argument, allowing
you to pass in arbitrary transformations to be performed on the input
images. Of the four transformations that appear in the code snippet
below, one we have already seen: \texttt{transform\_to\_tensor()}, used
to convert from R \texttt{double}s to \texttt{torch} tensors. The other
three, that all share the infix \texttt{\_random\_}, each trigger
non-deterministic data augmentation: be it by flipping the image
horizontally (\texttt{transform\_random\_horizontal\_flip()}) or
vertically (\texttt{transform\_random\_vertical\_flip()}), or through
rotations and translations(\texttt{transform\_random\_affine()}). In all
cases, the amount of distortion is configurable.

\begin{verbatim}
train_ds <- mnist_dataset(
  dir,
  download = TRUE,
  transform = . %>%
    transform_to_tensor() %>%
    # flip horizontally, with a probability of 0.5
    transform_random_horizontal_flip(p = 0.5) %>%
    # flip vertically, with a probability of 0.5
    transform_random_vertical_flip(p = 0.5) %>%
    # (1) rotate to the left or the right,
    #     up to respective angles of 45 degrees
    # (2) translate vertically or horizontally,
    #     not exceeding 10% of total image width/height
    transform_random_affine(
      degrees = c(-45, 45),
      translate = c(0.1, 0.1)
    )
)
\end{verbatim}

Again, let's look at a sample result
(fig.~\ref{fig-overfitting-mnist-transforms}).

\begin{verbatim}
train_dl <- dataloader(
  train_ds,
  batch_size = 128,
  shuffle = TRUE
)

train_images <- coro::collect(
  train_dl, 1
)[[1]]$x[1:32, 1, , ] %>% as.array()

par(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))
train_images %>%
  purrr::array_tree(1) %>%
  purrr::map(as.raster) %>%
  purrr::iwalk(~ {
    plot(.x)
  })
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/overfitting-mnist-transforms.png}

}

\caption{\label{fig-overfitting-mnist-transforms}MNIST, with random
rotations, translations, and flips.}

\end{figure}

The effects are clearly visible, and the ranges chosen for rotations and
translations seem sensible. But let's think about the flips. Does it
actually make \emph{sense} to include them?

In general, this would depend on the dataset, and more even, on the
task. Think of a cat, comfortably residing on a fluffy sofa. If the cat
were looking to the right instead of to the left, it would still be a
cat; if it was positioned upside-down, we'd probably assume the image
had been loaded incorrectly. Neither transformation would affect its
``catness''. It's different with digits, though. A flipped \(1\) is not
the same as a \(1\), at least not in a default context. Thus, for MNIST,
I'd rather go with rotations and translations only:

\begin{verbatim}
train_ds <- mnist_dataset(
  dir,
  download = TRUE,
  transform = . %>%
    transform_to_tensor() %>%
    transform_random_affine(
      degrees = c(-45, 45), translate = c(0.1, 0.1)
    )
)

train_dl <- dataloader(train_ds,
  batch_size = 128,
  shuffle = TRUE
)
\end{verbatim}

Now, to compare what happens with and without augmentation, you'd
separately train a model for both an augmented and a non-augmented
version of the training set. Here is an example setup:

\begin{verbatim}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    # nn_conv2d(in_channels, out_channels, kernel_size, stride)
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 2)
    self$conv3 <- nn_conv2d(64, 128, 3, 1)
    self$conv4 <- nn_conv2d(128, 256, 3, 2)
    self$conv5 <- nn_conv2d(256, 10, 3, 2)
  },
  forward = function(x) {
    x %>%
      self$conv1() %>%
      nnf_relu() %>%
      self$conv2() %>%
      nnf_relu() %>%
      self$conv3() %>%
      nnf_relu() %>%
      self$conv4() %>%
      nnf_relu() %>%
      self$conv5() %>%
      torch_squeeze()
  }
)

fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  fit(train_dl, epochs = 5, valid_data = valid_dl)
\end{verbatim}

With MNIST, we have at our disposition a huge training set, and at the
same time, we're dealing with a very homogeneous domain. Those are
exactly the conditions where we don't expect to see much
overfitting\footnote{I chose MNIST for this overview because it allows
  to easily discern the effects of various augmentation techniques
  (especially the next one we're going to discuss).}.

However, even with MNIST, you'll notice that with augmentation, as well
as the other ``overfitting antidotes'' to be introduced, it takes a lot
longer to achieve better performance on the training than on the test
set (if ever there \emph{is} better performance on the training set!).
For example, in the setup described above, with no data augmentation
applied, training-set accuracy surpassed that on the test set from the
third epoch onwards; whereas with augmentation, there was no sign of
overfitting during all five epochs I trained for.

Next -- still in the realm of data augmentation -- we look at a
technique that is domain-independent; that is, it can be applied to all
kinds of data, not just images.

\hypertarget{mixup}{%
\subsection{\texorpdfstring{Mixup\index{data augmentation!mixup (luz)}}{Mixup}}\label{mixup}}

Classic data augmentation, whatever it may be doing to the entities
involved -- move them, distort them, blur them -- it leaves them
\emph{intact}. A rotated cat is still a cat. \emph{Mixup} (Zhang et al.
2017), on the other hand, takes two entities and ``mixes them
together''. With mixup, we may have something that's half-cat and
half-squirrel. Or rather, in practice, with strongly unequal
\emph{mixing weights} used, ninety percent squirrel and ten percent cat.

As an idea, mixup generalizes to any domain. We could mix time series,
say, or categorical data of any kind. We could also mix numerical data,
although it's not clear why we would do it. After all, mixup is nothing
else but linear combination: take two values \(x1\) and \(x2\), and
construct \(x3 = w1 x1 + w2 x2\), where \(w1\) and \(w2\) are weights
summing to one. In neural networks, linear combination of numerical
values (``automatic mixup'') happens all the time, so that normally, we
wouldn't expect ``manual mixup'' to add much value.

For visual demonstration, however, images are still best. Starting from
MNIST's test set, we can apply mixup with different weight patterns --
equal, very unequal, and somewhere in-between -- and see what happens.
\texttt{luz} has a function, called
\texttt{nnf\_mixup()}\index{\texttt{nnf{\textunderscore}mixup()} (luz)},
that lets you play around with this.

(By the way, I'm introducing this function just so you can picture
(literally!) what is going on. To actually \emph{use} mixup, all that is
required is to pass the appropriate callback to \texttt{fit()}, and let
\texttt{setup()} know which loss function you want to use.)

Besides the input and target batches, \texttt{nnf\_mixup()} expects to
be passed the \emph{mixing weights}, one value per batch item. We start
with the most ``tame'' variant: with weights that are very unequal
between classes. Every resultant image will be composed, to ninety
percent, of the original item at that position, and to ten percent, of a
randomly-chosen different one
(fig.~\ref{fig-overfitting-mnist-mixup-0.9}):

\begin{verbatim}
first_batch <- coro::collect(valid_dl, 1)[[1]]

mixed <- nnf_mixup(x = first_batch$x,
                   y = first_batch$y,
                   weight = torch_tensor(rep(0.9, 128)))
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/overfitting-mnist-mixup-0.9.png}

}

\caption{\label{fig-overfitting-mnist-mixup-0.9}Mixing up MNIST, with
mixing weights of 0.9.}

\end{figure}

Do you agree that the mixed-in digits are just barely visible, if at
all? Still, the callback's default configuration results in mixing
ratios pretty close to this one. For MNIST, this probably is too
cautious a choice. But think of datasets where the objects are less
shape-like, less sharp-edged. Mixing two landscapes, at an equal-ish
ratio, would result in total gibberish. And here, too, the task plays a
role; not just the dataset per se. Mixing apples and oranges one-to-one
can make sense if we're looking for a higher-level concept -- a
superset, of sorts. But if all we're looking for is to correctly discern
oranges that look a bit like an apple, or apples that have something
``orange-y'' in them, then a ratio such as 9:1 might be just fine.

To develop an idea of what would happen for other proportions, let's
successively make the mixing ratio more equal.

First, here (fig.~\ref{fig-overfitting-mnist-mixup-0.7}) is 0.7:

\begin{verbatim}
mixed <- nnf_mixup(x = first_batch$x,
                   y = first_batch$y,
                   weight = torch_tensor(rep(0.7, 128)))
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/overfitting-mnist-mixup-0.7.png}

}

\caption{\label{fig-overfitting-mnist-mixup-0.7}Mixing up MNIST, with
mixing weights of 0.7.}

\end{figure}

And here (fig.~\ref{fig-overfitting-mnist-mixup-0.5}), 0.5:

\begin{verbatim}
mixed <- nnf_mixup(x = first_batch$x,
                   y = first_batch$y,
                   weight = torch_tensor(rep(0.5, 128)))
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/overfitting-mnist-mixup-0.5.png}

}

\caption{\label{fig-overfitting-mnist-mixup-0.5}Mixing up MNIST, with
mixing weights of 0.5.}

\end{figure}

To use mixup while training, all you need to do is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Add
  \texttt{luz\_callback\_mixup()}\index{\texttt{luz{\textunderscore}callback{\textunderscore}mixup()} (luz)}
  to the list of callbacks passed to \texttt{luz::fit()}. The callback
  takes an optional parameter, \texttt{alpha}, used in determining the
  mixing ratios. I'd recommend starting with the default, though, and
  start tweaking from there.
\item
  Wrap the loss you're using for the task (here, cross entropy) in
  \texttt{nn\_mixup\_loss()}\index{\texttt{nn{\textunderscore}mixup{\textunderscore}loss()} (luz)}.
\item
  Use the loss, not the accuracy metric, for monitoring training
  progress, since accuracy in this case is not well defined.
\end{enumerate}

Here is an example:

\begin{verbatim}
# redefine the training set to not use augmentation
train_ds <- mnist_dataset(
  dir,
  download = TRUE,
  transform = transform_to_tensor
)

train_dl <- dataloader(train_ds,
  batch_size = 128,
  shuffle = TRUE
)

fitted <- convnet %>%
  setup(
    loss = nn_mixup_loss(torch::nn_cross_entropy_loss()),
    optimizer = optim_adam
  ) %>%
  fit(
    train_dl,
    epochs = 5,
    valid_data = valid_dl,
    callbacks = list(luz_callback_mixup())
  )
\end{verbatim}

Mixup is an appealing technique that makes a lot of intuitive sense. If
you feel like, go ahead and experiment with it on different tasks and
different types of data.

Next, we move on to the next stage in the workflow: model definition.

\hypertarget{modeling-stage-dropout-and-regularization}{%
\section{Modeling stage: dropout and
regularization}\label{modeling-stage-dropout-and-regularization}}

Inside a neural network, there are two kinds of ``data'':
\emph{activations} -- tensors propagated from one layer to the next --
and \emph{weights}, tensors associated with individual layers. From the
two techniques we'll look at in this section, one (\emph{dropout})
affects the former; the other (\emph{regularization}), the latter.

\hypertarget{dropout}{%
\subsection{\texorpdfstring{Dropout\index{dropout}}{Dropout}}\label{dropout}}

Dropout (Srivastava et al. 2014) happens during training only. At each
forward pass, individual activations -- single values in the tensors
being passed on -- are \emph{dropped} (meaning: set to zero), with
configurable probability. Due to randomness, actual positions of
zeroed-out values in a tensor vary from pass to pass.

Put differently: Dynamically and reversibly, individual inter-neuron
connections are ``cut off''. Why would this help to avoid overfitting?

If different connections between neurons could be dropped out at
unforeseeable times, the network as a whole had better not get too
dependent on cooperation between individual units. But it is just this
kind of inter-individual cooperation that results in strong memorization
of examples presented during training. If that is made impossible, the
network as a whole has to focus on more general features, ones that that
emerge from more distributed, more random cooperation. Put differently,
we're introducing \emph{randomness}, or \emph{noise}, and no
hyper-specialized model will be able to deal with that.

Application of this technique in \texttt{torch} is very straightforward.
A dedicated layer takes care of it: \texttt{nn\_dropout()}. By ``takes
care'' I mean:

\begin{itemize}
\item
  In its \texttt{forward()} method, the layer checks whether we're in
  training or test mode. If the latter, nothing happens.
\item
  If we're training, it uses the dropout probability \(p\) it's been
  initialized with to zero out parts of the input tensor.
\item
  The partly-zeroed tensor is scaled up by the inverse of \(1-p\), to
  keep the overall magnitude of the tensor unchanged. The result is then
  passed on to the next layer.
\end{itemize}

Here is our convnet from above, with a few dropout layers interspersed:

\begin{verbatim}
convnet <- nn_module(
  "convnet",
  
  initialize = function() {
    # nn_conv2d(in_channels, out_channels, kernel_size, stride)
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 2)
    self$conv3 <- nn_conv2d(64, 128, 3, 1)
    self$conv4 <- nn_conv2d(128, 256, 3, 2)
    self$conv5 <- nn_conv2d(256, 10, 3, 2)
    
    self$drop1 <- nn_dropout(p = 0.2)
    self$drop2 <- nn_dropout(p = 0.2)
  },
  forward = function(x) {
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>%
      self$conv2() %>%
      nnf_relu() %>% 
      self$drop1() %>%
      self$conv3() %>% 
      nnf_relu() %>% 
      self$conv4() %>% 
      nnf_relu() %>% 
      self$drop2() %>%
      self$conv5() %>%
      torch_squeeze()
  }
)
\end{verbatim}

As always, experimentation will help in determining a good dropout rate,
as well as the number of dropout layers introduced. You may be
wondering, though -- how does this technique go together with the
data-related ones we presented before?

In practice (in the ``real world''), you would basically always use data
augmentation. As to dropout, it probably is \emph{the} go-to technique
in this area. A priori, there is no reason to \emph{not} use them
together -- all the more since both are configurable. One way to see it
is like this: You have a fixed budget of randomness; every technique you
use that adds randomness will take up some of that budget. How do you
know if you've exceeded it? By seeing no (or insufficient) progress on
the training set. There is a clear ranking of priorities here: It's no
use worrying about generalization to the test set as long as the model
is not learning at all. The number one requirement always is to get the
model to learn in the first place.

\hypertarget{regularization}{%
\subsection{\texorpdfstring{Regularization\index{weight decay}}{Regularization}}\label{regularization}}

Both dropout and regularization affect how the model's inner workings,
but they are very different in spirit.

Dropout introduces randomness. Looking for analogies in machine learning
overall, it has something in common with ensemble modeling. (By the way,
the idea of ensembling is as applicable, in theory, to neural networks,
as to other algorithms. It's just not that popular because training a
neural network takes a long time.)

Regularization, on the other hand, is similar to -- regularization. If
you know what is meant by this term in machine learning in general, you
know what is meant in deep learning. In deep learning, though, it is
often referred to by a different name: ``weight decay''. Personally, I
find this a little misleading. ``Decay'' seems to hint at some sort of
\emph{temporal} development; in fact, this is exactly its meaning in
``learning rate decay'', a training strategy that makes use of
decreasing learning rates over time.

In weight decay, or regularization, however, there's no dynamics
involved at all. Instead, we follow a fixed rule. That rule is: When
computing the loss, add to it a quantity proportional to the aggregate
size of the weights. The idea is to keep the weights small and
homogeneous, to prevent sharp cliffs and canyons in the loss function.

In deep learning, regularization as a strategy is nowhere as central as
data augmentation or dropout, which is why I'm not going to go into
great detail. If you want to learn more, check out one of the many great
introductions to statistical learning around.

Among data scientists, regularization probably is most often associated
with different variants of linear regression. Variants differ in what
they understand by a penalty ``proportional'' to the weights. In
\emph{ridge regression}, this quantity will be a fraction of the sum of
the \emph{squared} weights; in the \emph{Lasso}, their absolute values.
It is only the former algorithm that is implemented in \texttt{torch}.

Although semantically, regularization forms part of the ``business
logic'' -- which is why I'm listing it in the ``model'' section -- it
technically is implemented as part of an optimizer object. All of the
classic optimizers -- SGD, RMSProp, Adam, and relatives -- take a
\texttt{weight\_decay} argument used to indicate what fraction of the
sum of squared weights you'd like to have added to the loss.

In our example, you'd pass this argument to
\texttt{luz::set\_opt\_hparams()}, like so:

\begin{verbatim}
fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_accuracy())
    ) %>%
  set_opt_hparams(weight_decay = 0.00001) %>%
  fit(train_dl, epochs = 5, valid_data = valid_dl)
\end{verbatim}

As already hinted at above, regularization is not seen that often in the
context of neural networks. Nevertheless, given the wide range of
problems deep learning is applied to, it's good to be aware of its
availability.

\hypertarget{training-stage-early-stopping}{%
\section{\texorpdfstring{Training stage: Early
stopping\index{early stopping}}{Training stage: Early stopping}}\label{training-stage-early-stopping}}

We conclude this chapter with an example of an
anything-but-sophisticated, but very effective training technique: early
stopping. So far, in our running example, we've always let the model
learn for a pre-determined number of epochs. Thanks to the existence of
callbacks -- those ``hooks'' into the training process that let you
modify ``almost everything'' dynamically -- however, training duration
does not have to be fixed from the outset.

One of several callbacks related to the learning rate -- besides, e.g.,
\texttt{luz\_callback\_lr\_scheduler()} , that allows you to dynamically
adjust learning rate while training --
\texttt{luz\_callback\_early\_stopping()} will trigger an early exit
once some configurable condition is satisfied.

Called without parameters, it will monitor loss on the validation set,
and once validation loss stops decreasing, it will immediately cause
training to stop. However, less strict policies are possible. For
example, \texttt{luz\_callback\_early\_stopping(patience\ =\ 2)} will
allow for two consecutive epochs without improvement before triggering
an exit.

To make use of \texttt{luz\_callback\_early\_stopping()}, you add it to
the callbacks list in \texttt{fit()}:

\begin{verbatim}
fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_accuracy())
  ) %>%
  fit(train_dl,
    epochs = 5,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_early_stopping()
    )
  )
\end{verbatim}

In deep learning, early stopping is ubiquitous; it's hard to imagine why
one would \emph{not} want to use it.

Having discussed overfitting, we now go on to a complementary aspect of
model training: True, we want our models to generalize; but we also want
them to learn fast. That's what the next chapter is dedicated to.

\hypertarget{sec:training_efficiency}{%
\chapter{Speeding up training}\label{sec:training_efficiency}}

You could say that the topics discussed in this and the preceding
chapter relate like the non-negotiable and the desirable.
Generalization, the ability to abstract over individual instances, is a
\emph{sine qua non} of a good model; however, we need to arrive at such
a model in reasonable time (where reasonable means very different things
in different contexts).

This time, in presenting techniques I'll follow a different strategy,
ordering them not by stages in the workflow, but by increasing
generality. We'll be looking at three very different, very successful
(each in its own way) ideas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Batch normalization. \emph{Batchnorm} -- to introduce a popular
  abbreviation -- layers are added to a model to stabilize and, in
  consequence, speed up training.
\item
  Determining a good learning rate upfront, and dynamically varying it
  during training. As you might remember from our experiments with
  optimization, the learning rate has an enormous impact on training
  speed and stability.
\item
  Transfer learning. Applied to neural networks, the term commonly
  refers to using pre-trained models for feature detection, and making
  use of those features in a downstream task.
\end{enumerate}

\hypertarget{batch-normalization}{%
\section{\texorpdfstring{Batch
normalization\index{batch normalization}}{Batch normalization}}\label{batch-normalization}}

The idea behind batch normalization (Ioffe and Szegedy (2015)) directly
follows from the basic mechanism of backpropagation.

In backpropagation, each layer's weights are adapted, from the very last
to the very first. Now, let's focus on layer 17. When the time comes for
the next forward pass, it will have updated its weights in a way that
made sense, given the previous batch. However -- the layer right before
it will also have updated its weights. As will the one preceding its
predecessor, the one before that \ldots{} you get the picture. And so,
due to \emph{all prior layers now handling their inputs differently},
layer 17 will not quite get what it expects. In consequence, the
strategy that seemed optimal before might not be.

While the problem per se is algorithm-inherent, it is more likely to
surface the deeper the model. Due to the resulting instability, you need
to train with lower learning rates. And this, in turn, means that
training will take more time.

The solution Ioffe and Szegedy proposed was the following. At each pass,
and for every layer, normalize the activations. If that were all,
however, some sort of levelling would occur. That's because each layer
now has to adjust its activations so they have a mean of zero and a
standard deviation of one. In fact, such a requirement would not just
act as an equalizer \emph{between} layers, but also, \emph{within}:
meaning, it would make it harder, for each individual layer, to create
sharp internal distinctions.

For that reason, mean and standard deviation are not simply computed,
but \emph{learned}. In other words, they become \emph{model parameters}.

So far, we've been talking about this conceptually, suggesting an
implementation where each layer took care of this itself. This is not
how it's implemented, however. Rather, we have dedicated layers,
\emph{batchnorm} layers, that normalize and re-scale their inputs. It is
them who have mean and standard deviation as learnable parameters.

To use batch normalization in our MNIST example, we intersperse
batchnorm layers throughout the network, one after each convolution
block. There are three types of them, one for each of one-, two-, and
three-dimensional inputs (time series, images, and video, say). All of
them compute statistics individually per channel, and the number of
input channels is the only required argument to their constructors.

\begin{verbatim}
library(torch)
library(torchvision)
library(luz)

convnet <- nn_module(
  "convnet",
  initialize = function() {
    # nn_conv2d(in_channels, out_channels, kernel_size, stride)
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 2)
    self$conv3 <- nn_conv2d(64, 128, 3, 1)
    self$conv4 <- nn_conv2d(128, 256, 3, 2)
    self$conv5 <- nn_conv2d(256, 10, 3, 2)

    self$bn1 <- nn_batch_norm2d(32)
    self$bn2 <- nn_batch_norm2d(64)
    self$bn3 <- nn_batch_norm2d(128)
    self$bn4 <- nn_batch_norm2d(256)
  },
  forward = function(x) {
    x %>%
      self$conv1() %>%
      nn_relu() %>%
      self$bn1() %>%
      self$conv2() %>%
      nn_relu() %>%
      self$bn2() %>%
      self$conv3() %>%
      nn_relu() %>%
      self$bn3() %>%
      self$conv4() %>%
      nn_relu() %>%
      self$bn4() %>%
      self$conv5() %>%
      torch_squeeze()
  }
)
\end{verbatim}

One thing you may be wondering though: What happens during testing? The
whole notion of testing would be carried to absurdity, were we to apply
the same logic there as well. Instead, during evaluation we use the mean
and standard deviation determined on the training set. So, batch
normalization shares with dropout the fact that they behave differently
across phases.

Batch normalization can be stunningly successful, especially in image
processing. It's a technique you should always consider. What's more, it
has often been found to help with generalization, as well.

\hypertarget{dynamic-learning-rates}{%
\section{Dynamic learning rates}\label{dynamic-learning-rates}}

You won't be surprised to hear that the learning rate is central to
training performance. In backpropagation, layer weights are modified in
a direction given by the current loss; the learning rate affects the
size of the update.

With very small updates, the network might move in the right direction,
to eventually arrive at a satisfying local minimum of the loss function.
But the journey will be long. The bigger the updates, on the other hand,
the likelier it gets that it'll ``jump over'' that minimum. Imagine
moving down one leg of a parabola. Maybe the update is so big that we
don't just end up on the other leg (with equivalent loss), but at a
``higher'' place (loss) even. Then the next update will send us back to
the other leg, to a yet higher location. It won't take long until loss
becomes infinite -- the dreaded \texttt{NaN}, in R.

The goal is easily stated: We'd like to train with the highest-viable
learning rate, while avoiding to ever ``overshoot''. There are two
aspects to this.

First, we should know what would constitute too high a rate. To that
purpose, we use something called a \emph{learning rate
finder}\index{learning rate finder}. This technique owes a lot of its
popularity to the \href{https://docs.fast.ai}{fast.ai} library, and the
deep learning classes taught by its creators. The learning rate finder
gets called once, before training proper.

Second, we want to organize training in a way that at each time, the
optimal learning rate is used. Views differ on what \emph{is} an
optimal, stage-dependent rate. \texttt{torch} offers a set of so-called
\emph{learning rate schedulers} implementing various widely-established
techniques. Schedulers differ not just in strategy, but also, in how
often the learning rate is adapted.

\hypertarget{learning-rate-finder}{%
\subsection{Learning rate finder}\label{learning-rate-finder}}

The idea of the learning rate finder is the following. You train the
network for a single epoch, starting with a very low rate. Looping
through the batches, you keep increasing it, until you arrive at a very
high value. During the loop, you keep track of rates as well as
corresponding losses. Experiment finished, you plot rates and losses
against each other. You then pick a rate lower, but not very much lower,
than the one at which loss was minimal. The recommendation usually is to
choose a value one order of magnitude smaller than the one at minimum.
For example, if the minimum occurred at \texttt{0.01}, you would go with
\texttt{0.001}.

Nicely, we don't need to code this ourselves:
\texttt{luz::lr\_finder()}\index{\texttt{lr{\textunderscore}finder()} (luz)}
will run the experiment for us. All we need to do is inspect the
resulting graph -- and make the decision!

To demonstrate, let's first copy some prerequisites from the last
chapter. We use MNIST, with data augmentation. Model-wise, we build on
the default version of the CNN, and add in batch normalization.
\texttt{lr\_finder()} then expects the model to have been
\texttt{setup()} with a loss function and an optimizer:

\begin{verbatim}
library(torch)
library(torchvision)
library(luz)

dir <- "~/.torch-datasets" 

train_ds <- mnist_dataset(
  dir,
  download = TRUE,
  transform = . %>%
    transform_to_tensor() %>%
    transform_random_affine(
      degrees = c(-45, 45), translate = c(0.1, 0.1)
    )
)

train_dl <- dataloader(train_ds, batch_size = 128, shuffle = TRUE)

valid_ds <- mnist_dataset(
  dir,
  train = FALSE,
  transform = transform_to_tensor
)

valid_dl <- dataloader(valid_ds, batch_size = 128)

convnet <- nn_module(
  "convnet",
  initialize = function() {
    # nn_conv2d(in_channels, out_channels, kernel_size, stride)
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 2)
    self$conv3 <- nn_conv2d(64, 128, 3, 1)
    self$conv4 <- nn_conv2d(128, 256, 3, 2)
    self$conv5 <- nn_conv2d(256, 10, 3, 2)

    self$bn1 <- nn_batch_norm2d(32)
    self$bn2 <- nn_batch_norm2d(64)
    self$bn3 <- nn_batch_norm2d(128)
    self$bn4 <- nn_batch_norm2d(256)
  },
  forward = function(x) {
    x %>%
      self$conv1() %>%
      nnf_relu() %>%
      self$bn1() %>%
      self$conv2() %>%
      nnf_relu() %>%
      self$bn2() %>%
      self$conv3() %>%
      nnf_relu() %>%
      self$bn3() %>%
      self$conv4() %>%
      nnf_relu() %>%
      self$bn4() %>%
      self$conv5() %>%
      torch_squeeze()
  }
)

model <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_accuracy())
    )
\end{verbatim}

When called with default parameters, \texttt{lr\_finder()} will start
with a learning rate of \texttt{1e-7}, and increase that, over one
hundred steps, until it arrives at \texttt{0.1}. All of these values --
minimum learning rate, number of steps, and maximum learning rate -- can
be modified. For MNIST, I knew that higher learning rates should be
feasible; so I shifted that range a bit to the right:

\begin{verbatim}
rates_and_losses <- model %>%
  lr_finder(train_dl, start_lr = 0.0001, end_lr = 0.3)
\end{verbatim}

Plotting the recorded losses against their rates, we get both the exact
values (one for each of the steps), and an exponentially-smoothed
version (fig.~\ref{fig-efficiency-mnist-lr-finder}).

\begin{verbatim}
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/efficiency-mnist-lr-finder.png}

}

\caption{\label{fig-efficiency-mnist-lr-finder}Output of \texttt{luz}'s
learning rate finder, run on MNIST.}

\end{figure}

Here, we see that when rates exceed a value of about 0.01, losses become
noisy, and increase. The definitive explosion, though, seems to be
triggered only when the rate surpasses 0.1. In consequence, you might
decide to not exactly follow the ``one order of magnitude''
recommendation, and try a learning rate of 0.01 -- at least in case you
do what I'll be doing in the next section, namely, use the so-determined
rate not as a fixed-in-time value, but as a maximal one.

\hypertarget{learning-rate-schedulers}{%
\subsection{\texorpdfstring{Learning rate
schedulers\index{learning rate scheduler}}{Learning rate schedulers}}\label{learning-rate-schedulers}}

Once we have an idea where to upper-bound the learning rate, we can make
use of one of \texttt{torch}'s learning rate schedulers to orchestrate
rates over training. We will decide on a scheduler object, and pass that
to a dedicated \texttt{luz} callback:
\texttt{callback\_lr\_scheduler()}\index{\texttt{callback{\textunderscore}lr{\textunderscore}scheduler()} (luz)}.

Classically, a popular, intuitively appealing scheme used to be the
following. In early stages of training, try a reasonably high learning
rate, in order to make quick progress; once that has happened, though,
slow down, making sure you don't zig-zag around (and away from) a
presumably-found local minimum.

In the meantime, more sophisticated schemes have been developed.

One family of ideas keeps periodically turning up and down the learning
rate. Members of this family are known as, for example, ``cyclical
learning rates'' (Smith (2015)), or (some form of) ``annealing with
restarts'' (e.g., Loshchilov and Hutter (2016)). What differs between
members of the family is the shape of the resulting learning rate curve,
and the frequency of restarts (meaning, how often you turn up the rate
again, to begin a new period of descent). In \texttt{torch}, popular
representatives of this family are, for example, \texttt{lr\_cyclic()}
and \texttt{lr\_cosine\_annealing\_warm\_restarts()}.

A very different approach is represented by the \emph{one-cycle}
learning rate strategy (Smith and Topin (2017)). In this scheme, we
start from some initial -- low-ish -- learning rate, increase that up to
some user-specified maximum, and from there, decrease again, until we've
arrived at a rate significantly lower than the one we started with. In
\texttt{torch}, this is available as \texttt{lr\_one\_cycle()}, and this
is the strategy I was referring to above.

\texttt{lr\_one\_cycle()} allows for user-side tweaking in a number of
ways, and in real-life projects, you may want to play around a bit with
its many parameters. Here, we use the defaults. All we need to do, then,
is pass in the maximum rate we determined, and decide on how often we
want the learning rate to be updated. The logical way seems to be to do
it once per batch, something that will happen if we pass in number of
epochs and number of steps per epoch.

In the code snippet below, note that the arguments \texttt{max\_lr} ,
\texttt{epochs}, and \texttt{steps\_per\_epoch} really ``belong to''
\texttt{lr\_one\_cycle()}. We have to pass them to the callback, though,
because it is the callback that will instantiate the scheduler.

\texttt{call\_on}, however, genuinely forms part of the callback logic.
This is a harmless-looking argument that, nevertheless, we need to pay
attention to. Schedulers differ in whether their period is defined in
epochs, or in batches. \texttt{lr\_one\_cycle()} ``wakes up'' once per
batch; but there are others -- \texttt{lr\_step()}, for example - that
check whether an update is due once per epoch only. The default value of
\texttt{call\_on} is \texttt{on\_epoch\_end}; so for
\texttt{lr\_one\_cycle()}, we have to override the default.

\begin{verbatim}
num_epochs <- 5 

# the model has already been setup(), we continue from there
fitted <- model %>%
  fit(train_dl,
    epochs = num_epochs,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_lr_scheduler(
        lr_one_cycle,
        max_lr = 0.01,
        epochs = num_epochs,
        steps_per_epoch = length(train_dl),
        call_on = "on_batch_end"
      )
    )
  )
\end{verbatim}

At this point, we wrap up the topic of learning rate optimization. As
with so many things in deep learning, research progresses at a rapid
rate, and most likely, new scheduling strategies will continue to be
added. Now though, for a total change in scope.

\hypertarget{transfer-learning}{%
\section{\texorpdfstring{Transfer
learning\index{transfer learning}}{Transfer learning}}\label{transfer-learning}}

Transfer, as a general concept, is what happens when we have learned to
do one thing, and benefit from those skills in learning something else.
For example, we may have learned how to make some move with our left
leg; it will then be easier to learn how to do the same with our right
leg. Or, we may have studied Latin and then, found that it helped us a
lot in learning French. These, of course, are straightforward examples;
analogies between domains and skills can be a lot more subtle.

In comparison, the typical usage of ``transfer learning'' in deep
learning seems rather narrow, at first glance. Concretely, it refers to
making use of huge, highly effective models (often provided as part of
some library), that have already been trained, for a long time, on a
huge dataset. Typically, you would load the model, remove its output
layer, and add on a small-ish sequential module that takes the model's
now-last layer to the kind of output you require. Often, in example
tasks, this will go from the broad to the narrow -- as in the below
example, where we use a model trained on one thousand categories of
images to distinguish between ten types of digits.

But it doesn't have to be like that. In deep learning, too, models
trained on one task can be built upon in tasks that have
\emph{different}, but not necessarily more \emph{domain-constrained},
requirements. As of today, popular examples for this are found mostly in
natural language processing (NLP), a topic we don't cover in this book.
There, you find models trained to predict how a sentence continues --
resulting in general ``knowledge'' about a language -- used in logically
dependent tasks like translation, question answering, or text
summarization. Transfer learning, in that general sense, is something
we'll certainly see more and more of in the near future.

There is another, very important aspect to the popularity of transfer
learning, though. When you build on a pre-trained model, you'll
incorporate all of what it has learned - including its biases and
preconceptions. How much that matters, in your context, will depend on
what exactly you're doing. For us, who are classifying digits, it will
not matter whether the pre-trained model performs a lot better on cats
and dogs than on e-scooters, smart fridges, or garbage cans. But think
about this whenever models concern \emph{people}. Typically, these
high-performing models have been trained either on benchmark datasets,
or data massively scraped from the web. The former have, historically,
been very little concerned with questions of stereotypes and
representation. (Hopefully, that will change in the future.) The latter
are, by definition, subject to availability bias, as well as
idiosyncratic decisions made by the dataset creators. (Hopefully, these
circumstances and decisions will have been carefully documented. That is
something you'll need to check out.)

With our running example, we'll be in the former category: We'll be
downstream users of a benchmark dataset. The benchmark dataset in
question is \href{https://image-net.org/index.php}{ImageNet}, the
well-known collection of images we already encountered in our first
experience with Tiny Imagenet, two chapters ago.

In \texttt{torchvision}, we find a number of ready-to-use models that
have been trained on ImageNet. Among them is ResNet-18 (He et al.
(2015)). The ``Res'' in ResNet stands for ``residual'', or ``residual
connection''. Here \emph{residual} is used, as is common in statistics,
to designate an error term. The idea is to have some layers predict, not
something entirely new, but the difference between a target and the
previous layer's prediction -- the error, so to say. If this sounds
confusing, don't worry. For us, what matters is that due to their
architecture, ResNets can afford to be very deep, without becoming
excessively hard to train. And that in turn means they're very
performant, and often used as pre-trained feature detectors.

The first time you use a pre-trained model, its weights are downloaded,
and cached in an operating-system-specific location.

\begin{verbatim}
resnet <- model_resnet18(pretrained = TRUE)
resnet
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 11,689,512 parameters.

 Modules 
 conv1: <nn_conv2d> #9,408 parameters
 bn1: <nn_batch_norm2d> #128 parameters
 relu: <nn_relu> #0 parameters
 maxpool: <nn_max_pool2d> #0 parameters
 layer1: <nn_sequential> #147,968 parameters
 layer2: <nn_sequential> #525,568 parameters
 layer3: <nn_sequential> #2,099,712 parameters
 layer4: <nn_sequential> #8,393,728 parameters
 avgpool: <nn_adaptive_avg_pool2d> #0 parameters
 fc: <nn_linear> #513,000 parameters
\end{verbatim}

Have a look at the last module, a linear layer:

\begin{verbatim}
resnet$fc
\end{verbatim}

From the weights, we can see that this layer maps tensors with 512
features to ones with 1000 - the thousand different image categories
used in the ImageNet challenge. To adapt this model to our purposes, we
simply replace the very last layer with one that outputs feature vectors
of length ten:

\begin{verbatim}
resnet$fc <- nn_linear(resnet$fc$in_features, 10)
resnet$fc
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 5,130 parameters.

 Parameters 
 weight: Float [1:10, 1:512]
 bias: Float [1:10]
\end{verbatim}

What will happen if we now train the modified model on MNIST? Training
will progress with the speed of a Zenonian tortoise, since gradients
need to be propagated across a huge network. Not only is that a waste of
time; it is useless, as well. It could, if we were very patient, even be
harmful: We could destroy the intricate feature hierarchy learned by the
pre-trained model. Of course, in classifying digits we will make use of
just a tiny subset of learned higher-order features, but that is not a
problem. In any case, with the resources available to mere mortals, we
are unlikely to improve on ResNet's digit-discerning capabilities.

What we do, thus, is set all layer weights to non-trainable, apart from
just that last layer we replaced.

Putting it all together, we arrive at the following, concise definition
of a model:

\begin{verbatim}
convnet <- nn_module(
  initialize = function() {
    self$model <- model_resnet18(pretrained = TRUE)
    for (par in self$parameters) {
      par$requires_grad_(FALSE)
    }
    self$model$fc <- nn_linear(self$model$fc$in_features, 10)
  },
  forward = function(x) {
    self$model(x)
  }
)
\end{verbatim}

This we can now train with \texttt{luz}, like before. There's just one
further step required, and that's just because I'm using MNIST to
illustrate. Since ResNet has been trained on RGB images, its first layer
expects three channels, not one. We can work around this by multiplexing
the single grayscale channel into three identical ones, using
\texttt{torch\_expand()}. For important real-life tasks, this may not be
an optimal solution; but it will do well enough for MNIST.

A convenient place to perform the expansion is as part of the data
pre-processing pipeline, repeated here in modified form.

\begin{verbatim}
train_ds <- mnist_dataset(
  dir,
  download = TRUE,
  transform = . %>%
    transform_to_tensor() %>%
    (function(x) x$expand(c(3, 28, 28))) %>%
    transform_random_affine(
      degrees = c(-45, 45), translate = c(0.1, 0.1)
    )
)

train_dl <- dataloader(
  train_ds,
  batch_size = 128,
  shuffle = TRUE
)

valid_ds <- mnist_dataset(
  dir,
  train = FALSE,
  transform = . %>%
    transform_to_tensor %>%
    (function(x) x$expand(c(3, 28, 28)))
)

valid_dl <- dataloader(valid_ds, batch_size = 128)
\end{verbatim}

The code for training then looks as usual.

\begin{verbatim}
model <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_accuracy())
    ) %>%
  fit(train_dl,
      epochs = 5,
      valid_data = valid_dl)
\end{verbatim}

Before we wrap up both section and chapter, one additional comment. The
way we proceeded, above -- replacing the very last layer with a single
module outputting the final scores -- is just the easiest, most
straightforward thing to do.

For MNIST, this is good enough. Maybe, on inspection, we'd find that
single digits already form part of ResNet's feature hierarchy; but even
if not, a linear layer with \textasciitilde{} 5000 parameters should
suffice to learn them. However, the more there is ``still to be
learned'' -- equivalently, the more either dataset or task differ from
what was used (done, resp.) in model pre-training -- the more powerful a
sub-module we will want to chain on. We'll see an example of this in the
next chapter.

\hypertarget{sec:image-classification-2}{%
\chapter{Image classification, take two: Improving
performance}\label{sec:image-classification-2}}

In the last two chapters, we saw how changes to data input, network
architecture, and training modalities can result in improved results,
``improvement'' having two principal denotations: better generalization
to the test set, and faster training progress.

Now, we'll apply a few of those techniques to the image classification
task we started our journey into real-world deep learning with: Tiny
Imagenet. In terms of counteracting overfitting, we'll introduce data
augmentation, dropout layers, and early stopping. To speed up training,
we make use of the learning rate finder, add batchnorm layers, and
integrate a pre-trained network. We won't add-and-remove these
techniques one at a time, that is, we won't assess their effects in
isolation. While this is something you might want to do yourself, here
we want to avoid the impression that there is some fixed ranking -- this
is best, that is second \ldots{} -- , \emph{independently of dataset and
task}.

Instead, what we do is:

\begin{itemize}
\item
  Always use data augmentation. There is hardly ever a case where you'd
  \emph{not} want to use it -- unless, of course, you are already using
  a different data augmentation technique.
\item
  Always run with early stopping enabled. This will not just prevent
  overfitting, but also, save time.
\item
  Always make use of the learning rate finder, together with a one-cycle
  learning rate schedule.
\item
  For our first setup, we take the convnet from three chapters ago, and
  add dropout layers.
\item
  In scenario number two, we replace dropout by batch normalization.
  (Everything else stays the same.)
\item
  Third, we replace the model completely, by one chaining a pre-trained
  feature classifier (ResNet) and a small sequential model.
\end{itemize}

\hypertarget{data-input-common-for-all}{%
\section{Data input (common for all)}\label{data-input-common-for-all}}

All three runs use the same data input pipeline. Compared with our first
go at telling apart the two hundred classes in Tiny Imagenet, two things
are new.

First, we now apply data augmentation to the training set: rotations and
translations, to be precise.

Second, input tensors are normalized, channel-wise, to a set of given
means and standard deviations. This really is required for the third run
(using ResNet) only; we just do to our images what was done in training
ResNet. (The same goes for most of the pre-trained models trained on
ImageNet.) There really is no problem, though, in doing the same for
runs one and two; so normalization is part of the common pre-processing
pipeline.

\begin{verbatim}
library(torch)
library(torchvision)
library(torchdatasets)
library(luz)

set.seed(777)
torch_manual_seed(777)

dir <- "~/.torch-datasets"

train_ds <- tiny_imagenet_dataset(
  dir,
  download = TRUE,
  transform = . %>%
    transform_to_tensor() %>%
    transform_random_affine(
      degrees = c(-30, 30), translate = c(0.2, 0.2)
    ) %>%
    transform_normalize(
      mean = c(0.485, 0.456, 0.406),
      std = c(0.229, 0.224, 0.225)
    )
)

valid_ds <- tiny_imagenet_dataset(
  dir,
  split = "val",
  transform = function(x) {
    x %>%
      transform_to_tensor() %>%
      transform_normalize(
        mean = c(0.485, 0.456, 0.406),
        std = c(0.229, 0.224, 0.225))
  }
)

train_dl <- dataloader(
  train_ds,
  batch_size = 128,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 128)
\end{verbatim}

Next, we compare three different configurations.

\hypertarget{run-1-dropout}{%
\section{Run 1: Dropout}\label{run-1-dropout}}

In run one, we take the convnet we were using, and add dropout layers.

\begin{verbatim}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(3, 64, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.05),
      nn_conv2d(64, 128, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.05),
      nn_conv2d(128, 256, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.05),
      nn_conv2d(256, 512, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.05),
      nn_conv2d(512, 1024, kernel_size = 3, padding = 1), 
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1)),
      nn_dropout2d(p = 0.05),
    )
    self$classifier <- nn_sequential(
      nn_linear(1024, 1024),
      nn_relu(),
      nn_dropout(p = 0.05),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_dropout(p = 0.05),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
\end{verbatim}

Next, we run the learning rate finder
(fig.~\ref{fig-images2-lr-finder-dropout}).

\begin{verbatim}
model <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) 
    
rates_and_losses <- model %>% lr_finder(train_dl)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/images2-lr-finder-dropout.png}

}

\caption{\label{fig-images2-lr-finder-dropout}Learning rate finder, run
on Tiny Imagenet. Convnet with dropout layers.}

\end{figure}

We already know that discerning between two hundred classes is a task
that takes time; it's thus not surprising to see a flat-ish loss curve
during most of learning rate increase. We can conclude, though, that we
had better not exceed a learning rate of 0.01.

As in all further configurations, we now train with the one-cycle
learning rate scheduler, and early stopping enabled.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 50, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 2),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.01,
          epochs = 50,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end"),
        luz_callback_model_checkpoint(path = "cpt_dropout/"),
        luz_callback_csv_logger("logs_dropout.csv")
        ),
      verbose = TRUE)
\end{verbatim}

For me, training stopped after thirty-five epochs, at a validation
accuracy of 0.4, and a training accuracy that was just slightly higher:
0.44.

\begin{verbatim}
Epoch 1/50
Train metrics: Loss: 5.116 - Acc: 0.0128                                      
Valid metrics: Loss: 4.9144 - Acc: 0.0217
Epoch 2/50
Train metrics: Loss: 4.7217 - Acc: 0.042                                      
Valid metrics: Loss: 4.4143 - Acc: 0.067
Epoch 3/50
Train metrics: Loss: 4.3681 - Acc: 0.0791                                     
Valid metrics: Loss: 4.1145 - Acc: 0.105
...
...
Epoch 33/50
Train metrics: Loss: 2.3006 - Acc: 0.4304                                     
Valid metrics: Loss: 2.5863 - Acc: 0.4025
Epoch 34/50
Train metrics: Loss: 2.2717 - Acc: 0.4365                                     
Valid metrics: Loss: 2.6377 - Acc: 0.3889
Epoch 35/50
Train metrics: Loss: 2.2456 - Acc: 0.4402                                     
Valid metrics: Loss: 2.6208 - Acc: 0.4043
Early stopping at epoch 35 of 50
\end{verbatim}

Comparing with the initial approach, where after fifty epochs, we were
left with accuracies of 0.22 for validation, and 0.92 for training, we
see an impressive reduction in overfitting. Of course, we cannot really
say anything about the respective merits of dropout and data
augmentation here. If you're curious, please go ahead and find out!

\hypertarget{run-2-batch-normalization}{%
\section{Run 2: Batch normalization}\label{run-2-batch-normalization}}

In configuration number two, dropout is replaced by batch normalization.

\begin{verbatim}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(3, 64, kernel_size = 3, padding = 1),
      nn_batch_norm2d(64),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(64, 128, kernel_size = 3, padding = 1),
      nn_batch_norm2d(128),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(128, 256, kernel_size = 3, padding = 1),
      nn_batch_norm2d(256),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(256, 512, kernel_size = 3, padding = 1),
      nn_batch_norm2d(512),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(512, 1024, kernel_size = 3, padding = 1), 
      nn_batch_norm2d(1024),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1)),
    )
    self$classifier <- nn_sequential(
      nn_linear(1024, 1024),
      nn_relu(),
      nn_batch_norm1d(1024),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_batch_norm1d(1024),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
\end{verbatim}

Again, we run the learning rate finder
(fig.~\ref{fig-images2-lr-finder-batchnorm}):

\begin{verbatim}
model <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) 

rates_and_losses <- model %>% lr_finder(train_dl)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/images2-lr-finder-batchnorm.png}

}

\caption{\label{fig-images2-lr-finder-batchnorm}Learning rate finder,
run on Tiny Imagenet. Convnet with batchnorm layers.}

\end{figure}

This looks surprisingly different! Of course, this is in part due to the
scale on the loss axis; the loss does not explode as much, and thus, we
get better resolution in the early and middle stages. The loss not
exploding is an interesting finding in itself; the conclusion for us to
draw from this plot is to be a bit more careful with the learning rate.
This time, we'll choose 0.001 for the maximum.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 50, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 2),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.001,
          epochs = 50,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end"),
        luz_callback_model_checkpoint(path = "cpt_batchnorm/"),
        luz_callback_csv_logger("logs_batchnorm.csv")
        ),
      verbose = TRUE)
\end{verbatim}

Compared with scenario one, I saw slightly more overfitting with
batchnorm.

\begin{verbatim}
Epoch 1/50
Train metrics: Loss: 4.5434 - Acc: 0.0862                                     
Valid metrics: Loss: 4.0914 - Acc: 0.1332
Epoch 2/50
Train metrics: Loss: 3.9534 - Acc: 0.161                                      
Valid metrics: Loss: 3.7865 - Acc: 0.1809
Epoch 3/50
Train metrics: Loss: 3.6425 - Acc: 0.2054                                     
Valid metrics: Loss: 3.5965 - Acc: 0.2115
...
...
Epoch 19/50
Train metrics: Loss: 2.1063 - Acc: 0.4859                                     
Valid metrics: Loss: 2.621 - Acc: 0.3912
Epoch 20/50
Train metrics: Loss: 2.0514 - Acc: 0.4987                                     
Valid metrics: Loss: 2.6334 - Acc: 0.3914
Epoch 21/50
Train metrics: Loss: 1.9982 - Acc: 0.5069                                     
Valid metrics: Loss: 2.6603 - Acc: 0.3932
Early stopping at epoch 21 of 50
\end{verbatim}

\hypertarget{run-3-transfer-learning}{%
\section{Run 3: Transfer learning}\label{run-3-transfer-learning}}

Finally, the setup including transfer learning. A pre-trained ResNet is
used for feature extraction, and a small sequential model takes care of
classification. During training, all of ResNets weights are left
untouched.

\begin{verbatim}
convnet <- nn_module(
  initialize = function() {
    self$model <- model_resnet18(pretrained = TRUE)
    for (par in self$parameters) {
      par$requires_grad_(FALSE)
    }
    self$model$fc <- nn_sequential(
      nn_linear(self$model$fc$in_features, 1024),
      nn_relu(),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    self$model(x)
  }
)
\end{verbatim}

As always, we run the learning rate finder
(fig.~\ref{fig-images2-lr-finder-resnet}).

\begin{verbatim}
model <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) 

rates_and_losses <- model %>% lr_finder(train_dl)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/images2-lr-finder-resnet.png}

}

\caption{\label{fig-images2-lr-finder-resnet}Learning rate finder, run
on Tiny Imagenet. Convnet with transfer learning (ResNet).}

\end{figure}

A maximal rate of 0.01 looks like it could be on the edge, but I decided
to give it a try.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 50, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 2),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.01,
          epochs = 50,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end"),
        luz_callback_model_checkpoint(path = "cpt_resnet/"),
        luz_callback_csv_logger("logs_resnet.csv")
        ),
      verbose = TRUE)
\end{verbatim}

For me, this configuration resulted in early stopping after nine epochs
already, and yielded the best results by far: Final accuracy on the
validation set was 0.48. Interestingly, in this setup, accuracy ended up
\emph{worse} for training than for validation.

\begin{verbatim}
Epoch 1/50
Train metrics: Loss: 3.4036 - Acc: 0.2322                                     
Valid metrics: Loss: 2.5491 - Acc: 0.3884
Epoch 2/50
Train metrics: Loss: 2.7911 - Acc: 0.3436                                     
Valid metrics: Loss: 2.417 - Acc: 0.4233
Epoch 3/50
Train metrics: Loss: 2.6423 - Acc: 0.3726                                     
Valid metrics: Loss: 2.3492 - Acc: 0.4431
...
...
Valid metrics: Loss: 2.1822 - Acc: 0.4868
Epoch 7/50
Train metrics: Loss: 2.4031 - Acc: 0.4198                                     
Valid metrics: Loss: 2.1413 - Acc: 0.4889
Epoch 8/50
Train metrics: Loss: 2.3759 - Acc: 0.4252                                     
Valid metrics: Loss: 2.149 - Acc: 0.4958
Epoch 9/50
Train metrics: Loss: 2.3447 - Acc: 0.433                                      
Valid metrics: Loss: 2.1888 - Acc: 0.484
Early stopping at epoch 9 of 50
\end{verbatim}

In the next chapter, we stay with the domain -- images -- but vary the
task: We move on from classification to segmentation.

\hypertarget{sec:image-segmentation}{%
\chapter{Image segmentation}\label{sec:image-segmentation}}

\hypertarget{segmentation-vs.-classification}{%
\section{Segmentation
vs.~classification}\label{segmentation-vs.-classification}}

Both classification and segmentation are about labeling -- but both
don't label the same thing. In classification, it's complete images that
are categorized; in segmentation, it's individual pixels. For each and
every pixel, we ask: Which object, or what kind of object, is this pixel
part of? No longer are we just interested in saying ``this is a cat'';
this time, we need to know where exactly that cat is.

Note the qualifier: exactly. That's what constitutes the difference to
\emph{object detection}, where the class instances found are surrounded
by a so-called bounding box. The type of localization hint provided by
such boxes (rectangles, basically) is not sufficient for many tasks in,
say, health care, biology, or the earth sciences. For example, in
segmenting cell tissue, we need to see the actual boundaries between
clusters of cell types, not their straightened versions.

Object detection is not something we discuss in this book; but we've
built up quite some experience with classification. Compared to
classification, then, what changes?

Remember how, in the initial chapter on image classification, we talked
about translation invariance and translation equivariance. If an
operator is translation-invariant\index{invariance (translational)}, it
will return the same measurement when applied to location \(x\) and to
\(x=x+n\). If, on the other hand, it is translation-equivariant, it will
return a measurement adapted to the new location. In classification, the
difference did not really matter. Now though, in segmentation, we want
to avoid transformations that are not
shift-equivariant\index{equivariance (translational)}. We can't afford
to lose location-related information anymore.

Technically, this means that no layer type should be used that abstracts
over location. However, we just saw how successful the typical
convolutional architecture is at learning about images. We certainly
want to to re-use as much of that architecture as we can.

We will avoid pooling layers, since those destroy information about
location. But how, then, are we going to build up a hierarchy of
features? For that hierarchy to emerge, it would seem like \emph{some
form} of spatial downsizing \emph{has to} occur -- no matter how we make
it happen. And for sure, that representational hierarchy is something we
can't sacrifice: Whatever the downstream task, the features are required
for the model to develop an ``understanding'' of sorts about what is
displayed in the image. But: If we need to label every single pixel, the
network must output an image exactly equal, in resolution, to the input!
These are conflicting goals -- can they be combined?

\hypertarget{u-net-a-classic-in-image-segmentation}{%
\section{U-Net, a ``classic'' in image
segmentation}\label{u-net-a-classic-in-image-segmentation}}

The general U-Net architecture, first described in Ronneberger, Fischer,
and Brox (2015), has been used for countless tasks involving image
segmentation, as a sub-module of numerous composite models as well as in
various standalone forms. To explain the name ``U-Net'', there is no
better way than to reproduce a figure from the paper
(fig.~\ref{fig-segmentation-unet}):

\begin{figure}[H]

{\centering \includegraphics{images/segmentation-unet.png}

}

\caption{\label{fig-segmentation-unet}U-Net architecture from
Ronneberger, Fischer, and Brox (2015), reproduced with the principal
author's permission.}

\end{figure}

The left ``leg'' of the U shows a sequence of steps implementing a
successive decrease in spatial resolution, accompanied by an increase in
number of filters. The right leg illustrates the opposite mechanism:
While number of filters goes down, spatial size increases right until we
reach the original resolution of the input. (This is achieved via
\emph{upsampling}, a technique we'll talk about below.)

In consequence, we do in fact build up a feature hierarchy, and at the
same time, we are able to classify individual pixels. But the upsampling
process should result in significant loss of spatial information -- are
we really to expect sensible results?

Well, probably not, were it not for the mechanism, also displayed in the
above schematic, of channeling lower-level feature maps through the
system. This is the constitutive U-Net idea: In the ``down'' sequence,
when creating higher-level feature maps, we don't throw away the
lower-level ones; instead we keep them, to be eventually fed back into
the ``up'' sequence. In the ``up'' sequence, once some small-resolution
input has been upsampled, the matching-in-size features from the
``down'' process are appended. This means that each ``up'' step works on
an ensemble of feature maps: ones kept while downsizing, \emph{and} ones
incorporating higher-level information.

The fact that U-Net-based architectures are so pervasively used speaks
to the power of this idea.

\hypertarget{u-net-a-torch-implementation}{%
\section{\texorpdfstring{U-Net -- a \texttt{torch}
implementation}{U-Net -- a torch implementation}}\label{u-net-a-torch-implementation}}

Our \texttt{torch} implementation follows the general U-Net idea. As
always in this book, kernel sizes, number of filters, as well as
hyper-parameters should be seen as subject to experimentation.

The implementation is modular, emphasizing the fact that we can
distinguish two phases, an encoding and a decoding phase. Unlike in many
other encoder-decoder architectures, these are coupled: Since the
decoder needs to incorporate ``messages'' from the encoder, namely, the
conserved feature maps, it will have to know about their sizes.

\hypertarget{encoder}{%
\subsection{Encoder}\label{encoder}}

In the last chapter, we've seen how using a pre-trained feature
extractor speeds up training. Now that we need to keep feature maps on
our way ``down'', can we still apply this technique? We can, and we'll
see how to do it shortly. First, let's talk about the pre-trained model
we'll be using.

MobileNet v2 (Sandler et al. (2018)) features a convolutional
architecture optimized for mobile use. We won't go into details here,
but there is one thing we'd like to check: Does it lose translation
equivariance; for example, by using local pooling? Let's poke around a
bit to find out.

\begin{verbatim}
library(torch)
library(torchvision)
library(luz)

model <- model_mobilenet_v2(pretrained = TRUE)
model
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 3,504,872 parameters.

 Modules 
 features: <nn_sequential> #2,223,872 parameters
 classifier: <nn_sequential> #1,281,000 parameters
\end{verbatim}

Immediately, we see that \texttt{model\_mobilenet\_v2()} is a wrapping a
sequence of two modules: a container called \texttt{features} (the
feature detector, evidently), and another called \texttt{classifier}
(again, with a telling name). It's the former we're interested in.

\begin{verbatim}
model$features
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 2,223,872 parameters.

 Modules 
 0: <conv_bn_activation> #928 parameters
 1: <inverted_residual> #896 parameters
 2: <inverted_residual> #5,136 parameters
 3: <inverted_residual> #8,832 parameters
 4: <inverted_residual> #10,000 parameters
 5: <inverted_residual> #14,848 parameters
 6: <inverted_residual> #14,848 parameters
 7: <inverted_residual> #21,056 parameters
 8: <inverted_residual> #54,272 parameters
 9: <inverted_residual> #54,272 parameters
 10: <inverted_residual> #54,272 parameters
 11: <inverted_residual> #66,624 parameters
 12: <inverted_residual> #118,272 parameters
 13: <inverted_residual> #118,272 parameters
 14: <inverted_residual> #155,264 parameters
 15: <inverted_residual> #320,000 parameters
 16: <inverted_residual> #320,000 parameters
 17: <inverted_residual> #473,920 parameters
 18: <conv_bn_activation> #412,160 parameters
\end{verbatim}

Thus MobileNet is mostly made up of a bunch of ``inverted residual''
blocks. What do these consist of? Some further poking around tells us:

\begin{verbatim}
model$features[2]$`0`$conv
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 896 parameters.

 Modules 
 0: <conv_bn_activation> #352 parameters
 1: <nn_conv2d> #512 parameters
 2: <nn_batch_norm2d> #32 parameters
\end{verbatim}

If we want to be paranoid, we still need to check the first of these
modules:

\begin{verbatim}
model$features[2]$`0`$conv[1]$`0`
\end{verbatim}

\begin{verbatim}
An `nn_module` containing 352 parameters.

 Modules 
 0: <nn_conv2d> #288 parameters
 1: <nn_batch_norm2d> #64 parameters
 2: <nn_relu6> #0 parameters
\end{verbatim}

It seems like there really is no pooling applied. The question then is,
how would one obtain -- and keep around -- feature maps from different
stages? Here is how the encoder does it:

\begin{verbatim}
encoder <- nn_module(
  initialize = function() {
    model <- model_mobilenet_v2(pretrained = TRUE)
    self$stages <- nn_module_list(list(
      nn_identity(),
      model$features[1:2],
      model$features[3:4],
      model$features[5:7],
      model$features[8:14],
      model$features[15:18]
    ))
    for (par in self$parameters) {
      par$requires_grad_(FALSE)
    }
  },
  forward = function(x) {
    features <- list()
    for (i in 1:length(self$stages)) {
      x <- self$stages[[i]](x)
      features[[length(features) + 1]] <- x
    }
    features
  }
)
\end{verbatim}

The encoder splits up MobileNet v2's feature extraction blocks into
several stages, and applies one stage after the other. Respective
results are saved in a list.

We can construct an example, and inspect the sizes of the feature maps
obtained. For three-channel input of resolution 224 x 224 pixels, we
see:

\begin{verbatim}
sample <- torch_randn(1, 3, 224, 224)
sample_features <- encoder()(sample)
purrr::map(sample_features, purrr::compose(dim, as.array))
\end{verbatim}

\begin{verbatim}
[[1]]
[1]   1   3 224 224

[[2]]
[1]   1  16 112 112

[[3]]
[1]   1  24  56  56

[[4]]
[1]   1  32  28  28

[[5]]
[1]   1  96  14  14

[[6]]
[1]   1 320   7   7
\end{verbatim}

Next, we look at the decoder, which is a bit more complex.

\hypertarget{decoder}{%
\subsection{Decoder}\label{decoder}}

The decoder is made up of configurable blocks. A block receives two
input tensors: one that is the result of applying the previous decoder
block, and one that holds the feature map produced in the matching
encoder stage. In the forward pass, first the former is upsampled, and
passed through a nonlinearity. The intermediate result is then prepended
to the second argument, the channeled-through feature map. On the
resultant tensor, a convolution is applied, followed by another
nonlinearity.

\begin{verbatim}
decoder_block <- nn_module(
  initialize = function(in_channels,
                        skip_channels,
                        out_channels) {
    self$upsample <- nn_conv_transpose2d(
      in_channels = in_channels,
      out_channels = out_channels,
      kernel_size = 2,
      stride = 2
    )
    self$activation <- nn_relu()
    self$conv <- nn_conv2d(
      in_channels = out_channels + skip_channels,
      out_channels = out_channels,
      kernel_size = 3,
      padding = "same"
    )
  },
  forward = function(x, skip) {
    x <- x %>%
      self$upsample() %>%
      self$activation()
    input <- torch_cat(list(x, skip), dim = 2)
    input %>%
      self$conv() %>%
      self$activation()
  }
)
\end{verbatim}

We now look closer at how upsampling is achieved. Technically, what is
applied is a so-called transposed convolution -- hence the name of the
layer,
\texttt{nn\_conv\_transpose2d()}\index{transposed convolution}.(If
you're wondering about the \texttt{transpose}: Quite literally, the
kernel \emph{is} the transpose of a corresponding one that performs
downsampling.) However, it's more intuitive to picture the operation
like this: First zeroes are inserted between individual tensor values,
and then, a convolution with \texttt{strides} greater than \texttt{1} is
applied. See fig.~\ref{fig-segmentation-conv-arithmetic-transposed} for
a visualization.

\begin{figure}[H]

{\centering \includegraphics{images/segmentation-conv-arithmetic-transposed.png}

}

\caption{\label{fig-segmentation-conv-arithmetic-transposed}Transposed
convolution. Copyright Dumoulin and Visin (2016), reproduced under
\href{https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE}{MIT
license}.}

\end{figure}

Even though we won't go into technical details, we can do a quick check
that really, convolution and transposed convolution affect resolution in
opposite ways.

We start from a 1 x 1 ``image'', and apply a 3 x 3 filter with a
\texttt{stride} of 2. Together with padding, this results in an output
tensor of size 3 x 3.

\begin{verbatim}
img <- torch_randn(1, 1, 5, 5)

conv <- nn_conv2d(
  in_channels = 1,
  out_channels = 1,
  kernel_size = 3,
  stride = 2,
  padding = 1
)

convolved <- conv(img)
convolved
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,1,.,.) = 
 -0.4996 -0.2898  0.4643
  0.6608  1.2109  0.8377
  0.3615  0.5400  0.1567
[ CPUFloatType{1,1,3,3} ][ grad_fn = <ConvolutionBackward0> ]
\end{verbatim}

If we take that output, and now apply a transposed convolution, with the
same kernel size, strides, and padding as the above convolution, we are
back to the original resolution of 5 x 5:

\begin{verbatim}
transposed_conv <- nn_conv_transpose2d(
  in_channels = 1,
  out_channels = 1,
  kernel_size = 3,
  stride = 2,
  padding = 1
)

upsampled <- transposed_conv(convolved)
upsampled
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,1,.,.) = 
  0.4076  0.0940  0.3424  0.1920  0.1078
  0.2416  0.6456  0.2473  0.6500  0.2643
  0.0467  0.5028 -0.1243  0.6425 -0.0083
  0.2682  0.5003  0.2812  0.4150  0.2720
  0.1398  0.3832  0.0843  0.4155  0.2035
[ CPUFloatType{1,1,5,5} ][ grad_fn = <ConvolutionBackward0> ]
\end{verbatim}

After that quick check, back to the decoder block. What is the outcome
of its very first application?

Above, we saw that at the ``bottom of the U'', we will have a tensor of
size 7 x 7, with 320 channels. This tensor will be upsampled, and
concatenated with feature maps from the previous ``down'' stage. At that
stage, there had been 96 channels. That makes two thirds of the
information needed to instantiate a decoder block (\texttt{in\_channels}
and \texttt{skip\_channels}). The missing third, \texttt{out\_channels},
really is up to us. Here we choose 256.

We can thus instantiate a decoder block:

\begin{verbatim}
first_decoder_block <- decoder_block(
  in_channels = 320,
  skip_channels = 96,
  out_channels = 256
)
\end{verbatim}

To do a forward pass, the block needs to be passed two tensors: the
maximally-processed features, and their immediate precursors. Let's
check that our understanding is correct:

\begin{verbatim}
first_decoder_block(
  sample_features[[6]],
  sample_features[[5]]
) %>%
  dim()
\end{verbatim}

\begin{verbatim}
[1]   1 256  14  14
\end{verbatim}

Let me remark in passing that the purpose of exercises like this is not
just to explain some concrete architecture. They're also supposed to
illustrate how with \texttt{torch}, instead of having to rely on some
overall idea of what a piece of code will probably do, you can usually
find a way to \emph{know}.

Now that we've talked at length about the decoder blocks, we can quickly
characterize their ``manager'' of sorts, the decoder module. It
``merely'' instantiates and runs through the blocks.

\begin{verbatim}
decoder <- nn_module(
  initialize = function(
    decoder_channels = c(256, 128, 64, 32, 16),
    encoder_channels = c(16, 24, 32, 96, 320)) {
    encoder_channels <- rev(encoder_channels)
    skip_channels <- c(encoder_channels[-1], 3)
    in_channels <- c(encoder_channels[1], decoder_channels)

    depth <- length(encoder_channels)

    self$blocks <- nn_module_list()
    for (i in seq_len(depth)) {
      self$blocks$append(decoder_block(
        in_channels = in_channels[i],
        skip_channels = skip_channels[i],
        out_channels = decoder_channels[i]
      ))
    }
  },
  forward = function(features) {
    features <- rev(features)
    x <- features[[1]]
    for (i in seq_along(self$blocks)) {
      x <- self$blocks[[i]](x, features[[i + 1]])
    }
    x
  }
)
\end{verbatim}

\hypertarget{the-u}{%
\subsection{The ``U''}\label{the-u}}

Before we take the last step and look at the top-level module, let's see
how the U-shape comes about in our case. Here
(tbl.~\ref{tbl-segmentation-u}) is a table, displaying ``image'' sizes
at every step of the ``down'' and ``up'' passes, as well as the actors
responsible for shape manipulations:

\hypertarget{tbl-segmentation-u}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3562}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2740}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3699}}@{}}
\caption{\label{tbl-segmentation-u}Tensor sizes at various stages of the
encoding and decoding chains.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Encoder steps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Decoder steps}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Encoder steps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Decoder steps}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\emph{Input}}: \texttt{224\ x\ 224} (channels: 3) & &
\textbf{\emph{Output}}: \texttt{224\ x\ 224} (channels: 16) \\
& & \\
\(\Downarrow\) \emph{convolve (MobileNet)} & & \emph{upsample}
\(\Uparrow\) \\
& & \\
\texttt{112\ x\ 112} (channels: 16) & \emph{append} \(\Rightarrow\) &
\texttt{112\ x\ 112} (channels: 32) \\
& & \\
\(\Downarrow\) \emph{convolve (MobileNet)} & & \emph{upsample}
\(\Uparrow\) \\
& & \\
\texttt{56\ x\ 56} (channels: 24) & \emph{append} \(\Rightarrow\) &
\texttt{56\ x\ 56} (channels: 64) \\
& & \\
\(\Downarrow\) \emph{convolve (MobileNet)} & & \emph{upsample}
\(\Uparrow\) \\
& & \\
\texttt{28\ x\ 28} (channels: 32) & \emph{append} \(\Rightarrow\) &
\texttt{28\ x\ 28} (channels: 128) \\
& & \\
\(\Downarrow\) \emph{convolve (MobileNet)} & & \emph{upsample}
\(\Uparrow\) \\
& & \\
\texttt{14\ x\ 14} (channels: 96) & \emph{append} \(\Rightarrow\) &
\texttt{14\ x\ 14} (channels: 256) \\
& & \\
\(\Downarrow\) \emph{convolve (MobileNet)} & & \emph{upsample}
\(\Uparrow\) \\
& & \\
\texttt{7\ x\ 7} (channels: 320) & \emph{use as input} \(\Rightarrow\) &
\texttt{7\ x\ 7} (channels: 320) \\
\end{longtable}

Did you notice that the final output has sixteen channels? In the end,
we want to use the \emph{channels} dimension for class scores; so
really, we'll need to have as many channels as there are different
``pixel classes''. This, of course, is task-dependent, so it makes sense
to have a dedicated module take care of it. The top-level module will
then be a composition of the ``U'' part and a final, score-generating
layer.

\hypertarget{top-level-module}{%
\subsection{Top-level module}\label{top-level-module}}

In our task, there will be three pixel classes. The score-producing
submodule can then just be a final convolution, producing three
channels:

\begin{verbatim}
model <- nn_module(
  initialize = function() {
    self$encoder <- encoder()
    self$decoder <- decoder()
    self$output <- nn_conv2d(
      in_channels = 16,
      out_channels = 3,
      kernel_size = 3,
      padding = "same"
    )
  },
  forward = function(x) {
    x %>%
      self$encoder() %>%
      self$decoder() %>%
      self$output()
  }
)
\end{verbatim}

Now that I've already mentioned that there'll be three pixel classes to
tell apart, it's time for full disclosure: What, then, is the task we'll
use this model on?

\hypertarget{dogs-and-cats}{%
\section{Dogs and cats}\label{dogs-and-cats}}

This time, we need an image dataset that has each individual pixel
tagged. One of those, the
\href{https://www.robots.ox.ac.uk/~vgg/data/pets/}{Oxford Pet Dataset},
is an ensemble of cats and dogs. As provided by \texttt{torchdatasets},
it comes with three types of target data to choose from: the overall
class (cat or dog), the individual breed (there are thirty-seven of
them), and a pixel-level segmentation with three categories: foreground,
boundary, and background. The default is exactly the type of target we
need: the segmentation map.

\begin{verbatim}
library(torchvision)
library(torchdatasets)

dir <- "~/.torch-datasets"

ds <- oxford_pet_dataset(root = dir, download = TRUE)
\end{verbatim}

Images come in different sizes. As in the previous chapter, we want all
of them to share the same resolution; one that also fulfills the
requirements of MobileNet v2. The masks will need to be resized in the
same way. So far, then, this looks like a case for the
\texttt{transform\ =} and \texttt{target\_transform\ =} arguments we
encountered in the last chapter. But if we also want to apply data
augmentation, things get more complex.

Imagine we make use of random flipping. An input image will be flipped
-- or not -- according to some probability. But if the image is flipped,
the mask better had be, as well! Input and target transformations are
not independent, in this case.

A solution is to create a wrapper around \texttt{oxford\_pet\_dataset()}
that lets us ``hook into'' the \texttt{.getitem()} method, like so:

\begin{verbatim}
pet_dataset <- torch::dataset(
  inherit = oxford_pet_dataset,
  initialize = function(...,
                        size,
                        normalize = TRUE,
                        augmentation = NULL) {
    self$augmentation <- augmentation
    input_transform <- function(x) {
      x <- x %>%
        transform_to_tensor() %>%
        transform_resize(size)
      if (normalize) {
        x <- x %>%
          transform_normalize(
            mean = c(0.485, 0.456, 0.406),
            std = c(0.229, 0.224, 0.225)
          )
      }
      x
    }
    target_transform <- function(x) {
      x <- torch_tensor(x, dtype = torch_long())
      x <- x[newaxis, ..]
      # interpolation = 0 makes sure we
      # still end up with integer classes
      x <- transform_resize(x, size, interpolation = 0)
      x[1, ..]
    }
    super$initialize(
      ...,
      transform = input_transform,
      target_transform = target_transform
    )
  },
  .getitem = function(i) {
    item <- super$.getitem(i)
    if (!is.null(self$augmentation)) {
      self$augmentation(item)
    } else {
      list(x = item$x, y = item$y)
    }
  }
)
\end{verbatim}

With this wrapper, all we have to do is create a custom function that
decides what augmentation to apply once per input-target \emph{pair},
and manually calls the respective transformation functions.

Here, we flip every second image (on average). And if we do, we flip the
mask as well.

\begin{verbatim}
augmentation <- function(item) {
  vflip <- runif(1) > 0.5

  x <- item$x
  y <- item$y

  if (vflip) {
    x <- transform_vflip(x)
    y <- transform_vflip(y)
  }

  list(x = x, y = y)
}
\end{verbatim}

Since we're at it, let me mention types of augmentation that should be
helpful with slightly different formulations of the task. Why don't we
try (small-ish) random rotations, or translations, or both?

Like this:

\begin{verbatim}
angle <- runif(1, -12, 12)
x <- transform_rotate(x, angle)

# same effect as interpolation = 0, above
y <- transform_rotate(y, angle, resample = 0)
\end{verbatim}

As-is, that piece of code does not work. This is because a rotation will
introduce black pixels, or -- technically speaking -- zeroes in the
tensor. Before, possible target classes went from \texttt{1} to
\texttt{3}. Now there's an additional class, \texttt{0}. As a
consequence, loss computation will expect the model output to have four,
not three, slots in the second dimension -- and fail.

There are several possible workarounds. First, we could take the risk
and assume that in nearly all cases, this will affect only background
pixels. Under that assumption, we can just set all values that have
turned to \texttt{0} to \texttt{2}, the background class.

Another thing we can do is upsize the image, trigger the rotation, and
downsize again:

\begin{verbatim}
angle <- runif(1, -12, 12)

x <- transform_resize(x, size = c(268, 268))
y <- transform_resize(
  y,
  size = c(268, 268),
  interpolation = 0
)

x <- transform_rotate(x, angle)
y <- transform_rotate(y, angle, resample = 0)

x <- transform_center_crop(x, size = c(224, 224))
y <- transform_center_crop(y, size = c(224, 224))
\end{verbatim}

In this specific case, there still is a problem, though. In my
experiments, training performance got a lot worse. This could be because
we have ``boundary'' class. The whole \emph{rison d'tre} of a boundary
is to be sharp; the downside of sharp boundaries is their not dealing
well with resizings.

However, in the real world, segmentation requirements will vary widely.
Maybe you have just two classes, foreground and background. Maybe there
are many. Experimenting with rotations (and translations, that also will
introduce black pixels) cannot hurt.

Back on the main track, we can now make use of the dataset wrapper,
\texttt{pet\_dataset()}, to instantiate the training and validation
sets:

\begin{verbatim}
train_ds <- pet_dataset(root = dir,
                        split = "train",
                        size = c(224, 224),
                        augmentation = augmentation)

valid_ds <- pet_dataset(root = dir,
                        split = "valid",
                        size = c(224, 224))
\end{verbatim}

We create the data loaders, and run the learning rate finder
(fig.~\ref{fig-segmentation-lr-finder}):

\begin{verbatim}
train_dl <- dataloader(
  train_ds,
  batch_size = 32,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 32)

model <- model %>%
  setup(
    optimizer = optim_adam,
    loss = nn_cross_entropy_loss()
  )

rates_and_losses <- model %>% lr_finder(train_dl)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/segmentation-lr-finder.png}

}

\caption{\label{fig-segmentation-lr-finder}Learning rate finder, run on
the Oxford Pet Dataset.}

\end{figure}

From this plot, we conclude that a maximum learning rate of 0.01, at
least when run with a one-cycle strategy, should work fine.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 20, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 2),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.01,
          epochs = 20,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end")
      ),
      verbose = TRUE)
\end{verbatim}

\begin{verbatim}
Epoch 1/20
Train metrics: Loss: 0.6782                                               
Valid metrics: Loss: 0.4433
Epoch 2/20
Train metrics: Loss: 0.3705
Valid metrics: Loss: 0.3331
Epoch 3/20
Train metrics: Loss: 0.315
Valid metrics: Loss: 0.2999
...
...
Epoch 11/20
Train metrics: Loss: 0.1803
Valid metrics: Loss: 0.2161
Epoch 12/20
Train metrics: Loss: 0.1751
Valid metrics: Loss: 0.2191
Epoch 13/20
Train metrics: Loss: 0.1708
Valid metrics: Loss: 0.2203
Early stopping at epoch 13 of 20
\end{verbatim}

This looks like decent improvement; but -- as always -- we want to see
what the model actually says. To that end, we generate segmentation
masks for the first eight observations in the validation set, and plot
them overlayed on the images. (We probably don't need to also display
the ground truth, humans being rather familiar with cats and dogs.)

A convenient way to plot an image and superimpose a mask is provided by
the \texttt{raster} package.

\begin{verbatim}
library(raster)
\end{verbatim}

Pixel intensities have to be between zero and one, which is why in the
dataset wrapper, we have made it so normalization can be switched off.
To plot the actual images, we just instantiate a clone of
\texttt{valid\_ds} that leaves the pixel values unchanged. (The
predictions, on the other hand, will still have to be obtained from the
original validation set.)

\begin{verbatim}
valid_ds_4plot <- pet_dataset(
  root = dir,
  split = "valid",
  size = c(224, 224),
  normalize = FALSE
)
\end{verbatim}

Finally, the predictions are generated, and overlaid over the images
one-by-one (fig.~\ref{fig-segmentation-segmentation}):

\begin{verbatim}
indices <- 1:8

preds <- predict(
  fitted,
  dataloader(dataset_subset(valid_ds, indices))
)

png(
  "pet_segmentation.png",
  width = 1200,
  height = 600,
  bg = "black"
)

par(mfcol = c(2, 4), mar = rep(2, 4))

for (i in indices) {
  mask <- as.array(
    torch_argmax(preds[i, ..], 1)$to(device = "cpu")
  )
  mask <- raster::ratify(raster::raster(mask))

  img <- as.array(valid_ds_4plot[i][[1]]$permute(c(2, 3, 1)))
  cond <- img > 0.99999
  img[cond] <- 0.99999
  img <- raster::brick(img)

  # plot image
  raster::plotRGB(img, scale = 1, asp = 1, margins = TRUE)
  # overlay mask
  plot(
    mask,
    alpha = 0.4,
    legend = FALSE,
    axes = FALSE,
    add = TRUE
  )
}

dev.off()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/segmentation-segmentation.png}

}

\caption{\label{fig-segmentation-segmentation}Cats and dogs: Sample
images and predicted segmentation masks.}

\end{figure}

This looks pretty reasonable!

Now, it's time we let images be, and look at different application
domains. In the next chapter, we explore deep learning on tabular data.

\hypertarget{sec:tabular-data}{%
\chapter{Tabular data}\label{sec:tabular-data}}

So far, we've been working with images exclusively. With images, pixel
values are arranged in a grid; or several grids, actually, in case there
are several channels. However many there may be, all values are of the
same type: integers between \texttt{0} and \texttt{255}, for example, or
(when normalized) floats in the interval from \texttt{0} to \texttt{1}.
With tabular (a.k.a.: spreadsheet) data, however, you could have a mix
of numbers, (single) characters, and (single- or multi-token) text.

As we discussed, way back when talking about tensor creation,
\texttt{torch} cannot work with non-numerical data. In consequence,
characters and text will have to be pre-processed and encoded
numerically. For single characters, or individual strings, this poses
few difficulties: just convert the R character vectors to factors, and
then, the factors to integers. Pre-processing for regular text, however,
is a much more involved topic, one we can't go into here.

Now, assume that we're presented with an all-numeric data frame. (Maybe
it's been all-numeric from the outset; maybe we've followed the
above-mentioned ``integerization-via-factors'' recipe to make it so.)
Still, what these numbers mean may differ between features.

\hypertarget{types-of-numerical-data-by-example}{%
\section{Types of numerical data, by
example}\label{types-of-numerical-data-by-example}}

A classic distinction is that between interval, ordinal, and categorical
data. Assume that for three individuals, we're told that, for some
feature, their respective scores are \texttt{2}, \texttt{1}, and
\texttt{3}. What are we to make of this?

First, the numbers could just be encoding ``artifacts''. That would be
the case if, say, \texttt{1} stood for \emph{apple}, \texttt{2} for
\emph{orange}, and \texttt{3}, for \emph{pineapple}. Then, we'd have
categorical\index{categorical data} data.

Second, the numbers could represent grades, \texttt{1} denoting the
best, \texttt{2} the second-best, \texttt{3} the one thereafter \ldots{}
and so on. In this case, there is a ranking, or an ordering, between
values. We have no reason to assume, though, that the distance between
\texttt{1} and \texttt{2} is the same as that between \texttt{2} and
\texttt{3}. These are ordinal\index{ordinal data} data.

Finally, maybe those distances \emph{are} the same. Now, we're dealing
with interval\index{interval data} data.

Put in this way, the distinction may seem trivial. However, with
real-world datasets, it is not always easy to know what type of data
you're dealing with.

To illustrate, we now inspect the popular \emph{heart disease} dataset,
available on the \href{https://archive.ics.uci.edu/ml/index.php}{UCI
Machine Learning Repository}. It is this dataset we are going to build a
classifier for, in this chapter.

In what follows, the aspiration is not to be completely sure we're
``getting it right''; instead, it's about showing how to approach the
task in a conscientious (if I may say so) way.

We start by loading the dataset. Here \texttt{heart\_disease} is the
target, and missing values are indicated by a question mark.

\begin{verbatim}
library(torch)
library(luz)

library(purrr)
library(readr)
library(dplyr)

uci <- "https://archive.ics.uci.edu"
ds_path <- "ml/machine-learning-databases/heart-disease"
ds_file <- "processed.cleveland.data"

download.file(
 file.path(uci, ds_path, ds_file),
 destfile = "resources/tabular-heart.csv"
)

heart_df <- read_csv(
  "resources/tabular-heart.csv",
  col_names = c(
    "age", 
    # 1 = male; 0 = female
    "sex", 
    # chest pain type
    # (1 = typical angina, 2 = atypical angina,
    #  3 = non-anginal pain, 4 = asymptomatic)
    "pain_type", 
    # in mm Hg on admission
    "resting_blood_pressure", 
    # serum cholesterol in mg/dl
    "chol", 
    # > 120 mg/dl, true (1) or false (0)
    "fasting_blood_sugar", 
    # 0 = normal, 1 = ST-T wave abnormality
    # (T wave inversions and/or ST elevation
    # or depression of > 0.05 mV),
    # 2 = probable or definite left ventricular
    # hypertrophy by Estes' criteria
    "rest_ecg", 
    # during exercise
    "max_heart_rate", 
     # exercise induced angina (1 = yes, 0 = no),
    "ex_induced_angina",
    # ST depression induced by exercise relative to rest 
    "old_peak", 
    # slope of the peak exercise ST segment
    # (1 = upsloping, 2 = flat, 3 = downsloping) 
    "slope", 
    # number of major vessels (0-3) colored by fluoroscopy
    "ca", 
    # 3 = normal; 6 = fixed defect; 7 = reversible defect
    "thal", 
    # 1-4 = yes; 0 = no
    "heart_disease" 
  ),
  na = "?")
\end{verbatim}

As you see, I've annotated the features with information given by the
dataset creators.

Based on this information, as well as the actual values in the dataset,
the following look like interval data to me:

\begin{itemize}
\item
  \texttt{age},
\item
  \texttt{resting\_blood\_pressure},
\item
  \texttt{chol},
\item
  \texttt{max\_heart\_rate}, and
\item
  \texttt{old\_peak} .
\end{itemize}

At the other end of the dimension, some features are clearly conceived
as binary here: namely, the predictors \texttt{sex},
\texttt{fasting\_blood\_sugar}, and \texttt{ex\_induced\_angina}, as
well as the target, \texttt{heart\_disease}. Predictors that are binary,
with values either zero or one, are usually treated as numerical. No
information is lost that way. An alternative would be to represent them
by length-two vectors: either \texttt{{[}0,\ 1{]}} (when the given value
is \texttt{0}) or \texttt{{[}1,\ 0{]}} (when the given value is
\texttt{1}). This is called \emph{one-hot
encoding}\index{one-hot-encoding}, ``one-hot'' referring to just a
single position in the vector being non-zero (namely, the one
corresponding to the category in question). Normally, this is only done
when there are more than two categories, and we'll get back to this
technique when we implement the \texttt{dataset()} for this problem.

We now have just \texttt{pain\_type}, \texttt{rest\_ecg},
\texttt{slope}, \texttt{ca}, and \texttt{thal} remaining. Of those,
\texttt{pain\_type} , \texttt{rest\_ecg,} and \texttt{thal} look
categorical, or maybe ordinal, to some degree. Normally in machine
learning, ordinal data are treated as categorical, and unless the number
of different categories is high, this should not result in significant
loss of information.

What about \texttt{slope}? To a machine learning person, the naming
alone seems to suggest a continuous feature; and values as nicely
ordered as 1, 2, 3 might make you think that the variable must be at
least ordinal. However, a quick web search already turns up a different,
and much more complicated, reality.\footnote{See, e.g.,
  \url{https://en.my-ekg.com/how-read-ekg/st-segment.html}, or
  \url{https://ecg.utah.edu/lesson/10}, or
  \href{https://en.wikipedia.org/wiki/ST_segment}{Wikipedia}.} We'll
thus definitely want to treat this variable as categorical. (If this
were a real-world task, we should try consulting a domain expert, to
find out whether there's a better solution.)

Finally, for \texttt{ca}, we -- or me, actually -- don't know the
implication of how many major blood vessels (zero to four) got
\emph{colored by fluoroscopy}. (Again, if you have access to a domain
expert, make use of the opportunity and ask them.) The safest way is to
not assume an equal distance between measurements, but treat this
feature as merely ordinal, and thus, categorical.

Now that we've discussed what to do, we can implement our data provider:
\texttt{heart\_dataset()}.

\hypertarget{a-torch-dataset-for-tabular-data}{%
\section{\texorpdfstring{A \texttt{torch} \texttt{dataset} for tabular
data}{A torch dataset for tabular data}}\label{a-torch-dataset-for-tabular-data}}

Beyond adequate feature representation, there is one additional thing to
take care of. The dataset contains unknown values, which we coded as
\texttt{NA}:

\begin{verbatim}
which(is.na(heart_df), arr.ind = TRUE)
\end{verbatim}

\begin{verbatim}
     row col
[1,] 167  12
[2,] 193  12
[3,] 288  12
[4,] 303  12
[5,]  88  13
[6,] 267  13
\end{verbatim}

Fortunately, these occur in two columns only: \texttt{thal} and
\texttt{ca}.

\begin{verbatim}
heart_df %>% group_by(thal) %>% summarise(n())
\end{verbatim}

\begin{verbatim}
tibble: 4  2
thal `n()`
<dbl> <int>
  1      3   166
  2      6    18
  3      7   117
  4     NA     2
\end{verbatim}

\begin{verbatim}
heart_df %>% group_by(ca) %>% summarise(n())
\end{verbatim}

\begin{verbatim}
A tibble: 5  2
ca `n()`
<dbl> <int>
  1     0   176
  2     1    65
  3     2    38
  4     3    20
  5    NA     4
\end{verbatim}

Seeing how we're totally clueless as to what caused these missing
values, and considering that -- conveniently -- both features in
question are categorical and of low cardinality, it seems easiest to
just have those \texttt{NA}s represented by an additional factor value.

That decided, we can implement \texttt{heart\_dataset()}. Numerical
features will be scaled, a measure that should always be taken when
they're of different orders of magnitude. This will significantly speed
up training.

As to the categorical features, one thing we \emph{could} do is one-hot
encode them. For example:

\begin{verbatim}
nnf_one_hot(
  torch_tensor(
    heart_df$slope,
    dtype = torch_long()
  )
) %>% print(n = 5)
\end{verbatim}

\begin{verbatim}
torch_tensor
 0  0  1
 0  1  0
 0  1  0
 0  0  1
 1  0  0
... [the output was truncated (use n=-1 to disable)]
[ CPULongType{303,3} ]
\end{verbatim}

With one-hot encoding, we guarantee that each feature value differs from
all others exactly to the same degree. However, we can do better. We can
use a technique called \emph{embedding} to represent these feature
values in a space where they are \emph{not} all equally distinct from
one another.

We'll see how that works when we build the model, but now, we need to
make sure the prerequisites are satisfied. Namely, \texttt{torch}'s
embedding modules expect their input to be integers, not one-hot encoded
vectors (or anything else). So what we'll do is convert the categorical
features to factors, and from there, to integers.

Putting it all together, we arrive at the following \texttt{dataset()}
definition:

\begin{verbatim}
heart_dataset <- dataset(
  initialize = function(df) {
    self$x_cat <- self$get_categorical(df)
    self$x_num <- self$get_numerical(df)
    self$y <- self$get_target(df)
  },
  .getitem = function(i) {
    x_cat <- self$x_cat[i, ]
    x_num <- self$x_num[i, ]
    y <- self$y[i]
    list(x = list(x_cat, x_num), y = y)
  },
  .length = function() {
    dim(self$y)[1]
  },
  get_target = function(df) {
    heart_disease <- ifelse(df$heart_disease > 0, 1, 0)
    heart_disease
  },
  get_numerical = function(df) {
    df %>%
      select(
        -(c(
          heart_disease, pain_type,
          rest_ecg, slope, ca, thal
        ))
      ) %>%
      mutate(across(.fns = scale)) %>%
      as.matrix()
  },
  get_categorical = function(df) {
    df$ca <- ifelse(is.na(df$ca), 999, df$ca)
    df$thal <- ifelse(is.na(df$thal), 999, df$thal)
    df %>%
      select(
        pain_type, rest_ecg, slope, ca, thal
      ) %>%
      mutate(
        across(.fns = compose(as.integer, as.factor))
      ) %>%
      as.matrix()
  }
)
\end{verbatim}

Let's see if the output it produces matches our expectations.

\begin{verbatim}
ds <- heart_dataset(heart_df)
ds[1]
\end{verbatim}

\begin{verbatim}
$x
$x[[1]]
pain_type  rest_ecg     slope        ca      thal 
        1         3         3         1         2 

$x[[2]]
                   age                    sex 
            0.94715962             0.68506916 
resting_blood_pressure                   chol 
            0.75627397            -0.26446281 
   fasting_blood_sugar         max_heart_rate 
            2.39048352             0.01716893 
     ex_induced_angina               old_peak 
           -0.69548004             1.08554229 


$y
[1] 0
\end{verbatim}

It does.

This is a small dataset, so we'll forego creation of separate test and
validation sets.

\begin{verbatim}
train_indices <- sample(
  1:nrow(heart_df), size = floor(0.8 * nrow(heart_df)))
valid_indices <- setdiff(
  1:nrow(heart_df), train_indices)

train_ds <- dataset_subset(ds, train_indices)
train_dl <- train_ds %>% 
  dataloader(batch_size = 256, shuffle = TRUE)

valid_ds <- dataset_subset(ds, valid_indices)
valid_dl <- valid_ds %>% 
  dataloader(batch_size = 256, shuffle = FALSE)
\end{verbatim}

We're ready to move on to the model. But first, let's talk about
\emph{embeddings}.

\hypertarget{embeddings-in-deep-learning-the-idea}{%
\section{\texorpdfstring{Embeddings\index{embeddings} in deep learning:
The
idea}{Embeddings in deep learning: The idea}}\label{embeddings-in-deep-learning-the-idea}}

The main idea behind embeddings, the way this term is used in deep
learning, is to go beyond the default ``all are equally distinct from
each other'' representation of categorical values.

As in one-hot encoding, scalars get mapped to vectors. But this time,
there's no restriction as to how many slots may be non-empty, or as to
the values they may take. For example, integers 1, 2, and 3 could get
mapped to the following tensors:

\begin{verbatim}
one <- torch_tensor(c(1.555, 0.21, -3.33, 0.0007, 0.07))
two <- torch_tensor(c(0.33, -0.03, -2.177, 1.1, 0.0005))
three <- torch_tensor(c(-0.33, 2.99, 1.77, 1.08, 3.001))
\end{verbatim}

Now, we can use \texttt{nnf\_cosine\_similarity()} to find out how close
to each other these vectors are. Working, for convenience, in two
dimensions, say we have two parallel vectors, pointing in the same
direction. The angle between them is zero, and its cosine is one:

\begin{verbatim}
nnf_cosine_similarity(
  torch_ones(2),
  torch_ones(2) * 2.5,
  dim = 1
)
\end{verbatim}

\begin{verbatim}
torch_tensor
1
[ CPUFloatType{} ]
\end{verbatim}

Thus, a value of one indicates maximal similarity. In contrast, now take
them to still be parallel, but pointing in opposite directions. The
angle is one-hundred-eighty degrees, and the cosine is minus one:

\begin{verbatim}
nnf_cosine_similarity(
  torch_ones(2),
  -1.5 * torch_ones(2),
  dim = 1
)
\end{verbatim}

\begin{verbatim}
torch_tensor
-1
[ CPUFloatType{} ]
\end{verbatim}

These vectors are maximally dissimilar. In-between, we have angles
around ninety degrees, with vectors being (approximately) orthogonal; or
``independent'', in common parlance. With an angle of exactly ninety
degrees, the cosine is zero:

\begin{verbatim}
nnf_cosine_similarity(
  torch_tensor(c(1, 0)),
  torch_tensor(c(0, 1)),
  dim = 1
)
\end{verbatim}

\begin{verbatim}
torch_tensor
0
[ CPUFloatType{} ]
\end{verbatim}

Things work analogously in higher dimensions. So, we can determine which
of the above vectors \texttt{one}, \texttt{two}, and \texttt{three} are
closest to each other:

\begin{verbatim}
nnf_cosine_similarity(one, two, dim = 1)
nnf_cosine_similarity(one, three, dim = 1)
nnf_cosine_similarity(two, three, dim = 1)
\end{verbatim}

\begin{verbatim}
torch_tensor
0.855909
[ CPUFloatType{} ]

torch_tensor
-0.319886
[ CPUFloatType{} ]

torch_tensor
-0.245948
[ CPUFloatType{} ]
\end{verbatim}

Looking at how those were defined, these values make sense.

\hypertarget{embeddings-in-deep-learning-implementation}{%
\section{Embeddings in deep learning:
Implementation}\label{embeddings-in-deep-learning-implementation}}

By now you probably agree that embeddings are useful. But how do we get
them?

Conveniently, these vectors are learned as part of model training.
\emph{Embedding modules} are modules that take integer inputs and learn
to map them to vectors.

When creating such a module, you specify how many different integers
there are (\texttt{num\_embeddings}), and how long you want the learned
vectors to be (\texttt{embedding\_dim}). Together, these parameters tell
the module how its weight matrix should look. The weight matrix is
nothing but a look-up table (a mutable one, though!) that maps integers
to corresponding vectors:

\begin{verbatim}
module <- nn_embedding(num_embeddings = 3, embedding_dim = 5)
module$weight
\end{verbatim}

\begin{verbatim}
torch_tensor
 2.3271  0.0894  0.6558 -0.5836 -0.1074
 0.0367  0.1822 -0.0446  0.2059 -0.7540
-0.7577 -1.7773 -0.6619  1.2884  0.3946
[ CPUFloatType{3,5} ][ requires_grad = TRUE ]
\end{verbatim}

At module creation, these mappings are initialized randomly. Still, we
can test that the code does what we want by calling the module on some
feature -- \texttt{slope}, say:

\begin{verbatim}
# slope
module(ds[1]$x[[1]][3])
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.7577 -1.7773 -0.6619  1.2884  0.3946
[ CPUFloatType{1,5} ][ grad_fn = <EmbeddingBackward0> ]
\end{verbatim}

Now, you could say that we glossed over, with a certain nonchalance, the
question of how these mappings are optimized. Technically, this works
like for any module: by means of backpropagation. \emph{But there is an
implication:} It follows that the overall model, and even more
importantly, the given \emph{task}, will determine how ``good'' those
learned mappings are.

And yet, there is something even more important: the \emph{data}.

Sure, that goes without saying, you may think. But when embeddings are
used, the learned mappings are sometimes presented as an additional
outcome, a surplus benefit, of sorts. For example, the model itself
might be a classifier, predicting whether people are going to default on
a loan or not. Now assume that there is an input feature -- ethnicity,
say -- that is processed using embeddings. Once the model's been
trained, the acquired representation is extracted. In turn, that
representation will reflect all problems -- biases, injustices,
distortions -- that are present in the training dataset.

Below, I will show how to obtain and plot such a representation. To a
domain expert, this representation may or may not seem adequate; in any
case, no harm is likely to be caused \emph{in this example}. However,
when working on real-world tasks, we always have to be aware of possible
harms, and rigorously analyse any biases and assumptions inherent in the
training workflow.

Getting back to the implementation, here is the embedding module we use
for our task. Actually, there is no \emph{single} embedding module;
there is one for each categorical feature. The wrapper,
\texttt{embedding\_module()}, keeps them all in an
\texttt{nn\_module\_list()}, and when called, iterates over them and
concatenates their outputs:

\begin{verbatim}
embedding_module <- nn_module(
  initialize = function(cardinalities, embedding_dim) {
    self$embeddings <- nn_module_list(
      lapply(
        cardinalities,
        function(x) {
          nn_embedding(
            num_embeddings = x, embedding_dim = embedding_dim
          )
        }
      )
    )
  },
  forward = function(x) {
    embedded <- vector(
      mode = "list",
      length = length(self$embeddings)
    )
    for (i in 1:length(self$embeddings)) {
      embedded[[i]] <- self$embeddings[[i]](x[, i])
    }
    torch_cat(embedded, dim = 2)
  }
)
\end{verbatim}

This wrapper -- let's call it ``embedder'' -- will be one of the modules
that make up the top-level model.

\hypertarget{model-and-model-training}{%
\section{Model and model training}\label{model-and-model-training}}

The top-level module's logic is straightforward. For the categorical
part of its input, it delegates to the embedder, and to the embeddings
obtained it appends the numerical part. The resulting tensor is then
passed through a sequence of linear modules:

\begin{verbatim}
model <- nn_module(
  initialize = function(cardinalities,
                        num_numerical,
                        embedding_dim,
                        fc1_dim,
                        fc2_dim) {
    self$embedder <- embedding_module(
      cardinalities,
      embedding_dim
    )
    self$fc1 <- nn_linear(
      embedding_dim * length(cardinalities) + num_numerical,
      fc1_dim
    )
    self$drop1 <- nn_dropout(p = 0.7)
    self$fc2 <- nn_linear(fc1_dim, fc2_dim)
    self$drop2 <- nn_dropout(p = 0.7)
    self$output <- nn_linear(fc2_dim, 1)
  },
  forward = function(x) {
    embedded <- self$embedder(x[[1]])
    all <- torch_cat(list(embedded, x[[2]]), dim = 2)
    score <- all %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$drop1() %>%
      self$fc2() %>%
      nnf_relu() %>%
      self$drop2() %>%
      self$output()
    score[, 1]
  }
)
\end{verbatim}

Looking at the final output, you see that these are raw scores, not
probabilities. With a binary target, this means we'll make use of
\texttt{nn\_bce\_with\_logits\_loss()} to train the model.

Now, we still need some housekeeping and configuration:

\begin{verbatim}
# cardinalities of categorical features
cardinalities <- heart_df %>%
  select(pain_type, rest_ecg, slope, ca, thal) %>%
  mutate(across(.fns = as.factor)) %>%
  summarise(across(.fns = nlevels))

# cardinalities of categorical features,
# adjusted for presence of NAs in ca and thal
cardinalities <- cardinalities + c(0, 0, 0, 1, 1) 

# number of numerical features
num_numerical <- ncol(heart_df) - length(cardinalities) - 1

embedding_dim <- 7

fc1_dim <- 32
fc2_dim <- 32
\end{verbatim}

Note here the requested embedding dimension(s), \texttt{embedding\_dim}.

Usual best practice would choose lower values, corresponding, roughly,
to half a feature's cardinality. For example, if there were thirty
different values for some category, we might go with a vector length of
about fifteen. And definitely, this is what I'd do if I \emph{had}
thirty values. But in this example, cardinalities are much lower: two,
three, four, or five. (And that's already with \texttt{NA} taken as an
additional factor value.) Halving those numbers would hardly leave any
representational capacity. So here, I went the opposite way: give the
model a significantly bigger space to play with. (Beyond seven, the
chosen value, I didn't see further training improvements.)

All preparatory work done, we can train the model. Normally, at this
point we'd run the learning rate finder. Here, the dataset really is too
small for that to make sense, at least without substantially tweaking
the learning rate finder's default settings. Also, with a dataset as
tiny as this, experimentation takes very little time; and a few quick
experiments are what the learning rate chosen is based on.

\begin{verbatim}
fitted <- model %>%
  setup(
    optimizer = optim_adam,
    loss = nn_bce_with_logits_loss(),
    metrics = luz_metric_binary_accuracy_with_logits()
  ) %>%
  set_hparams(
    cardinalities = cardinalities,
    num_numerical = num_numerical,
    embedding_dim = embedding_dim,
    fc1_dim = fc1_dim, fc2_dim
  ) %>%
  set_opt_hparams(lr = 0.001) %>%
  fit(train_dl,
    epochs = 200,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_early_stopping(patience = 10)
    ),
    verbose = TRUE
  )
\end{verbatim}

\begin{verbatim}
# Epoch 1/200
# Train metrics: Loss: 0.7445 - Acc: 0.4091
# Valid metrics: Loss: 0.6988 - Acc: 0.541
# Epoch 2/200
# Train metrics: Loss: 0.7036 - Acc: 0.5248
# Valid metrics: Loss: 0.6966 - Acc: 0.5246
# Epoch 3/200
# Train metrics: Loss: 0.7029 - Acc: 0.5124
# Valid metrics: Loss: 0.6946 - Acc: 0.5082
# ...
# ...
# Epoch 124/200
# Train metrics: Loss: 0.3884 - Acc: 0.8512
# Valid metrics: Loss: 0.4026 - Acc: 0.8525
# Epoch 125/200
# Train metrics: Loss: 0.3961 - Acc: 0.8471
# Valid metrics: Loss: 0.4023 - Acc: 0.8525
# Epoch 126/200
# Train metrics: Loss: 0.359 - Acc: 0.8554
# Valid metrics: Loss: 0.4019 - Acc: 0.8525
# Early stopping at epoch 126 of 200
\end{verbatim}

\begin{verbatim}
fitted %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/tabular-heart-disease-fit.png}

}

\caption{\label{fig-tabular-heart-disease-fit}Losses and accuracies
(training and validation, resp.) for binary heart disease
classification.}

\end{figure}

As you see (fig.~\ref{fig-tabular-heart-disease-fit}), model training
went smoothly, and yielded good accuracy.

Before we leave the topic of tabular data, here's how to extract,
post-process, and visualize the learned representations.

\hypertarget{embedding-generated-representations-by-example}{%
\section{Embedding-generated representations by
example}\label{embedding-generated-representations-by-example}}

Here, example-wise, is the weight matrix for \texttt{slope}.

\begin{verbatim}
embedding_weights <- vector(mode = "list")

for (i in 1:length(fitted$model$embedder$embeddings)) {
  embedding_weights[[i]] <-
    fitted$model$embedder$embeddings[[i]]$
    parameters$weight$to(device = "cpu")
}

slope_weights <- embedding_weights[[3]]
slope_weights
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.9226 -1.0282  0.8935  0.3152  0.5481  0.8376  0.9990
 0.0604  0.1904  0.6788  0.8542  0.8007  1.5226 -0.1789
 1.2504 -0.0827 -0.7259  1.2885 -1.7847  0.1813  0.4418
[ CPUFloatType{3,7} ][ requires_grad = TRUE ]
\end{verbatim}

For visualization, we'd like to reduce the number of dimensions from
seven to two. We can accomplish that by running PCA -- Principal
Components Analysis -- using R's native \texttt{prcomp()}:

\begin{verbatim}
pca <- prcomp(slope_weights, center = TRUE, scale = TRUE)
pca
\end{verbatim}

\begin{verbatim}
Standard deviations (1, .., p=3):
[1] 2.138539e+00 1.557771e+00 2.173695e-16

Rotation (n x k) = (7 x 3):
            PC1         PC2         PC3
[1,]  0.4650931 -0.06650143  0.18974889
[2,]  0.2915618 -0.50187753  0.42034629
[3,] -0.4539313 -0.15412668  0.46092035
[4,]  0.4562585 -0.14058058 -0.16106015
[5,] -0.4203277 -0.28128658  0.29209764
[6,] -0.2903728 -0.50317497 -0.68060609
[7,] -0.1531762  0.60652402  0.01925451
\end{verbatim}

This printout reflects two pieces of information: the standard
deviations of the principal components (also available as
\texttt{pca\$sdev}), and the matrix of variable loadings (also available
as \texttt{pca\$rotation}).

The former reflect how important the resulting components are; we use
them to decide if reduction to two components seems permissible. Here's
how much variance is explained by the three components each:

\begin{verbatim}
(pca$sdev^2 / sum(pca$sdev^2)) %>% round(2)
\end{verbatim}

\begin{verbatim}
[1] 0.65 0.35 0.00
\end{verbatim}

From that output, leaving out the third component is more than
permissible.

The matrix of variable loadings, on the other hand, tells us how big a
role each variable (here: each slot in the learned embeddings) plays in
determining the ``meaning'' of a component. Here, a visualization is
more helpful than the raw numbers
(fig.~\ref{fig-tabular-heart-disease-biplot}):

\begin{verbatim}
biplot(pca)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/tabular-heart-disease-biplot.png}

}

\caption{\label{fig-tabular-heart-disease-biplot}\texttt{heart\_disease\$slope}:
PCA of embedding weights, biplot visualizing factor loadings.}

\end{figure}

This plot could be taken as indicating that -- from a purely
representational standpoint, i.e., not taking into account training
performance -- an embedding dimensionality of four would have been
sufficient.

Finally, how about the main thing we're after: a representation of slope
categories in two-dimensional space?

This information is provided by \texttt{pca\$x}. It tells us how the
original input categories relate to the principal components.

\begin{verbatim}
pca$x
\end{verbatim}

\begin{verbatim}
            PC1        PC2           PC3
[1,] -1.9164879  1.1343079  1.783213e-17
[2,] -0.3903307 -1.7761457  6.993398e-16
[3,]  2.3068187  0.6418377 -4.977644e-16
\end{verbatim}

Leaving out the third component, the distribution of categories in
(two-dimensional) space is easily visualized
(fig.~\ref{fig-tabular-heart-disease-pca}):

\begin{verbatim}
library(ggplot2)
library(ggrepel)

slopes <- c("up", "flat", "down")

pca$x[, 1:2] %>%
  as.data.frame() %>%
  mutate(class = slopes) %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_label_repel(aes(label = class)) +
  coord_cartesian(
    xlim = c(-2.5, 2.5),
    ylim = c(-2.5, 2.5)
  ) +
  theme(aspect.ratio = 1) +
  theme_classic()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/tabular-heart-disease-pca.png}

}

\caption{\label{fig-tabular-heart-disease-pca}\texttt{heart\_disease\$slope}:
PCA of embedding weights, locating the original input values in
two-dimensional space.}

\end{figure}

Whether this representation makes sense, I'll leave to the experts to
judge. My goal here was to demonstrate the technique, so you can employ
it when it \emph{does} yield some insight. Even when not, embedding
modules do contribute substantially to training success for
categorical-feature or mixed-feature data.

\hypertarget{sec:time-series}{%
\chapter{Time series}\label{sec:time-series}}

In this chapter, we'll again look at a new type of data: time series.
Previously, moving on from images to tabular data, we found a
substantial difference in that image data are homogeneous, while tabular
data aren't. With images, individual values correspond to pixels, or
positions on a grid. With tabular data, values can -- in principle -- be
``anything''; but most often, we deal with a mix of categorical,
ordinal, and numerical data. However, both types of application have one
thing in common: All values relate to the same point in time.

With time series, we're facing a new situation. Assume the series is
one-dimensional, that is, it has a single feature. Thus, the data type
is homogeneous. But now, the input is a sequence. What follows?

Before, when a mix of data types were present, we found we had to do
some pre-processing up-front. We also saw that, by adding in a new type
of module - the embedding module -- we could refine and enhance the
overall (linear) model. Now though, a bigger change is needed. We again
will have to do some pre-processing; but this time, we'll also need a
different type of top-level \emph{model}, a type as different from the
standard feed-forward architecture as is the convolutional one we
already studied.

\hypertarget{deep-learning-for-sequences-the-idea}{%
\section{Deep learning for sequences: the
idea}\label{deep-learning-for-sequences-the-idea}}

Say we have a sequence of daily average temperatures, measured in
degrees Celsius: \texttt{-1.1,\ 2.0,\ -0.2,\ -0.9,\ 4.5,\ -3.6,\ -9.1}.
Clearly, these values are not independent; we'd hardly guess that the
very next measurement would result in in, say, \texttt{21}. In fact, if
these seven averages were all you're given, your best guess for the next
day would probably just be \texttt{-9.1}. But when people say ``time
series'', they have longer sequences in mind. With longer sequences, you
can try to detect patterns, such as trends or periodicities. And that's
what the established techniques in time series analysis do.

For a deep learning model to do the same, it first of all has to
``perceive'' individual values as sequential. We make that happen by
increasing tensor dimensionality by one, and using the additional
dimension for sequential ordering. Now, the model has to do something
useful with that. Think back of what it is a linear model is doing: It
takes input \(\mathbf{X}\), multiplies by its weight matrix
\(\mathbf{W}\), and adds bias vector \(\mathbf{b}\):

\[
f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b}
\]

In sequence models, this type of operation is still present; it's just
that now, it is executed for every time step -- i.e., every position in
the sequence -- in isolation. But this means that now, the relationship
\emph{between time steps} has to be taken care of. To that end, the
module takes what was obtained at the previous time step, applies a
different weight matrix, and adds a different bias vector. This, in
itself, is again an affine transformation, -- just not of the input, but
of what is called the previous \emph{state}\index{statefulness (RNN)}.
The outputs from both affine computations are added, and the result then
serves as prior state to the computation due at the next time step.

In other words, \emph{at each time step}, two types of information are
combined: the (weight-transformed) input for the current time step, and
the (weight-transformed) state that resulted from processing the
previous one. In math:

\[
state_{(t)} = f(\mathbf{W_{input}}\mathbf{X_{(t)}} + \mathbf{b_{input}} + \mathbf{W_{state}}\mathbf{X_{(t-1)}} + \mathbf{b_{state}})
\]

This logic specifies a \emph{recurrence relation}\index{recurrence}, and
modules implementing it are called \emph{recurrent neural networks}
(RNNs). In the next section, we'll implement such a module ourselves;
you'll see how, in code, that recurrence maps to straightforward
iteration. Before, two remarks.

First, in the above formula, the function applied to the sum of the two
transformations represents an activation; typically for RNNs, the
default is the hyperbolic tangent,
\texttt{torch\_tanh()}\index{activation!hyperbolic tangent}.

Second, in the official \texttt{torch} documentation, you'll see the
formula written this way:

\[
h_{(t)} = f(\mathbf{W_{i\_h}\mathbf{X_{(t)}} }+ \mathbf{b_{i\_h}} + \mathbf{W_{h\_h}\mathbf{X}_{(t-1)}} + \mathbf{b}_{h\_h})
\]

Here \(h\) stands for ``hidden'', as in ``hidden state'', and subscripts
\(i\_h\) and \(h\_h\) stand for ``input-to-hidden'' and
``hidden-to-hidden'', respectively. The reason I'd like to de-emphasize
the ``hidden'' in ``hidden state'' is because in the \texttt{torch}
implementation, the state is not necessarily hidden from the user.
You'll see what I mean below. (Pretty soon, I'll give up resistance
against the term, though, since it is ubiquitous in descriptive prose as
well as code (for example, as regards variable naming). But I wanted to
state this clearly at least once, so you won't be confused when mapping
your mental model of the algorithm to the behavior of \texttt{torch}
RNNs.

\hypertarget{a-basic-recurrent-neural-network}{%
\section{A basic recurrent neural
network}\label{a-basic-recurrent-neural-network}}

In the above discussion, we identified two basic things a recurrent
neural network has to do: (1) iterate over the input sequence; and (2)
execute the ``business logic'' of a sequence, that is, combine
information from the previous as well as the current time step.

Commonly, these duties are divided between two different objects. One,
referred to as the ``cell'', implements the logic. The other takes care
of the iteration. The reason for this modularization is that both
``inner'' and ``outer'' logic should be modifiable independently. For
example, you might want to keep the way you iterate over time steps, but
modify what happens at each point in the iteration. This will get more
concrete later, when we talk about the most-used sub-types of RNNs.

In our basic implementation of a basic RNN, both cell and iteration
handler are \texttt{nn\_module()}s. First, we have the \emph{cell}.

\hypertarget{basic-rnn_cell}{%
\subsection{\texorpdfstring{Basic
\texttt{rnn\_cell()}\index{RNN cell}}{Basic rnn\_cell()}}\label{basic-rnn_cell}}

The logic combines two affine operations; and affine operations are just
what linear modules are for. We therefore just have our cell delegate to
two linear modules, append the respective outputs, and apply a
\emph{tanh} activation.

As already alluded to above, in naming the modules and parameters, I'm
following the \texttt{torch} conventions, so things will sound familiar
when we move on to actual \texttt{torch} modules. Most notably, this
includes referring to the state as ``hidden state'', and thus, to its
dimensionality as \texttt{hidden\_size}, even though state is hidden
from the user only under certain circumstances (which we'll come to).

\begin{verbatim}
library(torch)
library(zeallot) # for destructuring using %<-%

rnn_cell <- nn_module(
  initialize = function(input_size, hidden_size) {
    self$linear_i_h <- nn_linear(input_size, hidden_size)
    self$linear_h_h <- nn_linear(hidden_size, hidden_size)
  },
  forward = function(x, prev_state) {
    torch_tanh(self$linear_i_h(x) +
      self$linear_h_h(prev_state))
  }
)
\end{verbatim}

From the way the cell has been defined, we see that to instantiate it,
we need to pass \texttt{hidden\_size} and \texttt{input\_size} -- the
latter referring to the number of features in the dataset. Let's make
those 3 and 1, respectively:

\begin{verbatim}
cell <- rnn_cell(input_size = 1, hidden_size = 3)
\end{verbatim}

As a quick test, we call the module on a (tiny) batch of data, passing
in the previous (or: initial) state. As in the actual \texttt{torch}
implementation, the state is initialized to an all-zeros tensor:

\begin{verbatim}
cell(torch_randn(2, 1), torch_zeros(2, 3))
\end{verbatim}

\begin{verbatim}
torch_tensor
-0.6340  0.9571 -0.9886
-0.3007  0.9201 -0.9689
[ CPUFloatType{2,3} ][ grad_fn = <TanhBackward0> ]
\end{verbatim}

Note the dimensionality of the output. For each batch item, we get the
new state, of size \texttt{hidden\_size}.

Now, a cell is not normally supposed to be called by the user; instead,
we should call the to-be-defined \texttt{rnn\_module()}. That module
will take care of the iteration, delegating to an instance of
\texttt{rnn\_cell()} at each step. Let's implement this one next.

\hypertarget{basic-rnn_module}{%
\subsection{\texorpdfstring{Basic
\texttt{rnn\_module()}}{Basic rnn\_module()}}\label{basic-rnn_module}}

Conceptually, this module is easily characterized -- it iterates over
points in time. But there are a few things to note in the implementation
that follows.

First, note that it expects a single argument to \texttt{forward()}, not
two -- there is no need to pass in an initial state. (In actual
\texttt{torch} implementations, the user can pass an initial state to
start from, if they want. But if they don't, the state will start out as
all zeros, just like in this prototype.)

Second, and most importantly, let's talk about the dimensionality of
\texttt{x}, the single input argument. Where the cell operates on
tensors of size \texttt{batch\_size} times \texttt{num\_features}, the
iteration module expects its input to have an additional dimension,
inserted at position two -- right ``in the middle''. You can see this in
the line

\begin{verbatim}
c(batch_size, timesteps, num_features) %<-% x$size()
\end{verbatim}

This additional dimension is used to capture evolution over time.
\texttt{rnn\_module()} will iterate over its values, call
\texttt{rnn\_cell()} for each step in the sequence, and keep track of
the outputs:

\begin{verbatim}
for (t in 1:timesteps) {
  new_state <- self$cell(x[ , t, ], cur_state)
  states[[t]] <- new_state
  cur_state <- new_state
}
\end{verbatim}

As you see, in every call to \texttt{self\$cell}, the previous state is
passed as well, fulfilling the contract on
\texttt{rnn\_cell\$forward()}.

The complete code for \texttt{rnn\_module()} is just slightly longer
than that for the cell:

\begin{verbatim}
rnn_module <- nn_module(
  initialize = function(input_size, hidden_size) {
    self$cell <- rnn_cell(input_size, hidden_size)
    self$hidden_size <- hidden_size
  },
  forward = function(x) {
    c(batch_size, timesteps, num_features) %<-% x$size()
    init_hidden <- torch_zeros(batch_size, self$hidden_size)
    cur_state <- init_hidden

    # list containing the hidden states
    # (equivalently: outputs), of length timesteps
    states <- vector(mode = "list", length = timesteps)

    # loop over time steps
    for (t in 1:timesteps) {
      new_state <- self$cell(x[, t, ], cur_state)
      states[[t]] <- new_state
      cur_state <- new_state
    }

    # put sequence of states in dimension 2
    states <- torch_stack(states, dim = 2)

    list(states, states[, timesteps, ])
  }
)
\end{verbatim}

Note how dimension two, the one that, in the input, held the time
dimension, now is used to pack the states obtained for each time step.
I'll say more about that in a second, but first, let's test that module.
I'll stay with a state size (\texttt{hidden\_size}) of three, and make
our sample input have four consecutive measurements:

\begin{verbatim}
rnn <- rnn_module(input_size = 1, hidden_size = 3)

output <- rnn(torch_randn(2, 4, 1))
output
\end{verbatim}

\begin{verbatim}
[[1]]
torch_tensor
(1,.,.) = 
 -0.9066  0.8149 -0.3671
 -0.9772  0.2903 -0.7938
 -0.9724  0.6242 -0.7877
 -0.9811  0.4164 -0.8839

(2,.,.) = 
 -0.8901  0.8795  0.3131
 -0.9512  0.4883  0.4991
 -0.9297  0.4875  0.1878
 -0.9420  0.5741  0.1564
[ CPUFloatType{2,4,3} ][ grad_fn = <StackBackward0> ]

[[2]]
torch_tensor
-0.9811  0.4164 -0.8839
-0.9420  0.5741  0.1564
[ CPUFloatType{2,3} ][ grad_fn = <SliceBackward0> ]
\end{verbatim}

So, \texttt{rnn\_module()} returns a list of length two. First in the
list is a tensor containing the states at all time steps -- for each
batch item, and for each unit in the state. At the risk of being
redundant, here are its dimensions:

\begin{verbatim}
# batch_size, timesteps, hidden_size
dim(output[[1]])
\end{verbatim}

\begin{verbatim}
[1] 2 4 3
\end{verbatim}

The reason I'm stressing this is that you'll see the same convention
reappear in the actual \texttt{torch} implementation, and the
conventions associated with processing time series data can take some
time to get accustomed to.

Now, what about the second tensor? It really is a slice of the first --
one reflecting the final state only. Correspondingly, its number of
dimensions is reduced by one:

\begin{verbatim}
# batch_size, hidden_size
dim(output[[2]])
\end{verbatim}

\begin{verbatim}
[1] 2 3
\end{verbatim}

Now, as users, why would we need that second tensor?

We don't. We can just do the slicing ourselves. Remember how, above, I
first tried to avoid the term ``hidden states'', and said I'd rather
just talk about ``states'' instead? This is why: Whether the states
\emph{really} are hidden is up to implementation, that is,
\emph{developer choice}. A framework could decide to return the very
last state only, unless the caller explicitly asks for the preceding
ones. In that case, it would make sense to talk about ``the'' output on
the one hand, and the sequence of ``hidden states'', on the other. We
\emph{could} have coded our sample implementation like that. Instead, we
were following \texttt{torch}'s \texttt{nn\_rnn()}, which you'll
encounter in a second.

Thus, what I'm saying is: It all is a matter of conventions. But this
doesn't explain \emph{why} the \texttt{torch} developers chose to return
an additional, sliced-to-the-last-time-step, tensor: This clearly seems
redundant. Is it?

Well, it often is. \emph{Whether} it is or not depends, for one, on the
type of RNN. If you use \texttt{torch}'s \texttt{nn\_rnn()} (a
``simple'' RNN implementation not much employed in practice) or
\texttt{nn\_gru()} -- creating a default \emph{Gated Recurrent Network},
one of two ``classical'', tried-and-true architectures -- it will be.
If, on the other hand, you ask \texttt{torch} for a setup where a single
RNN module really is a composite of layers, and/or you use an
\emph{LSTM} (\emph{Long Short-Term Memory} Network, the second
``classic''), then one tensor will not be a subset of the other.

At this point, it definitely is time to look at those \texttt{torch}
modules.

\hypertarget{recurrent-neural-networks-in-torch}{%
\section{\texorpdfstring{Recurrent neural networks in
\texttt{torch}}{Recurrent neural networks in torch}}\label{recurrent-neural-networks-in-torch}}

Here, first, is \texttt{nn\_rnn()}, a more feature-rich, but
similar-in-spirit to our prototype recurrent module. In practice, you'll
basically always use either \texttt{nn\_gru()} or \texttt{nn\_lstm()},
which is why we won't spend much time on it. Especially, we don't talk
about optional arguments (yet), with two exceptions.

\begin{verbatim}
rnn <- nn_rnn(input_size = 1, 
              hidden_size = 3, 
              batch_first = TRUE,
              num_layers = 1)
\end{verbatim}

Both \texttt{batch\_first} and \texttt{num\_layers} arguments are
optional. The latter, \texttt{num\_layers}, allows for creating a stack
of RNN modules instead of a single one; this is convenient because the
user does not have to worry about how to correctly wire them together.
The default, though, is \texttt{1}: exactly what we're passing in,
above. The reason I'm specifying it explicitly is just so you know it
exists, and aren't confused by the module's output. Namely, you'll see
that the second in the list of tensors returned by
\texttt{rnn\$forward()} has an additional dimension that indicates the
layer.

In contrast, \texttt{batch\_first} is not set to its default; and it's
essential to be aware of this. By default, the convention for RNNs
differs from those for other modules; if we didn't pass the argument,
\texttt{torch} would expect the first dimension to be representing time
steps, not batch items. In this book, we'll always pass
\texttt{batch\_first\ =\ TRUE}.

Now, calling that RNN on the same test tensor we used in our manual
implementation above, and checking the dimensions of the output, we see:

\begin{verbatim}
output <- rnn(torch_randn(2, 4, 1))

# output
dim(output[[1]]) # batch_size, timesteps, hidden_size

# last hidden state (per layer)
dim(output[[2]]) # num_layers, batch_size, hidden_size
\end{verbatim}

\begin{verbatim}
[1] 2 4 3
[1] 1 2 3
\end{verbatim}

The first tensor in the list has a shape that exactly matches what would
be returned by our manual implementation.

Semantically, the respective second tensors in the output lists match up
as well, in that both of them zoom in on the final state. But
\texttt{torch}, allowing for the chaining of several RNNs in a single
module, returns the final state \emph{per layer}. In the \texttt{torch}
implementation, is that second tensor redundant? It is, in our example.
But were we to create a multi-layer RNN, it would give us information
not contained in the first tensor: namely, the last hidden state for
each non-final layer.

\hypertarget{rnns-in-practice-gru-and-lstm}{%
\section{RNNs in practice: GRU and
LSTM}\label{rnns-in-practice-gru-and-lstm}}

Basic recurrent networks, as created by \texttt{nn\_rnn()}, are nice for
explanatory purposes, but hardly ever used in practice. The reason is
that when you back-propagate through a long recurrence structure,
gradients are likely to either ``die'' or get out of bounds. These are
the so-called ``vanishing gradient'' and ``exploding gradient''
problems, respectively.

Already three decades before the compute- and big-data accelerated ``era
of deep learning'', an algorithmic solution had been found. \emph{Long
Short-Term Memory Networks}, described in Hochreiter and Schmidhuber
(1997), enabled training on reasonably long sequences by introducing
so-called \emph{gates} that act as filters in various places of the
state-threading calculation. \emph{Gated Recurrent Units}, put forward
(much more recently) in K. Cho et al. (2014), are similar in spirit, but
a bit simpler. Together, both architectures dominate the space.

With these models introducing additional logic, we see how the
division-of-labor strategy introduced above is useful: Iteration and
state threading are taken care of by different modules. This means that,
in principle, we could design our own LSTM or GRU \emph{cell}, and then,
iterate over it in the same fashion as before. Of course, there is no
need to re-implement existing functionality. But following the same
modularization approach, we can nicely experiment with variations to the
processing logic if we want.

Now, let's see what is returned by \texttt{nn\_gru()} and
\texttt{nn\_lstm()}, the constructors corresponding to the
aforementioned architectures. At this point, I should quickly comment on
optional arguments I haven't mentioned before.

In general, the argument list is the same for \texttt{nn\_rnn()},
\texttt{nn\_gru()}, and \texttt{nn\_lstm()}.

We have to indicate the number of features (\texttt{input\_size}) and
the size of the state (\texttt{hidden\_size}); we deliberately pass in
\texttt{batch\_first\ =\ TRUE;} we can have \texttt{torch} chain several
RNNs together using \texttt{num\_layers}. In case we \emph{do} want to
stack layers, we can drop out a fraction of interconnections using,
well, \texttt{dropout}. Finally, there is \texttt{bidirectional}. By
default, this argument is set to \texttt{FALSE}, meaning we're passing
through the sequence chronologically. With
\texttt{bidirectional\ =\ TRUE}, there is an additional pass in reverse
order, and weights from both passes are combined. Essentially, what
we're doing is predicting present from past as well as past from
present. This may sound like ``cheating'', but really, is not; it's just
making optimal use of dependencies in past data.

To keep our examples in sync, I'll now instantiate the GRU and LSTM
modules in the same way as \texttt{nn\_rnn()} above, making use of
single layer and a single direction.

First, a GRU:

\begin{verbatim}
gru <- nn_gru(
  input_size = 1, 
  hidden_size = 3, 
  batch_first = TRUE,
  num_layers = 1
)

output <- gru(torch_randn(2, 4, 1))

# output
dim(output[[1]]) # batch_size, timesteps, hidden_size

# last hidden state (per layer)
dim(output[[2]]) # num_layers, batch_size, hidden_size
\end{verbatim}

\begin{verbatim}
[1] 2 4 3
[1] 1 2 3
\end{verbatim}

As you see, dimension-wise, the output returned from a GRU is analogous
to that of a simple RNN.

With LSTM, however, we see a difference:

\begin{verbatim}
lstm <- nn_lstm(
  input_size = 1,
  hidden_size = 3,
  batch_first = TRUE
)

output <- lstm(torch_randn(2, 4, 1))

# output
dim(output[[1]]) # batch_size, timesteps, hidden_size

# last hidden state (per layer)
dim(output[[2]][[1]]) # num_layers, batch_size, hidden_size

# last cell state (per layer)
dim(output[[2]][[2]]) # num_layers, batch_size, hidden_size
\end{verbatim}

\begin{verbatim}
[1] 2 4 3
[1] 1 2 3
[1] 1 2 3
\end{verbatim}

Instead of two, we now have three tensors. The first and second are no
different from what we've seen so far; meaning, the second is as
redundant as for a GRU or a simple RNN. (At least when there's just a
single layer). What about the third? Shape-wise, it looks like the
second, the one we know returns the ``hidden state''. In fact, it
reflects an \emph{additional} state, one not present in a GRU. And this
one -- often called \emph{cell} state -- \emph{really} is available to
the user only for the last time step, even for single-layer LSTMs.

You could say that with LSTMs, some hidden states are more hidden than
others.

Now that we've gained some familiarity with \texttt{torch}'s RNN-related
conventions, we look at an actual time series application.

\hypertarget{forecasting-electricity-demand}{%
\section{Forecasting electricity
demand}\label{forecasting-electricity-demand}}

Our example time series, called \texttt{vic\_elec}, is available from
package \texttt{tsibbledata}. It reflects aggregated electricity demand
for Victoria, Australia, measured in half-hour intervals. Additional
features (which we won't use here) include temperature and a holiday
indicator. The dataset spans three years, ranging from January, 2012 to
December, 2014.

Before we start, we need to define our task. In fact, there'll be two of
them. In both, we'll attempt to predict future temperature based on past
measurements. First, we'll see how to predict the very next measurement;
in terms of measurement intervals, that's a single time step ahead.
Then, we'll modify the code to allow for forecasting several time steps
in advance.

\hypertarget{data-inspection}{%
\subsection{Data inspection}\label{data-inspection}}

The dataset being part of an ecosystem of packages dedicated to time
series analysis, there is not much to be done in terms of
pre-processing. However, as always or even more so, it is worth our
while to take time for data exploration.

\begin{verbatim}
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(lubridate)

# Tidy Temporal Data Frames and Tools
library(tsibble) 
# Feature Extraction and Statistics for Time Series
library(feasts) 
# Diverse Datasets for 'tsibble'
library(tsibbledata) 

library(torch)
library(luz)

vic_elec
\end{verbatim}

\begin{verbatim}
# A tsibble: 52,608 x 5 [30m] <Australia/Melbourne>
   Time                Demand Temperature Date       Holiday
   <dttm>               <dbl>       <dbl> <date>     <lgl>  
 1 2012-01-01 00:00:00  4383.        21.4 2012-01-01 TRUE   
 2 2012-01-01 00:30:00  4263.        21.0 2012-01-01 TRUE   
 3 2012-01-01 01:00:00  4049.        20.7 2012-01-01 TRUE   
 4 2012-01-01 01:30:00  3878.        20.6 2012-01-01 TRUE   
 5 2012-01-01 02:00:00  4036.        20.4 2012-01-01 TRUE   
 6 2012-01-01 02:30:00  3866.        20.2 2012-01-01 TRUE   
 7 2012-01-01 03:00:00  3694.        20.1 2012-01-01 TRUE   
 8 2012-01-01 03:30:00  3562.        19.6 2012-01-01 TRUE   
 9 2012-01-01 04:00:00  3433.        19.1 2012-01-01 TRUE   
10 2012-01-01 04:30:00  3359.        19.0 2012-01-01 TRUE   
#  with 52,598 more rows
\end{verbatim}

Concretely, we'll want to know what kinds of periodicities there are in
the data. Conveniently, we can obtain a decomposition into trend,
various seasonal components, and a remainder using
\texttt{feasts::STL()}. Here's what we see for a single year
(fig.~\ref{fig-timeseries-vic-elec-stl}):

\begin{verbatim}
decomp <- vic_elec %>% 
  filter(year(Date) == 2012) %>%
  model(STL(Demand)) %>% 
  components()

decomp %>% autoplot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-stl.png}

}

\caption{\label{fig-timeseries-vic-elec-stl}One year of electricity
demand, decomposed into trend, seasonal components, and remainder.}

\end{figure}

In this plot, the scale bar on the left immediately signals a
component's importance: the smaller the bar, the more dominant the
effect. Not surprisingly, day of week matters; so does time of day.

For greater granularity, we zoom in on a single month
(fig.~\ref{fig-timeseries-vic-elec-stl-month}).

\begin{verbatim}
decomp <- vic_elec %>% 
  filter(year(Date) == 2012, month(Date) == 1) %>%
  model(STL(Demand)) %>% 
  components()

decomp %>% autoplot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-stl-month.png}

}

\caption{\label{fig-timeseries-vic-elec-stl-month}A single month of
electricity demand, decomposed into trend, seasonal components, and
remainder.}

\end{figure}

Here, I've picked January, right in the hot Australian summer. Demand
for electricity, then, arises as a need for cooling, not heating. We
clearly see how it's highest around noon, and lowest during the night.
Every week, it peaks on Mondays and Tuesdays, declines on Wednesdays,
and is more or less stable in the second half of the week.

Now, while those two periodicities are important, the half-hourly rhythm
clearly isn't. For training the network, I'll thus aggregate pairs of
adjacent values, reducing the number of measurements to a half. Since
now, changes between consecutive values are bigger, this also makes the
task harder.

\hypertarget{forecasting-the-very-next-value}{%
\subsection{Forecasting the very next
value}\label{forecasting-the-very-next-value}}

We start with the more modest goal: predicting the very next data point.
First of all, we need a custom \texttt{torch::dataset()}.

\hypertarget{a-dataset-for-single-step-prediction}{%
\subsubsection{\texorpdfstring{A \texttt{dataset()} for single-step
prediction}{A dataset() for single-step prediction}}\label{a-dataset-for-single-step-prediction}}

In our demonstration of how \texttt{torch} RNNs work, we made use of a
toy data tensor that looked like this: \texttt{torch\_randn(2,\ 4,\ 1)}.

Here, the first dimension corresponded to batch items; the second, to
time steps; and the third, to features. The batch dimension will be
taken care of by the \texttt{dataloader()}. Thus, for the
\texttt{dataset()}, the task to return a single predictor reduces to
picking just as many consecutive measurements as desired, and stacking
them in the first dimension. The corresponding target, in this first
setup, should be the single measurement right after the last step in the
predictor sequence.

So in principle, \texttt{.getitem()}, when asked for the item at
position \texttt{i}, should return the following:

\begin{verbatim}
list(
  x = self$x[i:(i + self$n_timesteps - 1)],
  y = self$x[self$n_timesteps + i]
)
\end{verbatim}

The code you'll find below is a variation on this theme. Depending on
the time series in question, stacking consecutive measurements like this
may result in individual batches looking hardly any different from each
other. To avoid redundancy, we could do the following: Instead of
iterating over the series in order, we take samples. At construction
time, the \texttt{torch::dataset()} is told what fraction of the data
we'd like to be served in each epoch, and prepares a set of indices.
Then at runtime, it iterates over those indices. In the present case,
sampling is not really needed, since anyway we've decided on hourly
aggregation, and the time series itself isn't excessively long to start
with. The reason I'm showing this technique is that you might find it
useful in other applications.

Now, take a look at how we initialize the \texttt{dataset()}. If it is
supposed to stack consecutive measurements, it has to be told the
desired number of time steps. For easy experimentation, we make that
parameter (called \texttt{n\_timesteps}) a constructor argument.

Here is the complete \texttt{dataset()} code:

\begin{verbatim}
demand_dataset <- dataset(
  name = "demand_dataset",
  initialize = function(x, n_timesteps, sample_frac = 1) {
    self$n_timesteps <- n_timesteps
    self$x <- torch_tensor((x - train_mean) / train_sd)

    n <- length(self$x) - self$n_timesteps

    self$starts <- sort(sample.int(
      n = n,
      size = n * sample_frac
    ))
  },
  .getitem = function(i) {
    start <- self$starts[i]
    end <- start + self$n_timesteps - 1

    list(
      x = self$x[start:end],
      y = self$x[end + 1]
    )
  },
  .length = function() {
    length(self$starts)
  }
)
\end{verbatim}

Given that \texttt{vic\_elec} holds three years of data, a by-year split
into training, validation, and test sets seems to suggest itself. Here,
we perform the split, right after aggregating by full hour:

\begin{verbatim}
demand_hourly <- vic_elec %>%
  index_by(Hour = floor_date(Time, "hour")) %>%
  summarise(
    Demand = sum(Demand))

demand_train <- demand_hourly %>% 
  filter(year(Hour) == 2012) %>%
  as_tibble() %>%
  select(Demand) %>%
  as.matrix()

demand_valid <- demand_hourly %>% 
  filter(year(Hour) == 2013) %>%
  as_tibble() %>%
  select(Demand) %>%
  as.matrix()

demand_test <- demand_hourly %>% 
  filter(year(Hour) == 2014) %>%
  as_tibble() %>%
  select(Demand) %>%
  as.matrix()
\end{verbatim}

In the \texttt{dataset()} definition, you may have noticed the use of
\texttt{train\_mean} and \texttt{train\_sd} to standardize the data.
Time to compute them:

\begin{verbatim}
train_mean <- mean(demand_train)
train_sd <- sd(demand_train)
\end{verbatim}

Now, we're all set to construct the \texttt{dataset()} objects -- but
for one decision: What is a good value for \texttt{n\_timesteps}? As
with every hyper-parameter, in the end, nothing can beat
experimentation. But based on data exploration, we can establish a lower
bound: We definitely want to capture variation over day \emph{and} week.
With the hourly-aggregated data, this gives us a minimum length of 168:

\begin{verbatim}
n_timesteps <- 7 * 24
\end{verbatim}

Now, we instantiate the \texttt{dataset()} objects, and immediately
check tensor shapes:

\begin{verbatim}
train_ds <- demand_dataset(demand_train, n_timesteps)
valid_ds <- demand_dataset(demand_valid, n_timesteps)
test_ds <- demand_dataset(demand_test, n_timesteps)

dim(train_ds[1]$x)
dim(train_ds[1]$y)
\end{verbatim}

\begin{verbatim}
[1] 168   1
[1] 1
\end{verbatim}

Next, the respective \texttt{dataloader()}s:

\begin{verbatim}
batch_size <- 128

train_dl <- train_ds %>%
  dataloader(batch_size = batch_size, shuffle = TRUE)
valid_dl <- valid_ds %>%
  dataloader(batch_size = batch_size)
test_dl <- test_ds %>%
  dataloader(batch_size = length(test_ds))

b <- train_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()

dim(b$x)
dim(b$y)
\end{verbatim}

\begin{verbatim}
[1] 128 168   1
[1] 128   1
\end{verbatim}

Once we're dealing with batches we \emph{do} see the required
three-dimensional structure of the input tensor.

\hypertarget{model-1}{%
\subsubsection{Model}\label{model-1}}

In the model, the main workhorse is the LSTM. The number of
LSTM-internal layers is configurable, the default being set to one. Its
final output (or: ``hidden state''; more on that below) is passed on to
a linear module that outputs a single prediction.

\begin{verbatim}
model <- nn_module(
  initialize = function(input_size,
                        hidden_size,
                        dropout = 0.2,
                        num_layers = 1,
                        rec_dropout = 0) {
    self$num_layers <- num_layers

    self$rnn <- nn_lstm(
      input_size = input_size,
      hidden_size = hidden_size,
      num_layers = num_layers,
      dropout = rec_dropout,
      batch_first = TRUE
    )

    self$dropout <- nn_dropout(dropout)
    self$output <- nn_linear(hidden_size, 1)
  },
  forward = function(x) {
    (x %>%
      # these two are equivalent
      # (1)
      # take output tensor,restrict to last time step
      self$rnn())[[1]][, dim(x)[2], ] %>%
      # (2)
      # from list of state tensors,take the first,
      # and pick the final layer
      # self$rnn())[[2]][[1]][self$num_layers, , ] %>%
      self$dropout() %>%
      self$output()
  }
)
\end{verbatim}

Note two things about the top-level module's definition. First, its
constructor accepts two dropout-related arguments. The first,
\texttt{dropout}, specifies the fraction of elements to be zeroed out on
the path between LSTM and linear module; the second,
\texttt{rec\_dropout}, gets passed on to the LSTM, to be made use of
between individual layers. (If there is just a single layer, this
parameter has to equal zero, its default.)

Secondly, in \texttt{forward()}, I'm showing two equivalent ways to pass
output from the LSTM to the linear module. We can either ask
\texttt{torch} for ``the output'', and then, take the subset of values
corresponding to the last time step only; alternatively, we can extract
the ``hidden state'' tensor and zoom in on the final layer. Of course,
since whatever choice we make is fine, there is no need to complicate
the decision; I merely wanted to take the occasion to, one final time,
illustrate how to ``talk RNN'' with \texttt{torch}.

\hypertarget{training-2}{%
\subsubsection{Training}\label{training-2}}

Like so often, we start the training process by running the learning
rate finder. The model we're training is modest in size (12,928
parameters overall!), but powerful -- not in the least due to the
stacking of LSTM layers.

\begin{verbatim}
input_size <- 1
hidden_size <- 32
num_layers <- 2
rec_dropout <- 0.2

model <- model %>%
  setup(optimizer = optim_adam, loss = nn_mse_loss()) %>%
  set_hparams(
    input_size = input_size,
    hidden_size = hidden_size,
    num_layers = num_layers,
    rec_dropout = rec_dropout
  )

rates_and_losses <- model %>% 
  lr_finder(train_dl, start_lr = 1e-3, end_lr = 1)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-lr-finder.png}

}

\caption{\label{fig-timeseries-vic-elec-lr-finder}Learning rate finder
output for the one-step-ahead-forecast model.}

\end{figure}

From the plot (fig.~\ref{fig-timeseries-vic-elec-lr-finder}), a maximal
learning rate of 0.1 seems like a sensible choice, at least when
combined with the one-cycle learning rate scheduler. Indeed, we see fast
progress (fig.~\ref{fig-timeseries-vic-elec-fit}):

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 50, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 3),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.1,
          epochs = 50,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end")
      ),
      verbose = TRUE)

plot(fitted)
\end{verbatim}

\begin{verbatim}
Epoch 1/50
Train metrics: Loss: 0.3119
Valid metrics: Loss: 0.0715
Epoch 2/50
Train metrics: Loss: 0.0767
Valid metrics: Loss: 0.0562
...
...
Epoch 17/50
Train metrics: Loss: 0.0301
Valid metrics: Loss: 0.0295
Epoch 18/50
Train metrics: Loss: 0.0288 
Valid metrics: Loss: 0.0263
Early stopping at epoch 18 of 50
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-fit.png}

}

\caption{\label{fig-timeseries-vic-elec-fit}Fitting a one-step forecast
model on \texttt{vic\_elec}.}

\end{figure}

\hypertarget{inspecting-predictions}{%
\subsubsection{Inspecting predictions}\label{inspecting-predictions}}

Using \texttt{luz::evaluate()}, we can check whether performance on the
test set approximately matches that on the validation set. Here is mean
squared error, as obtained from test-set predictions:

\begin{verbatim}
evaluate(fitted, test_dl)
\end{verbatim}

\begin{verbatim}
A `luz_module_evaluation`
 Results 
loss: 0.0364
\end{verbatim}

But really, what does this value tell us? We need to actually
\emph{look} at forecasts to decide whether predictions are any good.

For visualization, let's choose a subset of the test data -- the final
month, say. We obtain predictions, and display them together with the
actual measurements (fig.~\ref{fig-timeseries-vic-elec-preds}):

\begin{verbatim}
demand_viz <- demand_hourly %>%
  filter(year(Hour) == 2014, month(Hour) == 12)

demand_viz_matrix <- demand_viz %>%
  as_tibble() %>%
  select(Demand) %>%
  as.matrix()

viz_ds <- demand_dataset(demand_viz_matrix, n_timesteps)
viz_dl <- viz_ds %>% dataloader(batch_size = length(viz_ds))

preds <- predict(fitted, viz_dl)
preds <- preds$to(device = "cpu") %>% as.matrix()
preds <- c(rep(NA, n_timesteps), preds)

pred_ts <- demand_viz %>%
  add_column(forecast = preds * train_sd + train_mean) %>%
  pivot_longer(-Hour) %>%
  update_tsibble(key = name)

pred_ts %>%
  autoplot() +
  scale_colour_manual(values = c("#08c5d1", "#00353f")) +
  theme_minimal() +
  theme(legend.position = "None")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-preds.png}

}

\caption{\label{fig-timeseries-vic-elec-preds}One-step ahead forecast on
the last month of test set.}

\end{figure}

Predictions look good! However, we know this was the simpler task of the
two.

\hypertarget{forecasting-multiple-time-steps-ahead}{%
\subsection{Forecasting multiple time steps
ahead}\label{forecasting-multiple-time-steps-ahead}}

In reality, being able to predict a single measurement may not
necessarily be of great use. Fortunately, not much adaptation is
required to obtain multiple predictions from this type of model. In
principle, we just have to modify the output size of the final linear
layer. For performance reasons, we'll go a bit further, but it'll still
be a moderate change.

Since the target now is a sequence, we also have to adapt the
\texttt{dataset()}.

\hypertarget{dataset-adaptation}{%
\subsubsection{\texorpdfstring{\texttt{Dataset()}
adaptation}{Dataset() adaptation}}\label{dataset-adaptation}}

Just like the number of consecutive measurements in an input tensor,
that of the target tensors should be configurable. We therefore
introduce a new parameter, \texttt{n\_forecast}, that indicates the
desired length:

\begin{verbatim}
demand_dataset <- dataset(
  name = "demand_dataset",
  initialize = function(x,
                        n_timesteps,
                        n_forecast,
                        sample_frac = 1) {
    self$n_timesteps <- n_timesteps
    self$n_forecast <- n_forecast
    self$x <- torch_tensor((x - train_mean) / train_sd)

    n <- length(self$x) -
      self$n_timesteps - self$n_forecast + 1

    self$starts <- sort(sample.int(
      n = n,
      size = n * sample_frac
    ))
  },
  .getitem = function(i) {
    start <- self$starts[i]
    end <- start + self$n_timesteps - 1

    list(
      x = self$x[start:end],
      y = self$x[(end + 1):(end + self$n_forecast)]$
        squeeze(2)
    )
  },
  .length = function() {
    length(self$starts)
  }
)
\end{verbatim}

We'll attempt to forecast demand for the subsequent seven days. Thus,
\texttt{n\_timesteps} and \texttt{n\_forecast} are equal; but that's a
coincidence, not a requirement.

\begin{verbatim}
n_timesteps <- 7 * 24
n_forecast <- 7 * 24

train_ds <- demand_dataset(
  demand_train,
  n_timesteps,
  n_forecast,
  sample_frac = 1
)
valid_ds <- demand_dataset(
  demand_valid,
  n_timesteps,
  n_forecast,
  sample_frac = 1
)
test_ds <- demand_dataset(
  demand_test,
  n_timesteps,
  n_forecast
)

batch_size <- 128
train_dl <- train_ds %>%
  dataloader(batch_size = batch_size, shuffle = TRUE)
valid_dl <- valid_ds %>%
  dataloader(batch_size = batch_size)
test_dl <- test_ds %>%
  dataloader(batch_size = length(test_ds))
\end{verbatim}

\hypertarget{model-adaptation}{%
\subsubsection{Model adaptation}\label{model-adaptation}}

In the model, we replace the final linear layer by a tiny multi-layer
perceptron. It will return \texttt{n\_forecast} predictions for each
time step.

\begin{verbatim}
model <- nn_module(
  initialize = function(input_size,
                        hidden_size,
                        linear_size,
                        output_size,
                        dropout = 0.2,
                        num_layers = 1,
                        rec_dropout = 0) {
    self$num_layers <- num_layers

    self$rnn <- nn_lstm(
      input_size = input_size,
      hidden_size = hidden_size,
      num_layers = num_layers,
      dropout = rec_dropout,
      batch_first = TRUE
    )

    self$dropout <- nn_dropout(dropout)
    self$mlp <- nn_sequential(
      nn_linear(hidden_size, linear_size),
      nn_relu(),
      nn_dropout(dropout),
      nn_linear(linear_size, output_size)
    )
  },
  forward = function(x) {
    x <- self$rnn(x)[[2]][[1]][self$num_layers, , ] %>%
      self$mlp()
  }
)
\end{verbatim}

\hypertarget{training-3}{%
\subsubsection{Training}\label{training-3}}

Not surprisingly, the training process is unaltered. It again starts
with running the learning rate finder
(fig.~\ref{fig-vic-elec-lr-finder-mlp}):

\begin{verbatim}
input_size <- 1
hidden_size <- 32
linear_size <- 512
dropout <- 0.5
num_layers <- 2
rec_dropout <- 0.2

model <- model %>%
  setup(optimizer = optim_adam, loss = nn_mse_loss()) %>%
  set_hparams(
    input_size = input_size,
    hidden_size = hidden_size,
    linear_size = linear_size,
    output_size = n_forecast,
    num_layers = num_layers,
    rec_dropout = rec_dropout
  )

rates_and_losses <- model %>% lr_finder(
  train_dl,
  start_lr = 1e-4,
  end_lr = 0.5
)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-lr-finder-mlp.png}

}

\caption{\label{fig-vic-elec-lr-finder-mlp}Learning rate finder output
for multiple-step prediction.}

\end{figure}

Based on that result, I'll go with a lower maximal rate this time.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl, epochs = 100, valid_data = valid_dl,
      callbacks = list(
        luz_callback_early_stopping(patience = 3),
        luz_callback_lr_scheduler(
          lr_one_cycle,
          max_lr = 0.01,
          epochs = 100,
          steps_per_epoch = length(train_dl),
          call_on = "on_batch_end")
      ),
      verbose = TRUE)

plot(fitted)
\end{verbatim}

\begin{verbatim}
Epoch 1/100
Train metrics: Loss: 0.9639                             
Valid metrics: Loss: 0.9714
Epoch 2/100
Train metrics: Loss: 0.823                              
Valid metrics: Loss: 0.7729
...
...
Epoch 21/100
Train metrics: Loss: 0.3833                             
Valid metrics: Loss: 0.4585
Epoch 22/100
Train metrics: Loss: 0.3796                             
Valid metrics: Loss: 0.4404
...
...
Epoch 41/100
Train metrics: Loss: 0.3103                             
Valid metrics: Loss: 0.3677
Epoch 42/100
Train metrics: Loss: 0.3089                             
Valid metrics: Loss: 0.3646
...
...
Epoch 60/100
Train metrics: Loss: 0.2707                             
Valid metrics: Loss: 0.3337
Epoch 61/100
Train metrics: Loss: 0.2617                             
Valid metrics: Loss: 0.3317
Early stopping at epoch 61 of 100
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-fit-mlp.png}

}

\caption{\label{fig-timeseries-vic-elec-fit-mlp}Fitting a multiple-step
forecast model on vic\_elec.}

\end{figure}

The loss curves (fig.~\ref{fig-timeseries-vic-elec-fit-mlp}) look
decent; how about actual forecasts?

\hypertarget{inspecting-predictions-1}{%
\subsubsection{Inspecting predictions}\label{inspecting-predictions-1}}

As before, let's first formally check that performance on the test set
does not differ too much from that on the validation set.

\begin{verbatim}
evaluate(fitted, test_dl)
\end{verbatim}

\begin{verbatim}
A `luz_module_evaluation`
 Results 
loss: 0.3782
\end{verbatim}

For visualization, we again look at the very last month only.

\begin{verbatim}
demand_viz <- demand_hourly %>%
  filter(year(Hour) == 2014, month(Hour) == 12)

demand_viz_matrix <- demand_viz %>%
  as_tibble() %>%
  select(Demand) %>%
  as.matrix()

n_obs <- nrow(demand_viz_matrix)

viz_ds <- demand_dataset(
  demand_viz_matrix,
  n_timesteps,
  n_forecast
)
viz_dl <- viz_ds %>%
  dataloader(batch_size = length(viz_ds))

preds <- predict(fitted, viz_dl)
preds <- preds$to(device = "cpu") %>%
  as.matrix()
\end{verbatim}

With each forecast now covering a week of measurements, we can't display
them all in the same plot. What we \emph{can} do is pick a few sample
indices, nicely spread out over the month, and plot those
(fig.~\ref{fig-timeseries-vic-elec-preds-mlp}):

\begin{verbatim}
example_preds <- vector(mode = "list", length = 3)
example_indices <- c(1, 201, 401)

for (i in seq_along(example_indices)) {
  cur_obs <- example_indices[i]
  example_preds[[i]] <- c(
    rep(NA, n_timesteps + cur_obs - 1),
    preds[cur_obs, ],
    rep(
      NA,
      n_obs - cur_obs + 1 - n_timesteps - n_forecast
    )
  )
}

pred_ts <- demand_viz %>%
  select(Demand) %>%
  add_column(
    p1 = example_preds[[1]] * train_sd + train_mean,
    p2 = example_preds[[2]] * train_sd + train_mean,
    p3 = example_preds[[3]] * train_sd + train_mean) %>%
  pivot_longer(-Hour) %>%
  update_tsibble(key = name)

pred_ts %>%
  autoplot() +
  scale_colour_manual(
    values = c(
      "#08c5d1", "#00353f", "#ffbf66", "#d46f4d"
    )
  ) +
  theme_minimal() +
  theme(legend.position = "None")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/timeseries-vic-elec-preds-mlp.png}

}

\caption{\label{fig-timeseries-vic-elec-preds-mlp}A sample of week-long
forecasts on the last month of test set.}

\end{figure}

It is instructive to take a closer look. The daily and weekly rhythms
are present; very much so in fact. What's harder to devine, from the
model's point of view, are the mini-trends we see developing over the
month. Without any external ``cues'', how should it know that next week,
demand for electricity will raise over (or fall below) the current
level? To significantly improve predictions, we would have to
incorporate additional data -- for example, temperature forecasts. Since
no book can cover everything, we won't pursue that topic here; instead,
we'll move on to our last deep learning application: classifying speech
utterances.

\hypertarget{sec:audio-classification}{%
\chapter{Audio classification}\label{sec:audio-classification}}

In our final chapter on deep learning, we look at the fascinating topic
of audio signals. And here, part of my goal is to convince you (if you
aren't convinced yet) of the utmost importance of \emph{domain
knowledge}. Let me explain.

Far too often, machine learning is seen as a magical device that, when
employed in a technically correct way, will yield great results, however
little the model developer (or user) may know about the domain in
question. In previous chapters, we've already seen that this is not
true, not even in the case of seemingly ``simple'' datasets.
Essentially, we saw that pre-processing matters -- \emph{always} -- and
that to adequately pre-process data, we need to know what they're
supposed to represent. However, techniques of incorporating domain
knowledge into the machine learning workflow can go a \emph{lot}
further. This is a topic that, definitely, deserves a book of its own.
But this chapter hopes to offer something like a glimpse ahead: With
audio signals, we will encounter a technique that, beyond being of great
appeal in itself, has the strength of generalizing to a wide range of
applications that deal with comparable data.

Let's start by characterizing the task.

\hypertarget{classifying-speech-data}{%
\section{Classifying speech data}\label{classifying-speech-data}}

The speech command dataset (Warden (2018)) comes with
\texttt{torchaudio}, a package that does for auditory data what
\texttt{torchvision} does for images and video. As of this writing,
there are two versions; the one provided by \texttt{torchaudio} is
number one. In that version, the dataset holds recordings of thirty
different, one- or two-syllable words, uttered by different speakers;
there are about 65,000 audio files overall. The task is to predict, from
the audio recording, which word was spoken.

To see what is involved, we download and inspect the data.

\begin{verbatim}
library(torch)
library(torchaudio)
library(luz)

ds <- speechcommand_dataset(
  root = "~/.torch-datasets", 
  url = "speech_commands_v0.01",
  download = TRUE
)

ds$classes
\end{verbatim}

\begin{verbatim}
[1]  "bed"    "bird"   "cat"    "dog"    "down"   "eight"
[7]  "five"   "four"   "go"     "happy"  "house"  "left"
[32] " marvin" "nine"   "no"     "off"    "on"     "one"
[19] "right"  "seven" "sheila" "six"    "stop"   "three"
[25]  "tree"   "two"    "up"     "wow"    "yes"    "zero" 
\end{verbatim}

Picking a sample at random, we see that the information we'll need is
contained in four properties: \texttt{waveform}, \texttt{sample\_rate},
\texttt{label\_index}, and \texttt{label}.

The first, \texttt{waveform}, will be our predictor.

\begin{verbatim}
sample <- ds[2000]
dim(sample$waveform)
\end{verbatim}

\begin{verbatim}
[1]     1 16000
\end{verbatim}

Individual tensor values are centered at zero, and range between -1 and
1. There are 16,000 of them, reflecting the fact that the recording
lasted for one second, and was registered at (or has been converted to,
by the dataset creators) a rate of 16,000 samples per second. The latter
information is stored in \texttt{sample\$sample\_rate}:

\begin{verbatim}
sample$sample_rate
\end{verbatim}

\begin{verbatim}
[1] 16000
\end{verbatim}

All recordings have been sampled at the same rate. Their length almost
always equals one second; the -- very -- few ones that are minimally
longer we can safely truncate.

Finally, the target is stored, in integer form, in
\texttt{sample\$label\_index}, with the corresponding word available
from \texttt{sample\$label}:

\begin{verbatim}
sample$label
sample$label_index
\end{verbatim}

\begin{verbatim}
[1] "bird"
torch_tensor
2
[ CPULongType{} ]
\end{verbatim}

How does this audio signal ``look''
(fig.~\ref{fig-audio-bird-waveform})?

\begin{verbatim}
library(ggplot2)

df <- data.frame(
  x = 1:length(sample$waveform[1]),
  y = as.numeric(sample$waveform[1])
  )

ggplot(df, aes(x = x, y = y)) +
  geom_line(size = 0.3) +
  ggtitle(
    paste0(
      "The spoken word \"", sample$label, "\": Sound wave"
    )
  ) +
  xlab("time") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-bird-waveform.png}

}

\caption{\label{fig-audio-bird-waveform}The spoken word ``bird'', in
time-domain representation.}

\end{figure}

What we see is a sequence of amplitudes, reflecting the sound wave
produced by someone saying ``bird''. Put differently, we have here a
time series of ``loudness values''. Even for experts, guessing
\emph{which} word resulted in those amplitudes is an impossible task.
This is where domain knowledge is relevant. The expert may not be able
to make much of the signal \emph{in this representation}; but they may
know a way to more meaningfully represent it.

At this point, you may be thinking: Right; but just because the task is
impossible for human beings it need not be impossible for a machine!
After all, a neural network and a person process information very
differently. Maybe an RNN, trained on these waves, can learn to
correctly map them to a set of words!

That could indeed be -- you may want to try -- but it turns out there is
a better way, one that both appeals to our, human, desire for
understanding \emph{and} uses deep learning in a most beneficient way.

This better way is owed to a mathematical fact that -- for me, at least
-- never ceases to inspire awe and wonder.

\hypertarget{two-equivalent-representations}{%
\section{Two equivalent
representations}\label{two-equivalent-representations}}

Imagine that instead of as a sequence of amplitudes over time, the above
wave were represented in a way that had no information about time at
all. Next, imagine we took that representation and tried to recover the
original signal. For that to be possible, the new representation would
somehow have to contain ``just as much'' information as the wave we
started from. That ``just as much'' is obtained by the \emph{Fourier
Transform}, and it consists of the magnitudes and phase shifts of the
different \emph{frequencies} that make up the signal. In part three,
we'll play around quite a bit with the Fourier Transform, so here I'll
keep the introduction brief. Instead, I'll focus on the elegant
symbiosis that will result, once we've arrived at the final
pre-processing step. But we're not quite there yet.

To start, how does the Fourier-transformed version of the ``bird'' sound
wave look? We obtain it by calling \texttt{torch\_fft\_fft()} (where
\texttt{fft} stands for Fast Fourier Transform):

\begin{verbatim}
dft <- torch_fft_fft(sample$waveform)
dim(dft)
\end{verbatim}

\begin{verbatim}
[1]     1 16000
\end{verbatim}

The length of this tensor is the same; however, its values are not in
chronological order. Instead, they represent the \emph{Fourier
coefficients}, corresponding to the frequencies contained in the signal.
The higher their magnitude, the more they contribute to the signal
(fig.~\ref{fig-audio-bird-dft}):

\begin{verbatim}
mag <- torch_abs(dft[1, ])

df <- data.frame(
  x = 1:(length(sample$waveform[1]) / 2),
  y = as.numeric(mag[1:8000])
)

ggplot(df, aes(x = x, y = y)) +
  geom_line(size = 0.3) +
  ggtitle(
    paste0(
      "The spoken word \"",
      sample$label,
      "\": Discrete Fourier Transform"
    )
  ) +
  xlab("frequency") +
  ylab("magnitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-bird-dft.png}

}

\caption{\label{fig-audio-bird-dft}The spoken word ``bird'', in
frequency-domain representation.}

\end{figure}

From this, alternate, representation, we could go back to the original
sound wave by taking the frequencies present in the signal, weighting
them according to their coefficients, and adding them up. (That opposite
direction is called the \emph{Inverse Fourier Transform}, and available
in \texttt{torch} as \texttt{torch\_fft\_ifft()}.)

This, in itself, is incredibly fascinating; but how does it help us with
our task of classifying audio signals? Were we to work with the sound
waves themselves, we'd feed them into an RNN. With frequencies, there is
no recurrence relation; so RNNs are not an option. We could use a
feed-forward neural network, then. But there is reason to expect this to
work particularly well.

\hypertarget{combining-representations-the-spectrogram}{%
\section{Combining representations: The
spectrogram}\label{combining-representations-the-spectrogram}}

In fact, what really would help us is a synthesis of both
representations; some sort of ``have your cake and eat it, too''. What
if we could divide the signal into small chunks, and run the Fourier
Transform on each of them? As you may have guessed from this leadup,
this indeed is something we can do; and the representation it creates is
called the \emph{spectrogram}\index{spectrogram}.

With a spectrogram, we still keep some time-domain information -- some,
since there is an unavoidable loss in granularity. On the other hand,
for each of the time segments, we learn about their spectral
composition. There's an important point to be made, though. The
resolutions we get in \emph{time} versus in \emph{frequency},
respectively, are inversely related. If we split up the signals into
many chunks (called ``windows''), the frequency representation per
window will not be very fine-grained. Conversely, if we want to get
better resolution in the frequency domain, we have to choose longer
windows, thus losing information about how spectral composition varies
over time. It seems, then, that all we can do is eat \emph{half} a cake,
and take pleasure in the sight of the other half.

Well, all said so far is correct; but as you'll see, this is far less of
a problem than it may seem now.

Before we unveil the mystery, though, let's create and inspect such a
spectrogram for our example signal. In the following code snippet, the
size of the -- overlapping -- windows is chosen so as to allow for
reasonable granularity in both the time and the frequency domain. We're
left with sixty-three windows, and, for each window, obtain two hundred
fifty-seven coefficients:

\begin{verbatim}
fft_size <- 512
window_size <- 512
power <- 0.5

spectrogram <- transform_spectrogram(
  n_fft = fft_size,
  win_length = window_size,
  normalized = TRUE,
  power = power
)

spec <- spectrogram(sample$waveform)$squeeze()
dim(spec)
\end{verbatim}

\begin{verbatim}
[1]   257 63
\end{verbatim}

We can display the spectrogram visually
(fig.~\ref{fig-audio-spectrogram}):

\begin{verbatim}
bins <- 1:dim(spec)[1]
freqs <- bins / (fft_size / 2 + 1) * sample$sample_rate 
log_freqs <- log10(freqs)

frames <- 1:(dim(spec)[2])
seconds <- (frames / dim(spec)[2]) *
  (dim(sample$waveform$squeeze())[1] / sample$sample_rate)

image(x = as.numeric(seconds),
      y = log_freqs,
      z = t(as.matrix(spec)),
      ylab = 'log frequency [Hz]',
      xlab = 'time [s]',
      col = hcl.colors(12, palette = "Light grays")
)
main <- paste0("Spectrogram, window size = ", window_size)
sub <- "Magnitude (square root)"
mtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)
mtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-spectrogram.png}

}

\caption{\label{fig-audio-spectrogram}The spoken word ``bird'':
Spectrogram.}

\end{figure}

We know that we've lost some resolution, in both time and frequency. By
displaying the square root of the coefficients' magnitudes, though --
and thus, enhancing sensitivity -- we were still able to obtain a
reasonable result. (With the \texttt{Light\ grays} color scheme,
brighter shades indicate higher-valued coefficients; darker ones, the
opposite.)

Finally, let's get back to the crucial question. If this representation
is, by necessity, a compromise -- why, then, would we want to employ it?
This is where we take the deep learning perspective. The spectrogram is
a two-dimensional representation: an image. With images, we have access
to a rich reservoir of techniques and architectures -- among all areas
deep learning has been successful in, image recognition still stands
out. Soon, you'll see that for this task, fancy architectures are not
even needed; a straightforward convnet will do a very good job.

\hypertarget{training-a-model-for-audio-classification}{%
\section{Training a model for audio
classification}\label{training-a-model-for-audio-classification}}

The plan is as follows. We'll end-to-end train a baseline model, which
already will be performing very well. Nevertheless, we'll try out two
ideas, to see if we can improve on that baseline. Should it turn out not
to be the case, we'll still have learned about some important
techniques.

\hypertarget{baseline-setup-training-a-convnet-on-spectrograms}{%
\subsection{Baseline setup: Training a convnet on
spectrograms}\label{baseline-setup-training-a-convnet-on-spectrograms}}

We start by creating a \texttt{torch::dataset()} that, starting from the
original \texttt{speechcommand\_dataset()}, computes a spectrogram for
every sample.

\begin{verbatim}
spectrogram_dataset <- dataset(
  inherit = speechcommand_dataset,
  initialize = function(...,
                        pad_to = 16000,
                        sampling_rate = 16000,
                        n_fft = 512,
                        window_size_seconds = 0.03,
                        window_stride_seconds = 0.01,
                        # power = 2 is default for
                        # transform_spectrogram()
                        # we stay with the default for now,
                        # but will make use of this option later
                        power = 2,
                        # this too will be explained later
                        n_mels = 0) {
    self$pad_to <- pad_to
    self$window_size_samples <- sampling_rate *
      window_size_seconds
    self$window_stride_samples <- sampling_rate *
      window_stride_seconds
    self$power <- power
    if (n_mels == 0) {
      self$spectrogram <- transform_spectrogram(
        n_fft = n_fft,
        win_length = self$window_size_samples,
        hop_length = self$window_stride_samples,
        normalized = TRUE,
        power = self$power
      )
    } else {
      self$spectrogram <- transform_mel_spectrogram(
        n_fft = n_fft,
        win_length = self$window_size_samples,
        hop_length = self$window_stride_samples,
        normalized = TRUE,
        power = self$power,
        n_mels = n_mels
      )
    }
    super$initialize(...)
  },
  .getitem = function(i) {
    item <- super$.getitem(i)

    x <- item$waveform
    # make sure all samples have the same length (57)
    # shorter ones will be padded,
    # longer ones will be truncated
    x <- nnf_pad(x, pad = c(0, self$pad_to - dim(x)[2]))
    x <- x %>% self$spectrogram()

    if (is.null(self$power)) {
      # there is an additional dimension now,
      # in position 4,
      # that we want to appear in front
      # (as a second channel)
      x <- x$squeeze()$permute(c(3, 1, 2))
    }

    y <- item$label_index
    list(x = x, y = y)
  }
)
\end{verbatim}

As always, we immediately check if all is well:

\begin{verbatim}
ds <- spectrogram_dataset(
  root = "~/.torch-datasets",
  url = "speech_commands_v0.01",
  download = TRUE
)

dim(ds[1]$x)
ds[1]$y
\end{verbatim}

\begin{verbatim}
[1]   1 257 101
torch_tensor
1
[ CPULongType{} ]
\end{verbatim}

Next, we split up the data, and instantiate the \texttt{dataset()} and
\texttt{dataloader()} objects.

\begin{verbatim}
train_ids <- sample(
  1:length(ds),
  size = 0.6 * length(ds)
)
valid_ids <- sample(
  setdiff(
    1:length(ds),
    train_ids
  ),
  size = 0.2 * length(ds)
)
test_ids <- setdiff(
  1:length(ds),
  union(train_ids, valid_ids)
)

batch_size <- 128

train_ds <- dataset_subset(ds, indices = train_ids)
train_dl <- dataloader(
  train_ds,
  batch_size = batch_size, shuffle = TRUE
)

valid_ds <- dataset_subset(ds, indices = valid_ids)
valid_dl <- dataloader(
  valid_ds,
  batch_size = batch_size
)

test_ds <- dataset_subset(ds, indices = test_ids)
test_dl <- dataloader(test_ds, batch_size = 64)

b <- train_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()

dim(b$x)
\end{verbatim}

\begin{verbatim}
[1] 128   1 257 101
\end{verbatim}

Like I said, the model is a straightforward convnet, with dropout and
batch normalization.

\begin{verbatim}
model <- nn_module(
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(1, 32, kernel_size = 3),
      nn_batch_norm2d(32),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(32, 64, kernel_size = 3),
      nn_batch_norm2d(64),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(64, 128, kernel_size = 3),
      nn_batch_norm2d(128),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(128, 256, kernel_size = 3),
      nn_batch_norm2d(256),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(256, 512, kernel_size = 3),
      nn_batch_norm2d(512),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1)),
      nn_dropout2d(p = 0.2)
    )

    self$classifier <- nn_sequential(
      nn_linear(512, 512),
      nn_batch_norm1d(512),
      nn_relu(),
      nn_dropout(p = 0.5),
      nn_linear(512, 30)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
\end{verbatim}

What is a good learning rate to train this model
(fig.~\ref{fig-audio-lr-finder-baseline})?

\begin{verbatim}
model <- model %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_accuracy())
  )

rates_and_losses <- model %>%
  lr_finder(train_dl)
rates_and_losses %>% plot()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-lr-finder-baseline.png}

}

\caption{\label{fig-audio-lr-finder-baseline}Learning rate finder, run
on the baseline model.}

\end{figure}

Based on the plot, I decided to use 0.01 as a maximal learning rate.

\begin{verbatim}
fitted <- model %>%
  fit(train_dl,
    epochs = 50, valid_data = valid_dl,
    callbacks = list(
      luz_callback_early_stopping(patience = 3),
      luz_callback_lr_scheduler(
        lr_one_cycle,
        max_lr = 1e-2,
        epochs = 50,
        steps_per_epoch = length(train_dl),
        call_on = "on_batch_end"
      ),
      luz_callback_model_checkpoint(path = "models_baseline/"),
      luz_callback_csv_logger("logs_baseline.csv")
    ),
    verbose = TRUE
  )

plot(fitted)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-fit-baseline.png}

}

\caption{\label{fig-audio-fit-baseline}Fitting the baseline model.}

\end{figure}

Model training stopped after thirty-two epochs
(fig.~\ref{fig-audio-fit-baseline}). Here is an excerpt from the logs:

\begin{verbatim}
"epoch","set","loss","acc"

1,"train",3.34194613126368,0.0687577255871446
1,"valid",3.08480389211692,0.137438195302843
2,"train",2.87943273042542,0.161155747836836
2,"valid",2.54789054393768,0.260275030902349
3,"train",2.42715575726204,0.279048207663782
3,"valid",2.08232365753136,0.417567985166873
...
...
30,"train",0.397343056799929,0.880098887515451
30,"valid",0.3777267414273,0.895395550061805
31,"train",0.395276123743042,0.880073135558302
31,"valid",0.375300966641482,0.895781829419036
32,"train",0.387298851523524,0.880691182529872
32,"valid",0.379925572989034,0.89323238566131
\end{verbatim}

With thirty classes, an accuracy of about eighty-nine percent seems
decent. We double-check on the test set:

\begin{verbatim}
evaluate(fitted, test_dl)
\end{verbatim}

\begin{verbatim}
loss: 0.3776
acc: 0.8898
\end{verbatim}

An interesting question is which words get confused most often. (Of
course, even more interesting is how error probabilities are related to
features of the spectrograms -- but this we have to leave to the
\emph{true} domain experts.)

A nice way of displaying the confusion matrix is to create an alluvial
plot (fig.~\ref{fig-audio-alluvial-baseline}). We see the predictions,
on the left, ``flow into'' the target slots. (Target-prediction pairs
less frequent than a thousandth of test set cardinality are hidden.)

\begin{verbatim}
preds_baseline <- predict(fitted, test_dl)
preds_baseline <-
  torch_argmax(preds_baseline, dim = 2) %>%
  as.numeric()

test_dl <- dataloader(
  test_ds,
  batch_size = length(test_ds)
)
b <- test_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()
targets_baseline <- b$y$to(device = "cpu") %>%
  as.numeric()

df_baseline <- data.frame(
  preds = preds_baseline,
  targets = targets_baseline
)

classes <- speechcommand_ds$classes

df <- df_baseline %>%
  mutate(correct = preds == targets) %>%
  mutate(
    preds = classes[preds],
    targets = classes[targets]
  ) %>%
  count(preds, targets, correct)

library(alluvial)
alluvial(
  df %>% select(preds, targets),
  freq = df$n,
  col = ifelse(df$correct, "#d1d1d1", "#aaaaaa"),
  border = ifelse(df$correct, "#d1d1d1", "#aaaaaa"),
  hide = df$n < nrow(df_baseline) / 1000
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-alluvial-baseline.png}

}

\caption{\label{fig-audio-alluvial-baseline}Alluvial plot, illustrating
which categories were confused most often.}

\end{figure}

That's it for the baseline approach. Let's see if we can do still
better.

\hypertarget{variation-one-use-a-mel-scale-spectrogram-instead}{%
\subsection{\texorpdfstring{Variation one: Use a
Mel-scale\index{spectrogram!Mel scale} spectrogram
instead}{Variation one: Use a Mel-scale spectrogram instead}}\label{variation-one-use-a-mel-scale-spectrogram-instead}}

In classical speech recognition, people did not necessarily limit
pre-processing to the Fourier Transform and spectrograms. Various other
steps used to -- or could -- follow, some of which seem obsolete once
neural networks are being used. There is at least one technique, though,
where it's hard to devine a priori whether it will help or not.

Due to the way our hearing system is built, we human beings don't
perceive differences equally accurately across the frequency range. For
example, while the distances between 440 Hz and 480 Hz on the one hand,
and 8000 Hz and 8040 Hz on the other, are the same mathematically, the
latter will be much harder to perceive. (This is no different with other
modes of perception -- to tell the difference between one kilogram and
two kilograms is easy, while doing the same for fourteen versus fifteen
kilograms is less so.

To accommodate this physiological precondition, one sometimes converts
the Fourier coefficients to the so-called \emph{Mel scale}. Various
formulae exist that do this; in one or the other way, they always
include taking the logarithm. But in practice, what usually is done is
to create overlapping filters that aggregate sets of Fourier
coefficients into a new representation, the Mel coefficients. In the
low-frequency range, the filters are narrow, and subsume only very few
Fourier coefficients. Then, they successively become wider, until, in
the very-high frequency range, a wide range of Fourier coefficients get
to contribute to a single Mel value.

We can make this more concrete using a \texttt{torch} helper function,
\texttt{functional\_create\_fb\_matrix()} . What this function does is
create a conversion matrix from Fourier- to Mel space:

c

\begin{verbatim}
fb <- functional_create_fb_matrix(
  n_freqs = 257,
  f_min = 0,
  f_max = 8000,
  n_mels = 16,
  sample_rate = 16000
)
dim(fb)
\end{verbatim}

\begin{verbatim}
[1] 257  16
\end{verbatim}

In our application, we'll use Mel spectrograms with one hundred
twenty-eight coefficients; given that we're training a convnet, we
wouldn't want to shrink spatial resolution too much. But visualization
is more helpful when done with fewer filters -- which is why, above,
I've told \texttt{torch} to generate sixteen filters only. Here they are
(fig.~\ref{fig-audio-mel-filterbank}):

\begin{verbatim}
df <- as_tibble(as.matrix(fb)) %>%
  rowid_to_column("x")

df %>%
  pivot_longer(!x) %>%
  ggplot(
    aes(x = x, y = value, color = name, group = name)
  ) +
  geom_line() +
  xlab("Fourier coefficient") +
  ylab("Contribution to Mel coefficient") +
  theme_minimal() +
  theme(legend.position = "None")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-mel-filterbank.png}

}

\caption{\label{fig-audio-mel-filterbank}Mel filter bank with sixteen
filters, as applied to 257 Fourier coefficients.}

\end{figure}

From this plot, it should make sense how Mel-scale transformation is
designed to compensate for lower perceptual resolution in
higher-frequency ranges.

Of course, in order to work with Mel spectrograms, you won't need to
generate filter banks yourself. For training, we'll just replace
\texttt{transform\_spectrogram()} by
\texttt{transform\_mel\_spectrogram()}. But sometimes, playing around
with helper functions can significantly aid understanding. For example,
using another low-levellish utility -- \texttt{functional\_mel\_scale()}
-- we can take a set of Fourier coefficients, convert them to Mel scale,
and compare.

Here, I'm taking the spectrogram for sample 2000 - our ``bird'' -- and
pick a time window of interest. I then use
\texttt{functional\_mel\_scale()} to obtain the respective Mel-scale
representation, and plot (fig.~\ref{fig-audio-mel-spectrogram}) the
magnitudes of both sets of coefficients against each other (showing just
the first half of the Fourier coefficients):

\begin{verbatim}
sample_spec <- ds[2000]$x
win_31 <- sample_spec[1, , 31]

mel_31 <- win_31$unsqueeze(2) %>%
  functional_mel_scale(n_mel = 128)

df <- data.frame(
  coefficient = 1:128,
  fourier = as.numeric(win_31[1:128]),
  mel = as.numeric(mel_31)
)

df %>%
  pivot_longer(
    !coefficient,
    names_to = "type", values_to = "magnitude"
  ) %>%
  ggplot(
    aes(x = coefficient, y = magnitude, color = type)
  ) +
  geom_line() +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/audio-mel-spectrogram.png}

}

\caption{\label{fig-audio-mel-spectrogram}Fourier and Mel coefficients,
compared on one window of the ``bird'' spectrogram.}

\end{figure}

Finally, before getting back to the task, let's quickly verify that
calling \texttt{functional\_mel\_scale()} is equivalent to constructing
a filter bank manually and multiplying the resulting matrix with the
Fourier coefficients. To do so, we now create a 257 x 128 matrix, and
apply it to the spectrogram window we extracted above. The results
should be equal (apart from small numerical errors):

\begin{verbatim}
fb <- functional_create_fb_matrix(
  n_freqs = 257,
  f_min = 0,
  f_max = 8000,
  n_mels = 128,
  sample_rate = 16000
)

(fb$t()$matmul(win_31) - mel_31[1, , 1]) %>%
  torch_max()
\end{verbatim}

\begin{verbatim}
torch_tensor
5.96046e-08
[ CPUFloatType{} ]
\end{verbatim}

Now that we have an idea of how Mel-scale transformation works, and
possibly a gut feeling as to whether this will help training or not:
Let's actually find out. Necessary modifications are minimal; only the
argument list to \texttt{spectrogram\_dataset()} is concerned:

\begin{verbatim}
ds <- spectrogram_dataset(
  root = "~/.torch-datasets",
  url = "speech_commands_v0.01",
  download = TRUE,
  n_mels = 128
)
\end{verbatim}

Model as well as training code stay the same. Learning rate finder
output also looked rather similar (fig.~\ref{fig-audio-lr-finder-mel}):

\begin{figure}[H]

{\centering \includegraphics{images/audio-lr-finder-mel.png}

}

\caption{\label{fig-audio-lr-finder-mel}Learning rate finder, run on the
Mel-transform-enriched model.}

\end{figure}

Using the same learning rate as previously, I saw training end after
nineteen epochs (fig.~\ref{fig-audio-fit-mel}).

\begin{figure}[H]

{\centering \includegraphics{images/audio-fit-mel.png}

}

\caption{\label{fig-audio-fit-mel}Fitting the Mel-transform-enriched
model.}

\end{figure}

Final validation accuracy was at about 0.86, minimally lower than for
the baseline model.

\begin{verbatim}
"epoch","set","loss","acc"
1,"train",3.35417570164001,0.0639678615574784
1,"valid",3.03987254348456,0.142228059332509
2,"train",2.6629929004931,0.220874536464771
2,"valid",2.20017999761245,0.367815203955501
3,"train",2.03871287512623,0.400262669962917
3,"valid",1.56310861835293,0.559100741656366
...
...
17,"train",0.534631294167899,0.838715492377421
17,"valid",0.524456901585355,0.854063658838072
18,"train",0.526362751336659,0.840801400906469
18,"valid",0.58039141460961,0.839076019777503
19,"train",0.511069792840216,0.847059126493613
19,"valid",0.511746386686961,0.858544499381953
\end{verbatim}

So in this task, no improvement was seen through Mel transformation. But
what matters is for us to be aware of this option, such that in other
projects, we may give this technique a try.

For completeness, here's double-checking against the test set.

\begin{verbatim}
evaluate(fitted, test_dl)
\end{verbatim}

\begin{verbatim}
loss: 0.5081
acc: 0.8555
\end{verbatim}

No surprise here, either. Interestingly though, this time the confusion
matrix looks rather different, thus hinting at a change in inner
workings (fig.~\ref{fig-audio-alluvial-mel}). (To be sure, though, we'd
have to run each version several times.)

\begin{figure}[H]

{\centering \includegraphics{images/audio-alluvial-mel.png}

}

\caption{\label{fig-audio-alluvial-mel}Alluvial plot for the
Mel-transform-enriched setup}

\end{figure}

Leaving further exploration to the experts, we proceed to the final
alternative.

\hypertarget{variation-two-complex-valued-spectograms}{%
\subsection{\texorpdfstring{Variation two: Complex-valued
spectograms\index{spectrogram!complex}}{Variation two: Complex-valued spectograms}}\label{variation-two-complex-valued-spectograms}}

When introducing spectrograms, I focused on their two-dimensional
structure: The Fourier Transform is performed independently for a set of
(overlapping) windows, leaving us with a grid where time slices and
Fourier coefficients are arranged in rows and columns. This grid, we
found, can be treated as an image of sorts. What I didn't expand on were
the actual values contained in that grid. Fourier coefficients are
complex-valued, and thus, if left alone would need to be plotted in
two-dimensional space -- and then, the spectrogram, in turn, would have
to be three-dimensional. In practice, for display (and other) purposes,
one often works with the magnitudes of the coefficients instead, or the
squares of those magnitudes.

Maybe you've noticed that
\texttt{transform\_spectrogram()}\index{\texttt{transform{\textunderscore}spectrogram()} (torchaudio)}
takes an argument, \texttt{power}, which I haven't commented on yet.
This argument lets you specify whether you'd like squared magnitudes
\texttt{(power\ =\ 2}, the default), absolute values
(\texttt{power\ =\ 1}), any other positive value (such as \texttt{0.5},
the one we used when displaying a concrete example) -- or both real and
imaginary parts of the coefficients (\texttt{power\ =\ NULL}). So far,
we've always gone with the default. But we might well wonder whether a
neural network could profit from the additional information contained in
the ``whole'' complex number. After all, when reducing to magnitudes we
lose the phase shifts for the individual coefficients, which could well
contain usable information. In any case, it's worth a try!

On the technical side, necessary modifications (again) are minimal. The
call to \texttt{spectrogram\_dataset()} now makes use of the
\texttt{power} argument. Specifying \texttt{power\ =\ NULL}, we
explicitly request both real and imaginary parts of the Fourier
coefficients. The idea is to pass them to the model's initial
\texttt{nn\_conv2d()} as two separate \emph{channels}.

\begin{verbatim}
ds <- spectrogram_dataset(
  root = "~/.torch-datasets",
  url = "speech_commands_v0.01",
  download = TRUE,
  power = NULL
)

dim(ds[1]$x)
\end{verbatim}

\begin{verbatim}
[1]   2 257 101
\end{verbatim}

Correspondingly, the first convolutional module now is set up to work on
two-channel inputs.

\begin{verbatim}
model <- nn_module(
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(2, 32, kernel_size = 3),
      nn_batch_norm2d(32),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(32, 64, kernel_size = 3),
      nn_batch_norm2d(64),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(64, 128, kernel_size = 3),
      nn_batch_norm2d(128),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(128, 256, kernel_size = 3),
      nn_batch_norm2d(256),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_dropout2d(p = 0.2),
      nn_conv2d(256, 512, kernel_size = 3),
      nn_batch_norm2d(512),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1)),
      nn_dropout2d(p = 0.2)
    )

    self$classifier <- nn_sequential(
      nn_linear(512, 512),
      nn_batch_norm1d(512),
      nn_relu(),
      nn_dropout(p = 0.5),
      nn_linear(512, 30)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
\end{verbatim}

Here is what I saw when running the learning rate finder
(fig.~\ref{fig-audio-lr-finder-complex}):

\begin{figure}[H]

{\centering \includegraphics{images/audio-lr-finder-complex.png}

}

\caption{\label{fig-audio-lr-finder-complex}Learning rate finder, run on
the complex-spectrogram model.}

\end{figure}

This time, training went on for forty epochs.

\begin{figure}[H]

{\centering \includegraphics{images/audio-fit-complex.png}

}

\caption{\label{fig-audio-fit-complex}Fitting the complex-spectrogram
model.}

\end{figure}

Visually, the loss and accuracy curves for the validation set look
smoother than in both other cases (fig.~\ref{fig-audio-fit-complex}).
Checking accuracies, we see:

\begin{verbatim}
"epoch","set","loss","acc"
1,"train",3.09768574611813,0.12396992171405
1,"valid",2.52993751740923,0.284378862793572
2,"train",2.26747255972008,0.333642356819118
2,"valid",1.66693911248562,0.540791100123609
3,"train",1.62294889937818,0.518464153275649
3,"valid",1.11740599192825,0.704882571075402
...
...
38,"train",0.18717994078312,0.943809229501442
38,"valid",0.23587799138006,0.936418417799753
39,"train",0.19338578602993,0.942882159044087
39,"valid",0.230597475945365,0.939431396786156
40,"train",0.190593419024368,0.942727647301195
40,"valid",0.243536252455384,0.936186650185414
\end{verbatim}

With an accuracy of \textasciitilde0.94, we now have a clear
improvement.

We can confirm this on the test set:

\begin{verbatim}
evaluate(fitted, test_dl)
\end{verbatim}

\begin{verbatim}
loss: 0.2373
acc: 0.9324
\end{verbatim}

Finally, the confusion matrix now looks like a cleaned-up version of the
baseline run (fig.~\ref{fig-audio-alluvial-complex}).

\begin{figure}[H]

{\centering \includegraphics{images/audio-alluvial-complex.png}

}

\caption{\label{fig-audio-alluvial-complex}Alluvial plot for the
complex-spectrogram setup.}

\end{figure}

We can safely say that taking into account phase information has
significantly improved performance.

With this, we're ending our tour of deep learning applications. But as
regards the magnificent Fourier Transform, we'll be going into a lot
more detail soon!

\part{Other things to do with torch: Matrices, Fourier Transform, and
Wavelets}

\hypertarget{sec:other-overview}{%
\chapter{Overview}\label{sec:other-overview}}

By now, we've talked a lot about deep learning. But \texttt{torch} is
fruitfully employed in other kinds of tasks, as well -- scientific
applications, for example, that rely on mathematical methods to discover
patterns, relations, and structure.

In this section, we concentrate on three topics. The first is matrix
computations -- a subject whose importance is hard to call into
question, seeing how \emph{all} computations in scientific computation
and machine learning are matrix computations (tensors just being
higher-order matrices). Concretely, we'll solve a least-squares problem
by means of matrix factorization, making use of functions like
\texttt{linalg\_cholesky()}, \texttt{linalg\_qr()}, and
\texttt{linalg\_svd()}. In addition, we'll take a short look at how
convolution (in its original, signal-processing sense) can be
implemented efficiently.

Next, we move on to a famous mathematical method we've already made
(indirect, but highly beneficial) use of: the Discrete Fourier Transform
(DFT). This time, though, we don't just \emph{use} it; instead, we aim
to understand \emph{why} and \emph{how} it works. Once we have that
understanding, a straightforward implementation is a matter of just a
few lines of code. A second chapter is then dedicated to implementing
the DFT efficiently, by means of the Fast Fourier Transform (FFT).
Again, we start by analyzing its workings, and go on to code it from
scratch. You'll see one of the hand-coded methods coming surprisingly
close, in performance, to \texttt{torch}'s own
\texttt{torch\_fft\_fft()}.

Finally, we explore an idea that is far more recent than Fourier
methods; namely, the Wavelet Transform. This transform is widely used in
data analysis, and we'll understand clearly why that's the case. In
\texttt{torch}, there is no dedicated method to compute the Wavelet
Transform; but we'll see how repeated use of \texttt{torch\_fft\_fft()}
results in an efficient implementation.

\hypertarget{sec:matrix-computations-1}{%
\chapter{Matrix computations: Least-squares
problems}\label{sec:matrix-computations-1}}

In this chapter and the next, we'll explore what \texttt{torch} lets us
do with matrices. Here, we take a look at various ways to solve
least-squares problems. The intention is two-fold.

Firstly, this subject often gets pretty technical, or rather,
\emph{computational}, very fast. Depending on your background (and
goals), this may just be what you want; you either know well, or do not
care about so much, the underlying concepts. But for some people, a
purely technical presentation, one that does not also dwell on the
\emph{concepts}, the abstract ideas underlying the subject, may well
fail to convey the fascination, the intellectual attraction it can
exert. That's why, in this chapter, I'll try to present things in a way
that the main ideas don't get obscured by ``computer-sciencey'' details
(details that are easily found in a number of excellent books, anyway).

\hypertarget{five-ways-to-do-least-squares}{%
\section{Five ways to do least
squares}\label{five-ways-to-do-least-squares}}

How do you compute linear least-squares regression? In R, using
\texttt{lm()}; in \texttt{torch}, there is
\texttt{linalg\_lstsq()}\index{\texttt{linalg{\textunderscore}lstsq()}}.
Where R, sometimes, hides complexity from the user, high-performance
computation frameworks like \texttt{torch} tend to ask a bit more
up-front effort, be it careful reading of documentation, or playing
around some, or both. For example, here is the central piece of
documentation for \texttt{linalg\_lstsq()}, elaborating on the
\texttt{driver} parameter to the function:

\begin{quote}
\texttt{driver} chooses the LAPACK/MAGMA function that will be used.

For CPU inputs the valid values are `gels', `gelsy', `gelsd, 'gelss'.

For CUDA input, the only valid driver is `gels', which assumes that A is
full-rank.

To choose the best driver on CPU consider:

\begin{itemize}
\tightlist
\item
  If A is well-conditioned (its condition number is not too large), or
  you do not mind some precision loss:

  \begin{itemize}
  \item
    For a general matrix: `gelsy' (QR with pivoting) (default)
  \item
    If A is full-rank: `gels' (QR)
  \end{itemize}
\item
  If A is not well-conditioned:

  \begin{itemize}
  \item
    `gelsd' (tridiagonal reduction and SVD)
  \item
    But if you run into memory issues: `gelss' (full SVD).
  \end{itemize}
\end{itemize}
\end{quote}

Whether you'll need to know this will depend on the problem you're
solving. But if you do, it certainly will help to have an idea what is
being talked about there, if only in a high-level way.

In our example problem below, we're going to be lucky. All drivers will
return the same result -- but only once we'll have applied a ``trick'',
of sorts. Still, we'll go on and dig deeper into the various methods
used by \texttt{linalg\_lstsq()}, as well as a few others of common use.
Concretely, we'll solve least squares:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  By means of the so-called \emph{normal equations}, the most direct
  way, in the sense that it immediately results from a mathematical
  statement of the problem.
\item
  Again, starting from the normal equations, but making use of
  \emph{Cholesky factorization} in solving them.
\item
  Yet again, taking the normal equations for a point of departure, but
  proceeding by means of \emph{LU} decomposition.
\item
  Fourth, employing another type of factorization -- \emph{QR} -- that,
  together with the final one, accounts for the vast majority of
  decompositions applied ``in the real world''. With QR decomposition,
  the solution algorithm does not start from the normal equations.
\item
  And fifth and finally, making use of \emph{Singular Value
  Decomposition} (SVD). Here, too, the normal equations are not needed.
\end{enumerate}

All methods will first be applied to a real-world dataset, and then, be
tested on a benchmark problem well known for its lack of stability.

\hypertarget{regression-for-weather-prediction}{%
\section{Regression for weather
prediction}\label{regression-for-weather-prediction}}

The dataset we'll use is available from the
\href{http://archive.ics.uci.edu/ml/machine-learning-dAtAbases/00514/Bias_correction_ucl.csv}{UCI
Machine Learning Repository}. The way we'll use it does not quite match
the original purpose of collection; instead of forecasting temperature
with machine learning, the original study (D. Cho et al. (2020)) really
was about bias correction of forecasts obtained from a numerical weather
prediction model. But never mind -- our focus here is on matrix methods,
and the dataset lends itself very well to the kinds of explorations
we're going to do.

\begin{verbatim}
set.seed(777)

library(torch)
torch_manual_seed(777)

library(dplyr)
library(readr)

library(zeallot)

uci <- "https://archive.ics.uci.edu"
ds_path <- "ml/machine-learning-databases/00514"
ds_file <- "Bias_correction_ucl.csv"

# download.file(
#   file.path(uci, ds_path, ds_file),
#   destfile = "resources/matrix-weather.csv"
# )

weather_df <- read_csv("resources/matrix-weather.csv") %>%
  na.omit()
weather_df %>% glimpse()
\end{verbatim}

\begin{verbatim}
Rows: 7,588
Columns: 25
$ station           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
$ Date              <date> 2013-06-30, 2013-06-30,
$ Present_Tmax      <dbl> 28.7, 31.9, 31.6, 32.0, 31.4, 31.9,
$ Present_Tmin      <dbl> 21.4, 21.6, 23.3, 23.4, 21.9, 23.5,
$ LDAPS_RHmin       <dbl> 58.25569, 52.26340, 48.69048,
$ LDAPS_RHmax       <dbl> 91.11636, 90.60472, 83.97359,
$ LDAPS_Tmax_lapse  <dbl> 28.07410, 29.85069, 30.09129,
$ LDAPS_Tmin_lapse  <dbl> 23.00694, 24.03501, 24.56563,
$ LDAPS_WS          <dbl> 6.818887, 5.691890, 6.138224,
$ LDAPS_LH          <dbl> 69.45181, 51.93745, 20.57305,
$ LDAPS_CC1         <dbl> 0.2339475, 0.2255082, 0.2093437,
$ LDAPS_CC2         <dbl> 0.2038957, 0.2517714, 0.2574694,
$ LDAPS_CC3         <dbl> 0.1616969, 0.1594441, 0.2040915,
$ LDAPS_CC4         <dbl> 0.1309282, 0.1277273, 0.1421253,
$ LDAPS_PPT1        <dbl> 0.0000000, 0.0000000, 0.0000000,
$ LDAPS_PPT2        <dbl> 0.000000, 0.000000, 0.000000,
$ LDAPS_PPT3        <dbl> 0.0000000, 0.0000000, 0.0000000,
$ LDAPS_PPT4        <dbl> 0.0000000, 0.0000000, 0.0000000,
$ lat               <dbl> 37.6046, 37.6046, 37.5776, 37.6450,
$ lon               <dbl> 126.991, 127.032, 127.058, 127.022,
$ DEM               <dbl> 212.3350, 44.7624, 33.3068, 45.7160,
$ Slope             <dbl> 2.7850, 0.5141, 0.2661, 2.5348,
$ `Solar radiation` <dbl> 5992.896, 5869.312, 5863.556,
$ Next_Tmax         <dbl> 29.1, 30.5, 31.1, 31.7, 31.2, 31.5,
$ Next_Tmin         <dbl> 21.2, 22.5, 23.9, 24.3, 22.5, 24.0,
\end{verbatim}

The way we're framing the task, basically everything in the dataset
serves (or would serve, if we kept it -- more on that below) as a
predictor. As target, we'll use \texttt{Next\_Tmax}, the maximal
temperature reached on the subsequent day. This means we need to remove
\texttt{Next\_Tmin} from the set of predictors, as it would make for too
powerful of a clue. We'll do the same for \texttt{station}, the weather
station id, and \texttt{Date}. This leaves us with twenty-one
predictors, including measurements of actual temperature
(\texttt{Present\_Tmax}, \texttt{Present\_Tmin}), model forecasts of
various variables (\texttt{LDAPS\_*}), and auxiliary information
(\texttt{lat}, \texttt{lon}, and
\texttt{\textasciigrave{}Solar\ radiation\textasciigrave{}}, among
others).

\begin{verbatim}
weather_df <- weather_df %>%
  select(-c(station, Next_Tmin, Date)) %>%
  mutate(across(.fns = scale))
\end{verbatim}

Note how, above, I've added a line to \emph{standardize} the predictors.
This is the ``trick'' I was alluding to above. We'll talk about why
we're doing this soon.

For \texttt{torch}, we split up the data into two tensors: a matrix
\texttt{A}, containing all predictors, and a vector \texttt{b} that
holds the target.

\begin{verbatim}
weather <- torch_tensor(weather_df %>% as.matrix())
A <- weather[ , 1:-2]
b <- weather[ , -1]

dim(A)
\end{verbatim}

\begin{verbatim}
[1] 7588   21
\end{verbatim}

Now, first let's determine the expected output.

\hypertarget{least-squares-i-setting-expectations-with-lm}{%
\subsection{\texorpdfstring{Least squares (I): Setting expectations with
\texttt{lm()}}{Least squares (I): Setting expectations with lm()}}\label{least-squares-i-setting-expectations-with-lm}}

If there's a least squares implementation we ``believe in'', it surely
must be \texttt{lm()}.

\begin{verbatim}
fit <- lm(Next_Tmax ~ . , data = weather_df)
fit %>% summary()
\end{verbatim}

\begin{verbatim}
Call:
lm(formula = Next_Tmax ~ ., data = weather_df)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.94439 -0.27097  0.01407  0.28931  2.04015 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)        2.605e-15  5.390e-03   0.000 1.000000    
Present_Tmax       1.456e-01  9.049e-03  16.089  < 2e-16 ***
Present_Tmin       4.029e-03  9.587e-03   0.420 0.674312    
LDAPS_RHmin        1.166e-01  1.364e-02   8.547  < 2e-16 ***
LDAPS_RHmax       -8.872e-03  8.045e-03  -1.103 0.270154    
LDAPS_Tmax_lapse   5.908e-01  1.480e-02  39.905  < 2e-16 ***
LDAPS_Tmin_lapse   8.376e-02  1.463e-02   5.726 1.07e-08 ***
LDAPS_WS          -1.018e-01  6.046e-03 -16.836  < 2e-16 ***
LDAPS_LH           8.010e-02  6.651e-03  12.043  < 2e-16 ***
LDAPS_CC1         -9.478e-02  1.009e-02  -9.397  < 2e-16 ***
LDAPS_CC2         -5.988e-02  1.230e-02  -4.868 1.15e-06 ***
LDAPS_CC3         -6.079e-02  1.237e-02  -4.913 9.15e-07 ***
LDAPS_CC4         -9.948e-02  9.329e-03 -10.663  < 2e-16 ***
LDAPS_PPT1        -3.970e-03  6.412e-03  -0.619 0.535766    
LDAPS_PPT2         7.534e-02  6.513e-03  11.568  < 2e-16 ***
LDAPS_PPT3        -1.131e-02  6.058e-03  -1.866 0.062056 .  
LDAPS_PPT4        -1.361e-03  6.073e-03  -0.224 0.822706    
lat               -2.181e-02  5.875e-03  -3.713 0.000207 ***
lon               -4.688e-02  5.825e-03  -8.048 9.74e-16 ***
DEM               -9.480e-02  9.153e-03 -10.357  < 2e-16 ***
Slope              9.402e-02  9.100e-03  10.331  < 2e-16 ***
`Solar radiation`  1.145e-02  5.986e-03   1.913 0.055746 .  
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

Residual standard error: 0.4695 on 7566 degrees of freedom
Multiple R-squared:  0.7802,    Adjusted R-squared:  0.7796 
F-statistic:  1279 on 21 and 7566 DF,  p-value: < 2.2e-16
\end{verbatim}

With an explained variance of 78\%, the forecast is working pretty well.
This is the baseline we want to check all other methods against. To that
purpose, we'll store respective predictions and prediction errors (the
latter being operationalized as root mean squared error, RMSE). For now,
we just have entries for \texttt{lm()}:

\begin{verbatim}
rmse <- function(y_true, y_pred) {
  (y_true - y_pred)^2 %>%
    sum() %>%
    sqrt()
}

all_preds <- data.frame(
  b = weather_df$Next_Tmax,
  lm = fit$fitted.values
)
all_errs <- data.frame(lm = rmse(all_preds$b, all_preds$lm))
all_errs
\end{verbatim}

\begin{verbatim}
       lm
1 40.8369
\end{verbatim}

\hypertarget{least-squares-ii-using-linalg_lstsq}{%
\subsection{\texorpdfstring{Least squares (II): Using
\texttt{linalg\_lstsq()}}{Least squares (II): Using linalg\_lstsq()}}\label{least-squares-ii-using-linalg_lstsq}}

Now, for a moment let's assume this was not about exploring different
approaches, but getting a quick result. In \texttt{torch}, we have
\texttt{linalg\_lstsq()}, a function dedicated specifically to solving
least-squares problems. (This is the function whose documentation I was
citing, above.) Just like we did with \texttt{lm()}, we'd probably just
go ahead and call it, making use of the default settings:

\begin{verbatim}
x_lstsq <- linalg_lstsq(A, b)$solution

all_preds$lstsq <- as.matrix(A$matmul(x_lstsq))
all_errs$lstsq <- rmse(all_preds$b, all_preds$lstsq)

tail(all_preds)
\end{verbatim}

\begin{verbatim}
              b         lm      lstsq
7583 -1.1380931 -1.3544620 -1.3544616
7584 -0.8488721 -0.9040997 -0.9040993
7585 -0.7203294 -0.9675286 -0.9675281
7586 -0.6239224 -0.9044044 -0.9044040
7587 -0.5275154 -0.8738639 -0.8738635
7588 -0.7846007 -0.8725795 -0.8725792
\end{verbatim}

Predictions resemble those of \texttt{lm()} very closely -- so closely,
in fact, that we may guess those tiny differences are just due to
numerical errors surfacing from deep down the respective call stacks.
RMSE, thus, should be equal as well:

\begin{verbatim}
all_errs
\end{verbatim}

\begin{verbatim}
       lm    lstsq
1 40.8369 40.8369
\end{verbatim}

It is; and this is a satisfying outcome. However, it only really came
about due to that ``trick'': normalization. Of course, when I say
``trick'', I don't really mean it. Standardizing the data is a common
operation, and especially with neural networks, it tends to get used
routinely, to speed up training. The point I'd like to make is this:
Frameworks for high-performance computation, like \texttt{torch}, will
often presuppose more domain knowledge, or more up-front analysis, on
the part of the user.

I'll explain.

\hypertarget{interlude-what-if-we-hadnt-standardized-the-data}{%
\subsection{Interlude: What if we hadn't standardized the
data?}\label{interlude-what-if-we-hadnt-standardized-the-data}}

For quick comparison, let's create an alternate matrix of predictors:
\emph{not} normalizing the data, this time.

\begin{verbatim}
weather_df_alt <- 
  read_csv("resources/matrix-weather.csv") %>% 
  na.omit() %>%
  select(-c(station, Next_Tmin, Date)) 

weather_alt <- torch_tensor(weather_df_alt %>% as.matrix())
A_alt <- weather_alt[ , 1:-2]
b_alt <- weather_alt[ , -1]
\end{verbatim}

To set our expectations, we again call \texttt{lm()}:

\begin{verbatim}
fit_alt <- lm(Next_Tmax ~ ., data = weather_df_alt)
all_preds_alt <- data.frame(
  b = weather_df_alt$Next_Tmax,
  lm = fit_alt$fitted.values
)

all_errs_alt <- data.frame(
  lm = rmse(
    all_preds_alt$b,
    all_preds_alt$lm
  )
)

all_errs_alt
\end{verbatim}

\begin{verbatim}
        lm
1 127.0765
\end{verbatim}

Now, we call \texttt{linalg\_lstsq()}, using the default arguments just
like we did before.

\begin{verbatim}
x_lstsq_alt <- linalg_lstsq(A_alt, b_alt)$solution

all_preds_alt$lstsq <- as.matrix(A_alt$matmul(x_lstsq_alt))
all_errs_alt$lstsq <- rmse(
  all_preds_alt$b, all_preds_alt$lstsq
)

all_errs_alt
\end{verbatim}

\begin{verbatim}
        lm    lstsq
1 127.0765 177.9128
\end{verbatim}

Wow -- what happened here? Thinking back of that piece of documentation
I've cited, maybe the default arguments aren't working out that well,
this time. Let's find out why.

\hypertarget{investigating-the-issue}{%
\subsubsection{Investigating ``the
issue''}\label{investigating-the-issue}}

To efficiently solve a linear least-squares problem, \texttt{torch}
calls into LAPACK, a set of Fortran routines designed to efficiently and
scaleably address the tasks most frequently found in linear algebra:
solving linear systems of equations, computing eigenvectors and
eigenvalues, and determining singular values.

The allowed \texttt{driver}s in \texttt{linalg\_lstsq()} correspond to
different LAPACK procedures\footnote{The documentation for
  \texttt{driver} cited above is basically an excerpt from the
  corresponding documentation in
  \href{https://www.netlib.org/lapack/lug/node27.html}{LAPACK}, as we
  can easily verify, since the page in question has conveniently been
  linked in the documentation for \texttt{linalg\_lstsq()}.}, and these
procedures all apply different algorithms in order to solve the problem
-- analogously to what'll we do ourselves, below.

Thus, in investigating what is going on, step one is to determine which
method gets used and why; analyse (if possible) why the result is
unsatisfying; determine the LAPACK routine we'd like to be using
instead, and check what happens if indeed we do. (Of course, given the
little effort involved, we'd probably give all methods a try.)

The main concept involved here is the \emph{rank} of a matrix.

\hypertarget{concepts-i-rank-of-a-matrix}{%
\subsubsection{\texorpdfstring{Concepts (I): Rank of a
matrix\index{matrix!rank}}{Concepts (I): Rank of a matrix}}\label{concepts-i-rank-of-a-matrix}}

\emph{``But wait!''} you may be thinking -- from the above-cited piece
of documentation, it seems like the first thing we should check is not
rank, but \emph{condition number}: whether the matrix is
``well-conditioned''. Yes, the condition number certainly is important,
and we'll get back to it very soon. However, there is something even
more fundamental at work here, something that does not really ``jump to
the eye''.

The central piece of information is found in that LAPACK\index{LAPACK}
piece of documentation we're being referred to by
\texttt{linalg\_lstsq()}. Between the four routines GELS, GELSY, GELSD,
and GELSS, differences are not restricted to implementation. The
\emph{goal} of optimization differs, as well. The rationale is the
following. Throughout, let's assume we're working with a matrix that has
more rows than columns (more observations than features, in the
most-frequent case):

\begin{itemize}
\item
  If the matrix is full-rank -- meaning, its columns are linearly
  independent -- there is no ``perfect'' solution. The problem is
  over-determined. All we can do is find the best possible
  approximation. This is done by minimizing the prediction error --
  we'll come back to that when discussing the normal equations. Minimize
  prediction error is what the GELS routine does, and it is GELS we
  should use when we have a full-rank matrix of predictors.
\item
  If the matrix is not full-rank, the problem is under-determined; there
  is an infinite number of solutions. All the remaining routines --
  GELSY, GELSD, and GELSS -- are suited to this situation. While they do
  proceed differently, they all pursue the same strategy, different from
  the one followed by GELS: In addition to the prediction error, they
  \emph{also} minimize the vector of coefficients. This is called
  finding a minimum-norm least-squares solution.
\end{itemize}

In sum, GELS (for full-rank matrices) and the three of GELSY, GELSD, and
GELSS (for when the matrix is rank-deficient) intentionally follow
different optimization criteria.

Now, as per the documentation for \texttt{linalg\_lstsq()}, when no
\texttt{driver} is passed explicitly, it is GELSY that gets called. That
should be fine if our matrix is rank-deficient -- but is it?

\begin{verbatim}
linalg_matrix_rank(A_alt)
\end{verbatim}

\begin{verbatim}
torch_tensor
21
[ CPULongType{} ]
\end{verbatim}

The matrix has twenty-one columns; so if its rank is twenty-one, then it
is full-rank for sure. We definitely want to be calling the GELS
routine.

\hypertarget{calling-linalg_lstsq-the-right-way}{%
\subsubsection{\texorpdfstring{Calling \texttt{linalg\_lstsq()} the
right
way}{Calling linalg\_lstsq() the right way}}\label{calling-linalg_lstsq-the-right-way}}

Now that we know what to pass for \texttt{driver}, here is the modified
call:

\begin{verbatim}
x_lstsq_alt <- linalg_lstsq(
  A_alt, b_alt,
  driver = "gels"
)$solution

all_preds_alt$lstsq <- as.matrix(A_alt$matmul(x_lstsq_alt))
all_errs_alt$lstsq <- rmse(
  all_preds_alt$b,
  all_preds_alt$lstsq
)

all_errs_alt
\end{verbatim}

\begin{verbatim}
        lm    lstsq
1 127.0765 127.9489
\end{verbatim}

Now, the respective RMSE values are very close. You'll be wondering,
though: Why didn't we have to specify the Fortran routine when working
with the \emph{standardized} matrix?

\hypertarget{why-did-standardization-help}{%
\subsubsection{Why did standardization
help?}\label{why-did-standardization-help}}

For our matrix, what standardization did was reduce significantly the
range spanned by the singular values. With \texttt{A}, the standardized
matrix, the largest singular value is about ten times as large as the
smallest one:

\begin{verbatim}
svals_normalized_A <- linalg_svdvals(A)/linalg_svdvals(A)[1]
svals_normalized_A %>% as.numeric()
\end{verbatim}

\begin{verbatim}
[1] 1.0000000 0.7473214 0.5929527 0.5233989 0.5188764 0.4706140
[7] 0.4391665 0.4249273 0.4034659 0.3815900 0.3621315 0.3557949
[13] 0.3297923 0.2707912 0.2489560 0.2229859 0.2175170 0.1852890
[19] 0.1627083 0.1553169 0.1075778
\end{verbatim}

While with \texttt{A\_alt}, it is a million times as large:

\begin{verbatim}
svals_normalized_A_alt <- linalg_svdvals(A_alt) /
  linalg_svdvals(A_alt)[1]
svals_normalized_A_alt %>% as.numeric()
\end{verbatim}

\begin{verbatim}
[1] 1.000000e+00 1.014369e-02 6.407313e-03 2.881966e-03
[5] 2.236537e-03 9.633782e-04 6.678377e-04 3.988165e-04
[9] 3.584047e-04 3.137257e-04 2.699152e-04 2.383501e-04
[13] 2.234150e-04 1.803384e-04 1.625245e-04 1.300101e-04
[17] 4.312536e-05 3.463851e-05 1.964120e-05 1.689913e-05
[18] 8.419599e-06
\end{verbatim}

Why is this important? It's here that we finally get back to the
\emph{condition number}.

\hypertarget{concepts-ii-condition-number}{%
\subsubsection{\texorpdfstring{Concepts (II): Condition
number\index{matrix!condition number}}{Concepts (II): Condition number}}\label{concepts-ii-condition-number}}

The higher the so-called \emph{condition number} of a matrix, the more
likely we are to run into problems of numerical stability when computing
with it. In torch, \texttt{linalg\_cond()} is used to obtain the
condition number. Let's compare the condition numbers for \texttt{A} and
\texttt{A\_alt}, respectively.

\begin{verbatim}
linalg_cond(A)
linalg_cond(A_alt)
\end{verbatim}

\begin{verbatim}
torch_tensor
9.2956
[ CPUFloatType{} ]

torch_tensor
118770
[ CPUFloatType{} ]
\end{verbatim}

That is quite a difference! How does it arise?

The condition number is defined as the matrix norm\index{matrix!norm} of
\texttt{A}, divided by the norm of its inverse. Different kinds of norm
may be used; the default is the 2-norm. In this case, condition number
can be computed from the matrix's singular
values\index{matrix!singular values}: Namely, the 2-norm of \texttt{A}
equals the largest singular value, while that of its inverse is given by
the smallest one.

We can verify this ourselves, using \texttt{linalg\_svdvals()} as
before:

\begin{verbatim}
linalg_svdvals(A)[1]/linalg_svdvals(A)[21]
linalg_svdvals(A_alt)[1]/linalg_svdvals(A_alt)[21]
\end{verbatim}

\begin{verbatim}
torch_tensor
9.29559
[ CPUFloatType{} ]

torch_tensor
118770
[ CPUFloatType{} ]
\end{verbatim}

To reiterate, this is a substantial difference. Incidentally, do you
remember that in the case of \texttt{A\_alt}, RMSE was a tiny bit worse
for \texttt{linalg\_lstsq()} than for \texttt{lm()}, even when using the
appropriate routine, GELS? Given that both essentially use the same
algorithm (QR factorization, to be introduced very soon) this may very
well have been due to numerical errors, arising from the high condition
number of \texttt{A\_alt}.

By now, I may have convinced you that with \texttt{torch}'s
\texttt{linalg} component, it helps to know a bit about how the
most-in-use least-squares algorithms work. Let's get acquainted.

\hypertarget{least-squares-iii-the-normal-equations}{%
\subsection{\texorpdfstring{Least squares (III): The normal
equations\index{normal equations}}{Least squares (III): The normal equations}}\label{least-squares-iii-the-normal-equations}}

We start by stating the goal. Given a matrix, \(\mathbf{A}\), that holds
features in its columns and observations in its rows, and a vector of
observed outcomes, \(\mathbf{b}\), we want to find regression
coefficients, one for each feature, that allow to approximate
\(\mathbf{b}\) as well as possible. Call the vector of regression
coefficients \(\mathbf{x}\). To obtain it, we need to solve a
simultaneous system of equations, that in matrix notation appears as

\[
\mathbf{Ax} = \mathbf{b}
\]

If \(\mathbf{b}\) were a square, invertible matrix, the solution could
directly be computed as \(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\). This
will hardly ever be possible, though; we'll (hopefully) always have more
observations than predictors. Another approach is needed. It directly
starts from the problem statement.

When we use the columns of \(\mathbf{A}\) to approximate \(\mathbf{b}\),
that approximation necessarily is in the column space of \(\mathbf{A}\).
\(\mathbf{b}\), on the other hand, normally won't be. We want those two
to be as close as possible; in other words, we want to minimize the
distance between them. Choosing the 2-norm for the distance, this yields
the objective

\[
minimize \ ||\mathbf{Ax}-\mathbf{b}||^2
\]

This distance is the (squared) length of the vector of prediction
errors. That vector necessarily is orthogonal to \(\mathbf{A}\) itself.
That is, when we multiply it with \(\mathbf{A}\), we get the zero
vector:

\[
\mathbf{A}^T(\mathbf{Ax} - \mathbf{b}) = \mathbf{0}
\]

A rearrangement of this equation yields the so-called \emph{normal
equations}:

\[
\mathbf{A}^T \mathbf{A} \mathbf{x} = \mathbf{A}^T \mathbf{b}
\]

These may be solved for \(\mathbf{x}\), computing the inverse of
\(\mathbf{A}^T\mathbf{A}\):

\[
\mathbf{x} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
\]

\(\mathbf{A}^T\mathbf{A}\) is a square matrix. It still might not be
invertible, in which case the so-called pseudoinverse would be computed
instead. In our case, this will not be needed; we already know
\(\mathbf{A}\) has full rank, and so does \(\mathbf{A}^T\mathbf{A}\).

Thus, from the normal equations we have derived a recipe for computing
\(\mathbf{b}\). Let's put it to use, and compare with what we got from
\texttt{lm()} and \texttt{linalg\_lstsq()}.

\begin{verbatim}
AtA <- A$t()$matmul(A)
Atb <- A$t()$matmul(b)
inv <- linalg_inv(AtA)
x <- inv$matmul(Atb)

all_preds$neq <- as.matrix(A$matmul(x))
all_errs$neq <- rmse(all_preds$b, all_preds$neq)

all_errs
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq
1 40.8369 40.8369 40.8369
\end{verbatim}

Having confirmed that the direct way works, we may allow ourselves some
sophistication. Four different matrix factorizations will make their
appearance: Cholesky, LU, QR, and Singular Value Decomposition. The
goal, in every case, is to avoid the expensive computation of the
(pseudo-) inverse. That's what all methods have in common. However, they
do not differ ``just'' in the way the matrix is factorized, but also, in
\emph{which} matrix is. This has to do with the constraints the various
methods impose. Roughly speaking, the order they're listed in above
reflects a falling slope of preconditions, or put differently, a rising
slope of generality. Due to the constraints involved, the first two
(Cholesky, as well as LU decomposition) will be performed on
\(\mathbf{A}^T\mathbf{A}\), while the latter two (QR and SVD) operate on
\(\mathbf{A}\) directly. With them, there never is a need to compute
\(\mathbf{A}^T\mathbf{A}\).

\hypertarget{least-squares-iv-cholesky-decomposition}{%
\subsection{\texorpdfstring{Least squares (IV): Cholesky
decomposition\index{matrix!Cholesky decomposition}}{Least squares (IV): Cholesky decomposition}}\label{least-squares-iv-cholesky-decomposition}}

In Cholesky decomposition, a matrix is factored into two triangular
matrices of the same size, with one being the transpose of the other.
This commonly is written either

\[
\mathbf{A} = \mathbf{L} \mathbf{L}^T
\] or

\[
\mathbf{A} = \mathbf{R}^T\mathbf{R}
\]

Here symbols \(\mathbf{L}\) and \(\mathbf{R}\) denote lower-triangular
and upper-triangular matrices, respectively.

For Cholesky decomposition to be possible, a matrix has to be both
symmetric and positive definite. These are pretty strong conditions,
ones that will not often be fulfilled in practice. In our case,
\(\mathbf{A}\) is not symmetric; this immediately implies we have to
operate on \(\mathbf{A}^T\mathbf{A}\) instead. And since \(\mathbf{A}\)
already is positive definite, we know that \(\mathbf{A}^T\mathbf{A}\)
is, as well.

In \texttt{torch}, we obtain the Cholesky decomposition of a matrix
using \texttt{linalg\_cholesky()}. By default, this call will return
\(\mathbf{L}\), a lower-triangular matrix.

\begin{verbatim}
# AtA = L L_t
AtA <- A$t()$matmul(A)
L <- linalg_cholesky(AtA)
\end{verbatim}

Let's check that we can reconstruct \(\mathbf{A}\) from \(\mathbf{L}\):

\begin{verbatim}
LLt <- L$matmul(L$t())
diff <- LLt - AtA
linalg_norm(diff, ord = "fro")
\end{verbatim}

\begin{verbatim}
torch_tensor
0.00258896
[ CPUFloatType{} ]
\end{verbatim}

Here, I've computed the Frobenius norm of the difference between the
original matrix and its reconstruction. The Frobenius norm individually
sums up all matrix entries, and returns the square root. In theory, we'd
like to see zero here; but in the presence of numerical errors, the
result is sufficient to indicate that the factorization worked fine.

Now that we have \(\mathbf{L}\mathbf{L}^T\) instead of
\(\mathbf{A}^T\mathbf{A}\), how does that help us? It's here that the
magic happens, and you'll find the same type of magic at work in the
remaining three methods. The idea is that due to some decomposition, a
more performant way arises of solving the system of equations that
constitute a given task.

With \(\mathbf{L}\mathbf{L}^T\), the point is that \(\mathbf{L}\) is
triangular, and when that's the case the linear system can be solved by
simple substitution. That is best visible with a tiny example:

\[
\begin{bmatrix}
  1 & 0 & 0\\
  2 & 3 & 0\\
  3 & 4 & 1
\end{bmatrix}
\begin{bmatrix}
  x1\\
  x2\\
  x3
\end{bmatrix}
=
\begin{bmatrix}
  1\\
  11\\
  15
\end{bmatrix}
\]

Starting in the top row, we immediately see that \(x1\) equals \(1\);
and once we know \emph{that} it is straightforward to calculate, from
row two, that \(x2\) must be \(3\). The last row then tells us that
\(x3\) must be \(0\).

In code, \texttt{torch\_triangular\_solve()} is used to efficiently
compute the solution to a linear system of equations where the matrix of
predictors is lower- or upper-triangular. An additional requirement is
for the matrix to be symmetric -- but that condition we already had to
satisfy in order to be able to use Cholesky factorization.

By default, \texttt{torch\_triangular\_solve()} expects the matrix to be
upper- (not lower-)triangular; but there is a function parameter,
\texttt{upper}, that lets us correct that expectation. The return value
is a list, and its first item contains the desired solution. To
illustrate, here is \texttt{torch\_triangular\_solve()}, applied to the
toy example we manually solved above:

\begin{verbatim}
some_L <- torch_tensor(
  matrix(c(1, 0, 0, 2, 3, 0, 3, 4, 1), nrow = 3, byrow = TRUE)
)
some_b <- torch_tensor(matrix(c(1, 11, 15), ncol = 1))

x <- torch_triangular_solve(
  some_b,
  some_L,
  upper = FALSE
)[[1]]
x
\end{verbatim}

\begin{verbatim}
torch_tensor
 1
 3
 0
[ CPUFloatType{3,1} ]
\end{verbatim}

Returning to our running example, the normal equations now look like
this:

\[
\mathbf{L}\mathbf{L}^T \mathbf{x} = \mathbf{A}^T \mathbf{b}
\]

We introduce a new variable, \(\mathbf{y}\), to stand for
\(\mathbf{L}^T \mathbf{x}\),

\[
\mathbf{L}\mathbf{y} = \mathbf{A}^T \mathbf{b}
\]

and compute the solution to \emph{this} system:

\begin{verbatim}
Atb <- A$t()$matmul(b)

y <- torch_triangular_solve(
  Atb$unsqueeze(2),
  L,
  upper = FALSE
)[[1]]
\end{verbatim}

Now that we have \(y\), we look back at how it was defined:

\[
\mathbf{y} = \mathbf{L}^T \mathbf{x}
\]

To determine \(\mathbf{x}\), we can thus again use
\texttt{torch\_triangular\_solve()}:

\begin{verbatim}
x <- torch_triangular_solve(y, L$t())[[1]]
\end{verbatim}

And there we are.

As usual, we compute the prediction error:

\begin{verbatim}
all_preds$chol <- as.matrix(A$matmul(x))
all_errs$chol <- rmse(all_preds$b, all_preds$chol)

all_errs
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol
1 40.8369 40.8369 40.8369 40.8369
\end{verbatim}

Now that you've seen the rationale behind Cholesky factorization -- and,
as already suggested, the idea carries over to all other decompositions
-- you might like to save yourself some work making use of a dedicated
convenience function, \texttt{torch\_cholesky\_solve()}. This will
render obsolete the two calls to \texttt{torch\_triangular\_solve()}.

The following lines yield the same output as the code above -- but, of
course, they \emph{do} hide the underlying
magic.\index{\texttt{linalg{\textunderscore}cholesky()}}

\begin{verbatim}
L <- linalg_cholesky(AtA)

x <- torch_cholesky_solve(Atb$unsqueeze(2), L)

all_preds$chol2 <- as.matrix(A$matmul(x))
all_errs$chol2 <- rmse(all_preds$b, all_preds$chol2)
all_errs
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol   chol2
1 40.8369 40.8369 40.8369 40.8369 40.8369
\end{verbatim}

Let's move on to the next method -- equivalently, to the next
factorization.

\hypertarget{least-squares-v-lu-factorization}{%
\subsection{\texorpdfstring{Least squares (V): LU
factorization\index{matrix!LU factorization}}{Least squares (V): LU factorization}}\label{least-squares-v-lu-factorization}}

LU factorization is named after the two factors it introduces: a
lower-triangular matrix, \(\mathbf{L}\), as well as an upper-triangular
one, \(\mathbf{U}\). In theory, there are no restrictions on LU
decomposition: Provided we allow for row exchanges, effectively turning
\(\mathbf{A} = \mathbf{L}\mathbf{U}\) into
\(\mathbf{A} = \mathbf{P}\mathbf{L}\mathbf{U}\) (where \(\mathbf{P}\) is
a permutation matrix), we can factorize any matrix.

In practice, though, if we want to make use of
\texttt{torch\_triangular\_solve()} , the input matrix has to be
symmetric. Therefore, here too we have to work with
\(\mathbf{A}^T\mathbf{A}\), not \(\mathbf{A}\) directly. (And that's why
I'm showing LU decomposition right after Cholesky -- they're similar in
what they make us do, though not at all similar in spirit.)

Working with \(\mathbf{A}^T\mathbf{A}\) means we're again starting from
the normal equations. We factorize \(\mathbf{A}^T\mathbf{A}\), then
solve two triangular systems to arrive at the final solution. Here are
the steps, including the not-always-needed permutation matrix
\(\mathbf{P}\):

\[
\begin{aligned}
\mathbf{A}^T \mathbf{A} \mathbf{x} &= \mathbf{A}^T \mathbf{b} \\
\mathbf{P} \mathbf{L}\mathbf{U} \mathbf{x} &= \mathbf{A}^T \mathbf{b} \\
\mathbf{L} \mathbf{y} &= \mathbf{P}^T \mathbf{A}^T \mathbf{b} \\
\mathbf{y} &= \mathbf{U} \mathbf{x}
\end{aligned}
\]

We see that when \(\mathbf{P}\) \emph{is} needed, there is an additional
computation: Following the same strategy as we did with Cholesky, we
want to move \(\mathbf{P}\) from the left to the right. Luckily, what
may look expensive -- computing the inverse -- is not: For a permutation
matrix, its transpose reverses the operation.

Code-wise, we're already familiar with most of what we need to do. The
only missing piece is
\texttt{torch\_lu()}\index{\texttt{torch{\textunderscore}lu()}}.
\texttt{torch\_lu()} returns a list of two tensors, the first a
compressed representation of the three matrices \(\mathbf{P}\),
\(\mathbf{L}\), and \(\mathbf{U}\). We can uncompress it using
\texttt{torch\_lu\_unpack()} :

\begin{verbatim}
lu <- torch_lu(AtA)

c(P, L, U) %<-% torch_lu_unpack(lu[[1]], lu[[2]]) 
\end{verbatim}

We move \(\mathbf{P}\) to the other side:

\begin{verbatim}
Atb <- P$t()$matmul(Atb)
\end{verbatim}

All that remains to be done is solve two triangular systems, and we are
done:

\begin{verbatim}
y <- torch_triangular_solve(
  Atb$unsqueeze(2),
  L,
  upper = FALSE
)[[1]]
x <- torch_triangular_solve(y, U)[[1]]

all_preds$lu <- as.matrix(A$matmul(x))
all_errs$lu <- rmse(all_preds$b, all_preds$lu)
all_errs[1, -5]
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol      lu
1 40.8369 40.8369 40.8369 40.8369 40.8369
\end{verbatim}

As with Cholesky decomposition, we can save ourselves the trouble of
calling \texttt{torch\_triangular\_solve()} twice.
\texttt{torch\_lu\_solve()} takes the decomposition, and directly
returns the final solution:

\begin{verbatim}
lu <- torch_lu(AtA)
x <- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])

all_preds$lu2 <- as.matrix(A$matmul(x))
all_errs$lu2 <- rmse(all_preds$b, all_preds$lu2)
all_errs[1, -5]
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol      lu      lu
1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369
\end{verbatim}

Now, we look at the two methods that don't require computation of
\(\mathbf{A}^T\mathbf{A}\).

\hypertarget{least-squares-vi-qr-factorization}{%
\subsection{\texorpdfstring{Least squares (VI): QR
factorization\index{matrix!QR factorization}}{Least squares (VI): QR factorization}}\label{least-squares-vi-qr-factorization}}

Any matrix can be decomposed into an orthogonal matrix, \(\mathbf{Q}\),
and an upper-triangular matrix, \(\mathbf{R}\). QR factorization is
probably the most popular approach to solving least-squares problems; it
is, in fact, the method used by R's \texttt{lm()}. In what ways, then,
does it simplify the task?

As to \(\mathbf{R}\), we already know how it is useful: By virtue of
being triangular, it defines a system of equations that can be solved
step-by-step, by means of mere substitution. \(\mathbf{Q}\) is even
better. An orthogonal matrix is one whose columns are orthogonal --
meaning, mutual dot products are all zero -- and have unit norm; and the
nice thing about such a matrix is that its inverse equals its transpose.
In general, the inverse is hard to compute; the transpose, however, is
easy. Seeing how computation of an inverse -- solving
\(\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}\) -- is just the central task in
least squares, it's immediately clear how significant this is.

Compared to our usual scheme, this leads to a slightly shortened recipe.
There is no ``dummy'' variable \(\mathbf{y}\) anymore. Instead, we
directly move \(\mathbf{Q}\) to the other side, computing the transpose
(which \emph{is} the inverse). All that remains, then, is
back-substitution. Also, since every matrix has a QR decomposition, we
now directly start from \(\mathbf{A}\) instead of
\(\mathbf{A}^T\mathbf{A}\):

\[
\begin{aligned}
\mathbf{A}\mathbf{x} &= \mathbf{b}\\
\mathbf{Q}\mathbf{R}\mathbf{x} &= \mathbf{b}\\
\mathbf{R}\mathbf{x} &= \mathbf{Q}^T\mathbf{b}\\
\end{aligned}
\]

In \texttt{torch},
\texttt{linalg\_qr()}\index{\texttt{linalg{\textunderscore}qr()}} gives
us the matrices \(\mathbf{Q}\) and \(\mathbf{R}\).

\begin{verbatim}
c(Q, R) %<-% linalg_qr(A)
\end{verbatim}

On the right side, we used to have a ``convenience variable'' holding
\(\mathbf{A}^T\mathbf{b}\) ; here, we skip that step, and instead, do
something ``immediately useful'': move \(\mathbf{Q}\) to the other side.

\begin{verbatim}
Qtb <- Q$t()$matmul(b)
\end{verbatim}

The only remaining step now is to solve the remaining triangular system.

\begin{verbatim}
x <- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]

all_preds$qr <- as.matrix(A$matmul(x))
all_errs$qr <- rmse(all_preds$b, all_preds$qr)
all_errs[1, -c(5,7)]
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol      lu      qr
1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 
\end{verbatim}

By now, you'll be expecting for me to end this section saying ``there is
also a dedicated solver in \texttt{torch}/\texttt{torch\_linalg}, namely
\ldots{}''). Well, not literally, no; but effectively, yes. If you call
\texttt{linalg\_lstsq()} passing \texttt{driver\ =\ "gels"}, it is QR
factorization that will be used.

\hypertarget{least-squares-vii-singular-value-decomposition-svd}{%
\subsection{\texorpdfstring{Least squares (VII): Singular Value
Decomposition
(SVD)\index{matrix!singular value decomposition}}{Least squares (VII): Singular Value Decomposition (SVD)}}\label{least-squares-vii-singular-value-decomposition-svd}}

In true climactic order, the last factorization method we discuss is the
most versatile, most diversely applicable, most semantically meaningful
one: \emph{Singular Value Decomposition (SVD)}. The third aspect,
fascinating though it is, does not relate to our current task, so I
won't go into it here. Here, it is universal applicability that matters:
Every matrix can be composed into components SVD-style.

Singular Value Decomposition factors an input \(\mathbf{A}\) into two
orthogonal matrices, called \(\mathbf{U}\) and \(\mathbf{V}^T\), and a
diagonal one, named \(\symbf{\Sigma}\), such that
\(\mathbf{A} = \mathbf{U} \symbf{\Sigma} \mathbf{V}^T\). Here
\(\mathbf{U}\) and \(\mathbf{V}^T\) are the \emph{left} and \emph{right
singular vectors}, and \(\symbf{\Sigma}\) holds the \emph{singular
values}.

\[
\begin{aligned}
\mathbf{A}\mathbf{x} &= \mathbf{b}\\
\mathbf{U}\symbf{\Sigma}\mathbf{V}^T\mathbf{x} &= \mathbf{b}\\
\symbf{\Sigma}\mathbf{V}^T\mathbf{x} &= \mathbf{U}^T\mathbf{b}\\
\mathbf{V}^T\mathbf{x} &= \mathbf{y}\\
\end{aligned}
\]

We start by obtaining the factorization, using
\texttt{linalg\_svd()}\index{\texttt{linalg{\textunderscore}svd()}} .
The argument \texttt{full\_matrices\ =\ FALSE} tells \texttt{torch} that
we want a \(\mathbf{U}\) of dimensionality same as \(\mathbf{A}\), not
expanded to 7588 x 7588.

\begin{verbatim}
c(U, S, Vt) %<-% linalg_svd(A, full_matrices = FALSE)

dim(U)
dim(S)
dim(Vt)
\end{verbatim}

\begin{verbatim}
[1] 7588   21
[1] 21
[1] 21 21
\end{verbatim}

We move \(\mathbf{U}\) to the other side -- a cheap operation, thanks to
\(\mathbf{U}\) being orthogonal.

\begin{verbatim}
Utb <- U$t()$matmul(b)
\end{verbatim}

With both \(\mathbf{U}^T\mathbf{b}\) and \(\symbf{\Sigma}\) being
same-length vectors, we can use element-wise multiplication to do the
same for \(\symbf{\Sigma}\). We introduce a temporary variable,
\texttt{y}, to hold the result.

\begin{verbatim}
y <- Utb / S
\end{verbatim}

Now left with the final system to solve,
\(\mathbf{\mathbf{V}^T\mathbf{x} = \mathbf{y}}\), we again profit from
orthogonality -- this time, of the matrix \(\mathbf{V}^T\).

\begin{verbatim}
x <- Vt$t()$matmul(y)
\end{verbatim}

Wrapping up, let's calculate predictions and prediction error:

\begin{verbatim}
all_preds$svd <- as.matrix(A$matmul(x))
all_errs$svd <- rmse(all_preds$b, all_preds$svd)

all_errs[1, -c(5, 7)]
\end{verbatim}

\begin{verbatim}
       lm   lstsq     neq    chol      lu     qr      svd
1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369
\end{verbatim}

That concludes our tour of important least-squares algorithms. Wrapping
up the example, we take a quick look at performance.

\hypertarget{checking-execution-times}{%
\subsection{Checking execution times}\label{checking-execution-times}}

Like I said, the focus in this chapter is on concepts, not performance.
But once you work with bigger datasets, you inevitably will care about
speed. Also, it's just interesting to see how fast those methods are!
So, let's do a quick performance benchmark. Just, please, don't
extrapolate from these results -- instead, run analogous code on the
data you care about.

To time them, we need all algorithms encapsulated in their respective
functions. Here they are:

\begin{verbatim}
# normal equations
ls_normal_eq <- function(A, b) {
  AtA <- A$t()$matmul(A)
  x <- linalg_inv(AtA)$matmul(A$t())$matmul(b)
  x
}

# normal equations and Cholesky decomposition (done manually)
# A_t A x = A_t b
# L L_t x = A_t b
# L y = A_t b  
# L_t x = y
ls_cholesky_diy <- function(A, b) {
  AtA <- A$t()$matmul(A)
  Atb <- A$t()$matmul(b)
  L <- linalg_cholesky(AtA)
  y <- torch_triangular_solve(
    Atb$unsqueeze(2),
    L,
    upper = FALSE
  )[[1]]
  x <- torch_triangular_solve(y, L$t())[[1]]
  x
}

# torch's Cholesky solver
ls_cholesky_solve <- function(A, b) {
  AtA <- A$t()$matmul(A)
  Atb <- A$t()$matmul(b)
  L <- linalg_cholesky(AtA)
  x <- torch_cholesky_solve(Atb$unsqueeze(2), L)
  x
}

# normal equations and LU factorization (done manually)
# A_t A x = A_t b
# P L U x = A_t b
# L y = P_t A_t b          # where y = U x
# U x = y
ls_lu_diy <- function(A, b) {
  AtA <- A$t()$matmul(A)
  Atb <- A$t()$matmul(b)
  lu <- torch_lu(AtA)
  c(P, L, U) %<-% torch_lu_unpack(lu[[1]], lu[[2]]) 
  Atb <- P$t()$matmul(Atb)
  y <- torch_triangular_solve(
    Atb$unsqueeze(2),
    L,
    upper = FALSE
  )[[1]]
  x <- torch_triangular_solve(y, U)[[1]]
  x
}

# torch's LU solver
ls_lu_solve <- function(A, b) {
  AtA <- A$t()$matmul(A) 
  Atb <- A$t()$matmul(b)
  lu <- torch_lu(AtA)
  m = lu[[1]]
  pivots = lu[[2]]
  x <- torch_lu_solve(Atb$unsqueeze(2), m, pivots)
  x
}

# QR factorization
# A x = b
# Q R x = b
# R x = Q_t b 
ls_qr <- function(A, b) {
  c(Q, R) %<-% linalg_qr(A)
  Qtb <- Q$t()$matmul(b)
  x <- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]
  x
}

# SVD
# A x = b
# U S V_ x = b
# S V_t x = U_t b
# S y = U_t b 
# V_t x = y
ls_svd <- function(A, b) {
  c(U, S, Vt) %<-% linalg_svd(A, full_matrices = FALSE)
  Utb <- U$t()$matmul(b)
  y <- Utb / S
  x <- Vt$t()$matmul(y)
  x
}

# torch's general least squares solver
ls_lstsq <- function(A, b) {
  x <- linalg_lstsq(A, b)
  x
}
\end{verbatim}

We use the \texttt{bench} package to profile those methods. The
\texttt{mark()} function does a lot more than just track time; however,
here we just take a glance at the distributions of execution times
(fig.~\ref{fig-least-squares-benchmark}):

\begin{verbatim}
set.seed(777)
torch_manual_seed(777)
library(bench)
library(ggplot2)

res <- mark(ls_normal_eq(A, b),
            ls_cholesky_diy(A, b),
            ls_cholesky_solve(A, b),
            ls_lu_diy(A, b),
            ls_lu_solve(A, b),
            ls_qr(A, b),
            ls_svd(A, b),
            ls_lstsq(A, b)$solution,
            min_iterations = 1000)

autoplot(res, type = "ridge") + theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/least-squares-benchmark.png}

}

\caption{\label{fig-least-squares-benchmark}Timing least-squares
algorithms, by example.}

\end{figure}

In conclusion, we saw how different ways of factorizing a matrix can
help in solving least squares problems. We also quickly showed a way to
time those strategies; however, speed is not all that counts. We want
the solution to be reliable, as well. The technical term here is
\emph{stability}.

\hypertarget{a-quick-look-at-stability}{%
\section{\texorpdfstring{A quick look at
stability\index{matrix!conditioning and stability}}{A quick look at stability}}\label{a-quick-look-at-stability}}

We've already talked about condition numbers. The concept of stability
is similar in spirit, but refers to an \emph{algorithm} instead of a
\emph{matrix}. In both cases, the idea is that small changes in the
input to a calculation should lead to small changes in the output. Whole
books have been dedicated to this topic, so I'll refrain from going into
details\footnote{To learn more, consider consulting one of those books,
  for example, the widely-used (and concise) treatment by Trefethen and
  Bau (1997).}.

Instead, I'll use an example of an ill-conditioned least-squares problem
-- meaning, the matrix is ill-conditioned -- for us to form an idea
about the stability of the algorithms we've discussed\footnote{The
  example is taken from the book by Trefethen and Bau referred to in the
  footnote above. Credits to Rachel Thomas, who brought this to my
  attention by virtue of using it in her
  \href{https://github.com/fastai/numerical-linear-algebra}{numerical
  linear algebra course}.}.

The matrix of predictors is a 100 x 15 Vandermonde matrix, created like
so:

\begin{verbatim}
set.seed(777)
torch_manual_seed(777)

m <- 100
n <- 15
t <- torch_linspace(0, 1, m)$to(dtype = torch_double())

A <- torch_vander(t, N = n, increasing = TRUE)$to(
  dtype = torch_double()
)
\end{verbatim}

Condition number is very high:

\begin{verbatim}
linalg_cond(A)
\end{verbatim}

\begin{verbatim}
torch_tensor
2.27178e+10
[ CPUDoubleType{} ]
\end{verbatim}

Even higher is the condition number obtained when we multiply it with
its transpose -- remember that some algorithms actually need to work
with \emph{this} matrix:

\begin{verbatim}
linalg_cond(A$t()$matmul(A))
\end{verbatim}

\begin{verbatim}
torch_tensor
7.27706e+17
[ CPUDoubleType{} ]
\end{verbatim}

Next, we have the prediction target:

\begin{verbatim}
b <- torch_exp(torch_sin(4*t))
b <- b/2006.787453080206
\end{verbatim}

In our example above, we ended up with the same RMSE for all methods. It
will be interesting to see what happens here. I'll restrict myself to
the ``DIY'' ones among the methods shown before. Here they are, listed
again for convenience:

\begin{verbatim}
# normal equations
ls_normal_eq <- function(A, b) {
  AtA <- A$t()$matmul(A)
  x <- linalg_inv(AtA)$matmul(A$t())$matmul(b)
  x
}

# normal equations and Cholesky decomposition (done manually)
# A_t A x = A_t b
# L L_t x = A_t b
# L y = A_t b  
# L_t x = y
ls_cholesky_diy <- function(A, b) {
  # add a small multiple of the identity matrix 
  # to counteract numerical instability
  # if Cholesky decomposition fails in your 
  # setup, increase eps
  eps <- 1e-10
  id <- eps * torch_diag(torch_ones(dim(A)[2]))
  AtA <- A$t()$matmul(A) + id
  Atb <- A$t()$matmul(b)
  L <- linalg_cholesky(AtA)
  y <- torch_triangular_solve(
    Atb$unsqueeze(2),
    L,
    upper = FALSE
  )[[1]]
  x <- torch_triangular_solve(y, L$t())[[1]]
  x
}

# normal equations and LU factorization (done manually)
# A_t A x = A_t b
# P L U x = A_t b
# L y = P_t A_t b          # where y = U x
# U x = y
ls_lu_diy <- function(A, b) {
  AtA <- A$t()$matmul(A)
  Atb <- A$t()$matmul(b)
  lu <- torch_lu(AtA)
  c(P, L, U) %<-% torch_lu_unpack(lu[[1]], lu[[2]]) 
  Atb <- P$t()$matmul(Atb)
  y <- torch_triangular_solve(
    Atb$unsqueeze(2),
    L,
    upper = FALSE
  )[[1]]
  x <- torch_triangular_solve(y, U)[[1]]
  x
}

# QR factorization
# A x = b
# Q R x = b
# R x = Q_t b 
ls_qr <- function(A, b) {
  c(Q, R) %<-% linalg_qr(A)
  Qtb <- Q$t()$matmul(b)
  x <- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]
  x
}

# SVD
# A x = b
# U S V_ x = b
# S V_t x = U_t b
# S y = U_t b 
# V_t x = y
ls_svd <- function(A, b) {
  c(U, S, Vt) %<-% linalg_svd(A, full_matrices = FALSE)
  Utb <- U$t()$matmul(b)
  y <- Utb / S
  x <- Vt$t()$matmul(y)
  x
}
\end{verbatim}

Let's see, then!

\begin{verbatim}
algorithms <- c(
  "ls_normal_eq",
  "ls_cholesky_diy",
  "ls_lu_diy",
  "ls_qr",
  "ls_svd"
)

rmses <- purrr::map(
  algorithms,
  function(m) {
    rmse(
      as.numeric(b),
      as.numeric(A$matmul(get(m)(A, b)))
    )
  }
)

rmse_df <- data.frame(
  method = algorithms,
  rmse = unlist(rmses)
)

rmse_df
\end{verbatim}

\begin{verbatim}
           method         rmse
1    ls_normal_eq 2.882399e-03
2 ls_cholesky_diy 1.373906e-06
3       ls_lu_diy 1.274305e-07
4           ls_qr 3.436749e-08
5          ls_svd 3.436749e-08
\end{verbatim}

This is pretty impressive! We clearly see how the normal equations,
straightforward though they are, may not be the best option once
problems cease to be well-conditioned. Cholesky as well as LU
decomposition fare better; however, the clear ``winners'' are QR
factorization and the SVD. No wonder those two (with two variants each)
are the ones made use of by \texttt{linalg\_lstsq()}.

\hypertarget{sec:matrix-computations-2}{%
\chapter{Matrix computations:
Convolution}\label{sec:matrix-computations-2}}

In deep learning, we talk about convolutions, convolutional layers, and
convolutional neural networks. However, as explained in the chapter on
image processing, the thing we're referring to when doing so really is
something different: cross-correlation.

Formally, the difference is minor: A sign is flipped. Semantically,
these are not the same at all. As we saw, cross-correlation lets us spot
similarities: It serves as a \emph{feature detector}. Convolution is
harder to characterize in an abstract way. Whole books could be written
on the eminent role it plays in signal processing, as well as on its
mathematical significance. Here, we have to leave aside the deeper
underpinnings. Instead, we hope to gain some insight into its operation
- firstly, by thinking through and picturing the steps involved, and
secondly, by implementing it in code. As in the previous chapter, the
focus is on understanding, and creating a basis for further
explorations, should you be so inclined.

\hypertarget{why-convolution}{%
\section{\texorpdfstring{Why
convolution?\index{convolution (signal processing)}}{Why convolution?}}\label{why-convolution}}

In signal processing, \emph{filters} are used to modify a signal in some
desired way -- for example, to cut off the high frequencies. Imagine you
have the Fourier-transformed representation of a time series; meaning, a
set of frequencies with associated magnitudes and phases. You'd like to
set all frequencies higher than some threshold to zero. The easiest way
is to multiply the set of frequencies by a sequence of ones and zeroes.
If you do that, filtering is happening in the frequency domain, and
often, that's by far the most convenient way.

What, though, if the same result should be achieved in the time domain
-- that is, working with the raw time series? In that case, you'd have
to find the time-domain representation of the filter (achieved by the
\emph{Inverse Fourier Transform}). This representation would then have
to be \emph{convolved} with the time series. Put differently,
convolution in the time domain corresponds to multiplication in the
frequency domain. This basic fact gets made use of all the time.

Now, let's try to understand better what convolution does, and how it is
implemented. We begin with a single dimension, and then, explore a bit
of what happens in the two-dimensional case.

\hypertarget{convolution-in-one-dimension}{%
\section{Convolution in one
dimension}\label{convolution-in-one-dimension}}

We start by creating a simple signal, \texttt{x}, and a simple filter,
\texttt{h}. That choice of variable names is not a whim; in signal
processing, \(h\) is the usual symbol denoting the \emph{impulse
response}, a term we'll get to very soon.

\begin{verbatim}
library(torch)

x <- torch_arange(start = 1, end = 4) 
h <- torch_tensor(c(-1, 0, 1))
\end{verbatim}

Now -- given that we \emph{do} have \texttt{torch\_conv1d()} available
-- why don't we call it and see what happens? The way convolution is
defined, output length equals input length plus filter length, minus
one. Using \texttt{torch\_conv1d()}, to obtain length-six output, given
a filter of length three, we need to pad it by two on both sides.

In the following code, don't let the calls to \texttt{view()} distract
you -- they're present only due to \texttt{torch} expecting
three-dimensional input, with dimensions one and two relating to batch
item and channel, as usual.

\begin{verbatim}
torch_conv1d(
  x$view(c(1, 1, 4)),
  h$view(c(1, 1, 3)),
  padding = 2
)
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,.,.) = 
  1  2  2  2 -3 -4
[ CPUFloatType{1,1,6} ]
\end{verbatim}

But wait, you'll be thinking -- didn't we say that what
\texttt{torch\_conv1d()} computes is cross-correlation, not convolution?
Well, R has \texttt{convolve()} -- let's double-check:\footnote{The
  argument \texttt{type\ =\ "open"} is passed to request linear, not
  circular, convolution.}

\begin{verbatim}
x_ <- as.numeric(x)
h_ <- as.numeric(h)

convolve(x_, h_, type = "open")
\end{verbatim}

\begin{verbatim}
[1]  1  2  2  2 -3 -4
\end{verbatim}

The result is the same. However, looking into the documentation for
\texttt{convolve()}, we see:

\begin{quote}
Note that the usual definition of convolution of two sequences
\texttt{x} and \texttt{y} is given by
\texttt{convolve(x,\ rev(y),\ type\ =\ "o")}.
\end{quote}

Evidently, we need to reverse the order of items in the filter:

\begin{verbatim}
convolve(x_, rev(h_), type = "open")
\end{verbatim}

\begin{verbatim}
[1] -1 -2 -2 -2  3  4
\end{verbatim}

Indeed, the result is different now. Let's do the same with
\texttt{torch\_conv1d()}:

\begin{verbatim}
torch_conv1d(
  x$view(c(1, 1, 4)),
  h$flip(1)$view(c(1, 1, 3)),
  padding = 2
)
\end{verbatim}

\begin{verbatim}
torch_tensor
(1,.,.) = 
 -1 -2 -2 -2  3  4
[ CPUFloatType{1,1,6} ]
\end{verbatim}

Again, the outcome is the same between \texttt{torch} and R. So: That
laconic phrase, found in the ``Details'' section of
\texttt{convolve()}'s documentation, captures the complete difference
between cross-correlation and convolution: In convolution, the second
argument is reversed. Or \emph{flipped}, in signal-processing speak.
(``Flipped'', indeed, happens to be a far better term, since it
generalizes to higher dimensions.)

Technically, the difference is tiny -- just a change in sign. But
mathematically, it is essential -- in the sense that it directly derives
from what a filter \emph{is}, and what it \emph{does}. We'll be able to
get some insight into this soon.

The operation underlying convolution can be pictured in two ways.

\hypertarget{two-ways-to-think-about-convolution}{%
\subsection{\texorpdfstring{Two ways to think about
convolution\index{convolution!two ways to think about}}{Two ways to think about convolution}}\label{two-ways-to-think-about-convolution}}

For one, we can look at a single output value and determine how it comes
about. That is, we ask which input elements contribute to its value, and
how those are being combined. This may be called the ``output view'',
and it's one we're already familiar with from cross-correlation.

As to cross-correlation, we described it like this. A filter ``slides''
over an image, and at each image location (pixel), we sum up the
products of surrounding input pixels with the corresponding
``overlayed'' filter values. Put differently, each output pixel results
from computing the \emph{dot product} between matched input and filter
values.

The second way of looking at things is from the point of view of the
input (named, accordingly, the ``input view''). It asks: In what way
does each input value contribute to the output? This view takes some
more getting-used-to than the first; but maybe that's just a matter of
socialization -- the manner in which the topic is usually presented in a
neural-networks context. In any case, the input view is highly
instructive, in that it allows us to learn about the mathematical
\emph{meaning} of convolution.

We're going to look at both, starting with more familiar one, the output
view.

\hypertarget{output-view}{%
\subsubsection{Output view}\label{output-view}}

In the output view, we start by padding the input signal on both sides,
just like we did when calling \texttt{torch\_conv2d()} with
\texttt{padding\ =\ 2}. As required, we flip the impulse response,
turning it into \texttt{1,\ 0,\ -1}. Then, we picture the ``sliding''.

Below, you find this visualized in tabular form
(tbl.~\ref{tbl-convolution-output}). The bottom row holds the result,
obtained from summing up the individual products at each position.

\hypertarget{tbl-convolution-output}{}
\begin{longtable}[]{@{}rrrrrrr@{}}
\caption{\label{tbl-convolution-output}Convolution: Output
view.}\tabularnewline
\toprule\noalign{}
Signal & Flipped IR & & & & & \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Signal & Flipped IR & & & & & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{0} & \texttt{1} & & & & & \\
\texttt{0} & \texttt{0} & \texttt{1} & & & & \\
\texttt{1} & \texttt{-1} & \texttt{0} & \texttt{1} & & & \\
\texttt{2} & & \texttt{-1} & \texttt{0} & \texttt{1} & & \\
\texttt{3} & & & \texttt{-1} & \texttt{0} & \texttt{1} & \\
\texttt{4} & & & & \texttt{-1} & \texttt{0} & \texttt{1} \\
\texttt{0} & & & & & \texttt{-1} & \texttt{0} \\
\texttt{0} & & & & & & \texttt{-1} \\
\textbf{Result} & \texttt{-1} & \texttt{-2} & \texttt{-2} & \texttt{-2}
& \texttt{3} & \texttt{4} \\
\end{longtable}

After all we've said on the topic, this depiction should offer few
surprises. On to the input view.

\hypertarget{input-view}{%
\subsubsection{Input view}\label{input-view}}

The essential thing about the input view is the way we conceptualize the
input signal: Each individual element is seen as a -- \emph{scaled} and
\emph{shifted -- impulse}.

The \emph{impulse} is given by the \emph{unit sample} (or: impulse)
function, delta (\(\delta\)). This function is zero everywhere, except
at zero, where its value is one:

\[
\delta [n]={\begin{cases}1\ \ \ if \ n=0\\0\ \ \ if \ n \ne 0\end{cases}}
\]

This is like a Kronecker delta, \(\delta_{ij}\)\footnote{The Kronecker
  delta, \(\delta_{ij}\), evaluates to one if \(i\) equals \(j\), and to
  zero, otherwise.}, with one of the indices being fixed at 0:

\[
\delta [n]= \delta _{n0}= \delta _{0n}
\]

Thus, equipped with only that function, \(\delta[n]\) -- with \(n\)
representing discrete time, say -- we can represent exactly one signal
value, the one at time \(n = 0\)\footnote{I'm using \(n\), instead of
  \(t\), to index into different positions, because the signal -- like
  any digitized one -- only ``exists'' at discrete points in time (or
  space). In some contexts, this reads a bit awkward, but it at least is
  consistent.}, and its only possible value is \texttt{1}. Now we add to
this the operations \emph{scale} and \emph{shift}.

\begin{itemize}
\item
  By scaling, we can produce any value at \(n = 0\); for example:
  \(x_0 = 0 * \delta [n]\).
\item
  By shifting, we can affect values at other points in time. For
  example, time \(n = 3\) can be addressed as
  \(\delta [n - 3]= \delta _{n3}\), since \(n - 3 = 0\).
\item
  Combining both, we can represent any value at any point in time. For
  example: \(x_5 = 1.11 * \delta [n - 5]\).
\end{itemize}

So far, we've talked just about the signal. What about the filter? Just
like the impulse is essential in characterizing a signal, a filter is
completely described by its \emph{impulse response}\footnote{Like
  everywhere in the chapter, when I talk of filters, I think of linear
  time-invariant systems only. The restriction to time-invariant systems
  is immanent in the convolution operation.}. The impulse response, by
definition, is what comes out when the input is an impulse (that is,
happens at time \(n = 0\)). In notation analogous to that used for the
signal, with \(h\) denoting the impulse response, we have:

\[
h[n] = h[n- 0] \equiv h(\delta[n- 0]) 
\]

In our example, that would be the sequence \texttt{-1,\ 0,\ 1}. But just
like the signal needs to be represented at additional times, not just at
\(0\), the filter has to be applicable to other positions, as well. To
that purpose, again, a shift operation is employed, and it is formalized
in an analogous way: For instance, \(h[n - 1]\) means the filter is
applied to time \(1\), the time when \(n - 1\) equals zero. These shifts
correspond to what we informally refer to as ``sliding''.

Now, all that remains to be done is combine the pieces. At time
\(n = 0\), we take the un-shifted impulse response, and \emph{scale} it
by the amplitude of the signal. In our example, that value was \(1\).
Thus: \(1 * h[n - 0] = 1 * [-1, 0, 1] = [-1, 0, 1]\). For the other
times, we shift the impulse response to the input position in question,
and multiply. Finally, once we've obtained all contributions from all
input positions, we add them up, thus obtaining the convolved output.

The following table aims to illustrate that
(tbl.~\ref{tbl-convolution-input}):

\hypertarget{tbl-convolution-input}{}
\begin{longtable}[]{@{}rrr@{}}
\caption{\label{tbl-convolution-input}Convolution: Input
view.}\tabularnewline
\toprule\noalign{}
Signal & Impulse response & Product \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Signal & Impulse response & Product \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{1} & \texttt{h{[}n\ -\ 0{]}} &
\texttt{-1\ \ 0\ \ 1\ \ 0\ \ 0\ \ 0} \\
\texttt{2} & \texttt{h{[}n\ -\ 1{]}} &
\texttt{0\ -2\ \ 0\ \ 2\ \ 0\ \ 0} \\
\texttt{3} & \texttt{h{[}n\ -\ 2{]}} &
\texttt{0\ \ 0\ -3\ \ 0\ \ 3\ \ 0} \\
\texttt{4} & \texttt{h{[}n\ -\ 3{]}} &
\texttt{0\ \ 0\ \ 0\ -4\ \ 0\ \ 4} \\
\textbf{Sum} & & \texttt{-1\ -2\ -2\ -2\ \ 3\ \ 4} \\
\end{longtable}

Personally, while I do find the output view easier to grasp, I feel I
can derive more insight from the input view. In particular, it answers
the -- unavoidable -- question: So \emph{why} do we flip the impulse
response?

It turns out that, far from being due to whatever mysterious forces, the
minus sign is merely a mechanical outcome of the \emph{way signals are
represented}: The signal measured at time \(n = 2\) is denoted by
\(\delta [n - 2]\) (two minus two yielding zero); and the filter applied
to that signal, accordingly, as \(h[n -2]\).

\hypertarget{implementation-1}{%
\subsection{Implementation}\label{implementation-1}}

From the way I've described the output view, you may well think there's
not much to say about how to code this. It looks straightforward: Loop
over the input vector, and compute the dot product at every prospective
output position. But that would mean calculating many vector products,
the more, the longer the input sequence.

Fortunately, there is a better way. Single-dimension (linear)
convolution is computed by means of Toeplitz matrices, matrices that
have some number of constant diagonals, and values of zero everywhere
else. Once the filter has been formulated as a Toeplitz matrix, there is
just a single multiplication to be carried out: that of the Toeplitz
matrix and the input. And even though the matrix will need to have as
many columns as the input has values (otherwise we couldn't do the
multiplication), computational cost is small due to the matrix's being
``nearly empty''.

Here is such a Toeplitz matrix\index{convolution!Toeplitz matrix},
constructed for our running example:

\begin{verbatim}
h <-torch_tensor(
  rbind(c(-1, 0, 0, 0),
        c(0, -1, 0, 0),
        c(1, 0, -1, 0),
        c(0, 1, 0, -1),
        c(0, 0, 1, 0),
        c(0, 0, 0, 1)
        ))
h
\end{verbatim}

\begin{verbatim}
torch_tensor
-1  0  0  0
 0 -1  0  0
 1  0 -1  0
 0  1  0 -1
 0  0  1  0
 0  0  0  1
[ CPUFloatType{6,4} ]
\end{verbatim}

Let's check that multiplication with our example input yields the
expected result:

\begin{verbatim}
h$matmul(x)
\end{verbatim}

\begin{verbatim}
torch_tensor
-1
-2
-2
-2
 3
 4
[ CPUFloatType{6} ]
\end{verbatim}

It does. Now, let's move on to two dimensions. Conceptually, there is no
difference, but actual computation (both ``by hand'' and using matrices)
gets a lot more involved. Thus, we'll content ourselves with presenting
a (generalizeable) part of the manual calculation, and, in the
computational part, don't aim at elucidating every single detail.

\hypertarget{convolution-in-two-dimensions}{%
\section{Convolution in two
dimensions}\label{convolution-in-two-dimensions}}

To show how, conceptually, one-dimensional and two-dimensional
convolution are analogous, we assume the output view.

\hypertarget{how-it-works-output-view}{%
\subsection{How it works (output view)}\label{how-it-works-output-view}}

This time, the example input is two-dimensional. It could look like
this:

\[
\begin{bmatrix}
  1 & 4 & 1\\
  2 & 5 & 3\\
\end{bmatrix}
\]

The same goes for the filter. Here is a possible one:

\[
\begin{bmatrix}
  1 & 1\\
  1 & -1\\
\end{bmatrix}
\]

We take the output view, the one where the filter ``slides'' over the
input. But, to keep things readable, let me just pick a single output
value (``pixel'') for demonstration. If the input is of size
\texttt{m1\ x\ n1}, and the filter, \texttt{m2\ x\ n2}, the output will
have size \texttt{(m1\ +\ m2\ -\ 1)\ x\ (n1\ +\ n2\ -\ 1)}; thus, it
will be \texttt{3\ x\ 4} in our case. I'll pick the value at position
\texttt{(0,\ 1)} -- counting rows from the bottom, as is usual in image
processing:

\[
\begin{bmatrix}
  . & . & . & .\\
  . & . & . & .\\
  . & y_{01} & . & .\\
\end{bmatrix}
\]

Here is the input, displayed in a table that will allow us to picture
elements at non-existing (negative) positions.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Position (x/y) & -1 & 0 & 1 & 2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & & 1 & 4 & 1 \\
\textbf{0} & & 2 & 5 & 3 \\
\textbf{-1} & & & & \\
\end{longtable}

And here, the filter, with values arranged correspondingly:

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Position (x/y) & -1 & 0 & 1 & 2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & & 1 & 1 & \\
\textbf{0} & & 1 & -1 & \\
\textbf{-1} & & & & \\
\end{longtable}

As in the one-dimensional case, the first thing to be done is flip the
filter. Flipping here means rotation by hundred-eighty degrees.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Position (x/y) & -1 & 0 & 1 & 2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & & & & \\
\textbf{0} & -1 & 1 & & \\
\textbf{-1} & 1 & 1 & & \\
\end{longtable}

Next, the filter is shifted to the desired output position. What we want
to do is shift to the right by one, leaving unaffected vertical
position.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Position (x/y) & -1 & 0 & 1 & 2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & & & & \\
\textbf{0} & & -1 & 1 & \\
\textbf{-1} & & 1 & 1 & \\
\end{longtable}

Now we are all set to compute the output value at position
\texttt{(0,\ 1)}. It's the dot product of all overlapping image and
filter values:

\begin{longtable}[]{@{}ccccc@{}}
\toprule\noalign{}
Position (x/y) & -1 & 0 & 1 & 2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & & & & \\
\textbf{0} & & -1*2=-2 & 1*5=5 & \\
\textbf{-1} & & & & \\
\end{longtable}

The final result, then, is \texttt{-2\ +\ 5\ =\ 3}.

\[
\begin{bmatrix}
  . & . & . & .\\
  . & . & . & .\\
  . & 3 & . & .\\
\end{bmatrix}
\]

All values still missing can be computed in an analogous way. But we'll
skip that exercise, and take a look at how an actual computation would
proceed.

\hypertarget{implementation-2}{%
\subsection{Implementation}\label{implementation-2}}

The way two-dimensional convolution is actually implemented in code
again involves Toeplitz matrices. Like I already said, we won't go into
why exactly every step takes the \emph{exact form} it takes -- the
intent here is to show a working example, an example you could build on,
if you wanted, for your own explorations.

\hypertarget{step-one-prepare-filter-matrix}{%
\subsubsection{Step one: Prepare filter
matrix}\label{step-one-prepare-filter-matrix}}

We start by padding the filter to the output size, \texttt{3\ x\ 4}.

\begin{verbatim}
0  0 0 0
1  1 0 0
1 -1 0 0
\end{verbatim}

We then create a Toeplitz matrix for every row in the filter, starting
at the bottom.

\begin{verbatim}
# H0
 1  0  0  
-1  1  0  
 0 -1  1  
 0  0 -1  
 
# H1
 1  0  0  
 1  1  0  
 0  1  1  
 0  0  1  
 
# H2
 0  0  0  
 0  0  0  
 0  0  0  
\end{verbatim}

In code, we have:

\begin{verbatim}
H0 <- torch_tensor(
  cbind(
    c(1, -1, 0, 0),
    c(0, 1, -1, 0),
    c(0, 0, 1, -1)
  )
)

H1 <- torch_tensor(
  cbind(
    c(1, 1, 0, 0),
    c(0, 1, 1, 0),
    c(0, 0, 1, 1)
  )
)

H2 <- torch_tensor(0)$unsqueeze(1)
\end{verbatim}

Next, these three matrices are assembled so as to form a
\emph{doubly-blocked Toeplitz} \emph{matrix}. Like so:

\begin{verbatim}
H0   0
H1  H0
H2  H1
\end{verbatim}

One way of coding this is to (twice) use \texttt{torch\_block\_diag()}
to build up the two non-zero blocks, and concatenate them:

\begin{verbatim}
H <- torch_cat(
  list(
    torch_block_diag(list(H0, H0)), torch_zeros(4, 6)
  )
) +
  torch_cat(
    list(
      torch_zeros(4, 6),
      torch_block_diag(list(H1, H1))
    )
  )

H
\end{verbatim}

\begin{verbatim}
torch_tensor
 1  0  0  0  0  0
-1  1  0  0  0  0
 0 -1  1  0  0  0
 0  0 -1  0  0  0
 1  0  0  1  0  0
 1  1  0 -1  1  0
 0  1  1  0 -1  1
 0  0  1  0  0 -1
 0  0  0  1  0  0
 0  0  0  1  1  0
 0  0  0  0  1  1
 0  0  0  0  0  1
[ CPUFloatType{12,6} ]
\end{verbatim}

The final matrix has two non-zero ``bands'', separated by two all-zero
diagonals. This is the final form of the filter needed for matrix
multiplication.

\hypertarget{step-two-prepare-input}{%
\subsubsection{Step two: Prepare input}\label{step-two-prepare-input}}

To be multiplicable with this \texttt{12\ x\ 6} matrix, the input needs
to be flattened into a vector. Again, we proceed row-by-row, starting
from the bottom here as well.

\begin{verbatim}
x0 <- torch_tensor(c(2, 5, 3)) 
x1 <- torch_tensor(c(1, 4, 1))

x <- torch_cat(list(x0, x1))
x
\end{verbatim}

\begin{verbatim}
torch_tensor
 2
 5
 3
 1
 4
 1
[ CPUFloatType{6} ]
\end{verbatim}

\hypertarget{step-three-multiply}{%
\subsubsection{Step three: Multiply}\label{step-three-multiply}}

By now, convolution has morphed into straightforward matrix
multiplication:

\begin{verbatim}
y <- H$matmul(x)
y
\end{verbatim}

\begin{verbatim}
torch_tensor
  2
  3
 -2
 -3
  3
 10
  5
  2
  1
  5
  5
  1
[ CPUFloatType{12} ]
\end{verbatim}

All that remains to be done is reshape the output into the correct
two-dimensional structure. Building up the rows in order (again,
bottom-first) we obtain:

\[
\begin{bmatrix}
  1 & 5 & 5 & 1\\
  3 & 10 & 5 & 2\\
  2 & 3 & -2 & -3\\
\end{bmatrix}
\]

Looking at element \texttt{(0,\ 1)}, we see that the computation
confirms our manual calculation.

Herewith, we conclude the topic of matrix computations with
\texttt{torch}. But, as we move on to our next topic, the Fourier
transform, we won't actually stray that far away. Remember how, above,
we said that time-domain convolution corresponds to frequency-domain
multiplication?

This correspondence is often used to speed up computation: The input
data are Fourier-transformed, the result is multiplied by the filter,
and the filtered frequency-domain representation is transformed back
again. Just have a look at the documentation for R's
\texttt{convolve()}. It directly starts out stating:

\begin{quote}
Use the Fast Fourier Transform to compute the several kinds of
convolutions of two sequences.
\end{quote}

On to the Fourier Transform, then!

\hypertarget{sec:signal-processing-1}{%
\chapter{Exploring the Discrete Fourier Transform
(DFT)}\label{sec:signal-processing-1}}

This is the first chapter dedicated to the Fourier Transform, but it's
not the first time we're encountering it. In the convolution chapter I
mentioned, in passing, that the desired outcome is often more
economically achieved in the Fourier domain, with convolution turning
into multiplication. What's more, we actually \emph{made} \emph{use} of
the DFT in the chapter on audio classification. There, a call to
\texttt{torchaudio}'s \texttt{transform\_spectrogram()} triggered the
computation of a whole bunch of Fourier Transforms, on a set of
overlapping time windows constructed from the original input sequence.

In this chapter, we won't content ourselves with purely calling the
\texttt{torch}-provided functions. Instead, we'd like to get a feeling
for what's actually going on. To that purpose, we explore the ideas
underlying Fourier analysis, and -- in a straightforward, literal way --
translate them into \texttt{torch}. In the subsequent chapter, building
on the understanding we'll have gained, we will then improve on
performance by implementing one of the algorithms subsumed under the
family of \emph{Fast Fourier Transforms} (FFTs).

\hypertarget{understanding-the-output-of-torch_fft_fft}{%
\section{\texorpdfstring{Understanding the output of
\texttt{torch\_fft\_fft()}\index{\texttt{torch{\textunderscore}fft{\textunderscore}fft()}}}{Understanding the output of torch\_fft\_fft()}}\label{understanding-the-output-of-torch_fft_fft}}

Our explorations take off hands-on: We call the main function associated
with the (forward) Fourier Transform, \texttt{torch\_fft\_fft()}
\footnote{Its counterpart -- that takes the Fourier representation and
  yields a time-domain signal -- is called
  \texttt{torch\_fft\_ifft()}\index{\texttt{torch{\textunderscore}fft{\textunderscore}ifft()}}
  (with \texttt{ifft} standing for \emph{Inverse Fourier Transform}).},
and see if we can make sense of its output.

As we care about actual \emph{understanding}, we start from the simplest
possible example signal, a pure cosine that performs one revolution over
the complete sampling period.

\hypertarget{starting-point-a-cosine-of-frequency-1}{%
\subsection{Starting point: A cosine of frequency
1}\label{starting-point-a-cosine-of-frequency-1}}

The way we set things up, there will be sixty-four samples; the sampling
period thus equals \texttt{N\ =\ 64}. The content of
\texttt{frequency()}, the below helper function used to construct the
signal, reflects how we represent the cosine. Namely:

\[
f(x) = cos(\frac{2 \pi}{N} \ k \ x)
\]

Here \(x\) values progress over time (or space), and \(k\) is the
\emph{frequency index.} A cosine is periodic with period \(2 \pi\); so
if we want it to first return to its starting state after sixty-four
samples, and \(x\) runs between zero and sixty-three, we'll want \(k\)
to be equal to \(1\). Like that, we'll reach the initial state again at
position \(x = \frac{2 \pi}{64} * 1 * 64\).

\begin{verbatim}
library(torch)
library(ggplot2)
library(patchwork)

N <- 64
sample_positions <- 0:(N - 1)

frequency <- function(k, N) {
  (2 * pi / N) * k
}

x <- torch_cos(frequency(1, N) * sample_positions)
\end{verbatim}

Let's quickly confirm this did what it was supposed to
(fig.~\ref{fig-dft-cos-1-rev}):

\begin{verbatim}
df <- data.frame(x = sample_positions, y = as.numeric(x))

ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-1-rev.png}

}

\caption{\label{fig-dft-cos-1-rev}Pure cosine that accomplishes one
revolution over the complete sample period (64 samples).}

\end{figure}

Now that we have the input signal, \texttt{torch\_fft\_fft()} computes
for us the Fourier coefficients, that is, the importance of the various
frequencies present in the signal. The number of frequencies considered
will equal the number of sampling points: So \(X\) will be of length
sixty-four as well.

(In our example, you'll notice that the second half of coefficients will
equal the first in magnitude.\footnote{Expanding on this a bit: For
  real-valued signals, the magnitudes as well as the real parts of
  corresponding coefficients are equal, while the phases and the
  imaginary parts are conjugated. In other words, the coefficients are
  complex conjugates of each other. We'll see this in later examples.}
This is the case for every real-valued signal. In such cases, you could
call \texttt{torch\_fft\_rfft()} instead, which yields ``nicer'' (in the
sense of shorter) vectors to work with. Here though, I want to explain
the general case, since that's what you'll find done in most expositions
on the topic.)

\begin{verbatim}
Ft <- torch_fft_fft(x)
\end{verbatim}

Even with the signal being real, the Fourier coefficients are complex
numbers. There are four ways to inspect them. The first is to extract
the real part:

\begin{verbatim}
real_part <- Ft$real
as.numeric(real_part) %>% round(5)
\end{verbatim}

\begin{verbatim}
[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
[57] 0 0 0 0 0 0 0 32
\end{verbatim}

Only a single coefficient is non-zero, the one at position 1. (We start
counting from zero, and may discard the second half, as explained
above.)

Now looking at the imaginary part, we find it is zero throughout:

\begin{verbatim}
imag_part <- Ft$imag
as.numeric(imag_part) %>% round(5)
\end{verbatim}

\begin{verbatim}
[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[57] 0 0 0 0 0 0 0 0
\end{verbatim}

At this point we know that there is just a single frequency present in
the signal, namely, that at \(k = 1\). This matches (and it better had
to) the way we constructed the signal: namely, as accomplishing a single
revolution over the complete sampling period.

Since, in theory, every coefficient could have non-zero real and
imaginary parts, often what you'd report is the magnitude (the square
root of the sum of squared real and imaginary parts):

\begin{verbatim}
magnitude <- torch_abs(Ft)
as.numeric(magnitude) %>% round(5)
\end{verbatim}

\begin{verbatim}
[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
[57] 0 0 0 0 0 0 0 32
\end{verbatim}

Unsurprisingly, these values exactly reflect the respective real parts.

Finally, there's the phase, indicating a possible shift of the signal (a
pure cosine is unshifted). In \texttt{torch}, we have
\texttt{torch\_angle()} complementing \texttt{torch\_abs()}, but we need
to take into account roundoff error here. We know that in each but a
single case, the real and imaginary parts are both exactly zero; but due
to finite precision in how numbers are presented in a computer, the
actual values will often not be zero. Instead, they'll be very small. If
we take one of these ``fake non-zeroes'' and divide it by another, as
happens in the angle calculation, big values can result. To prevent this
from happening, our custom implementation rounds both inputs before
triggering the division.

\begin{verbatim}
phase <- function(Ft, threshold = 1e5) {
  torch_atan2(
    torch_abs(torch_round(Ft$imag * threshold)),
    torch_abs(torch_round(Ft$real * threshold))
  )
}

as.numeric(phase(Ft)) %>% round(5)
\end{verbatim}

\begin{verbatim}
[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[57] 0 0 0 0 0 0 0 0
\end{verbatim}

As expected, there is no phase shift in the signal.

Let's visualize what we found (fig.~\ref{fig-dft-cos-1-rev-dft}).

\begin{verbatim}
create_plot <- function(x, y, quantity) {
  df <- data.frame(
    x_ = x,
    y_ = as.numeric(y) %>% round(5)
  )
  ggplot(df, aes(x = x_, y = y_)) +
    geom_col() +
    xlab("frequency") +
    ylab(quantity) +
    theme_minimal()
}

p_real <- create_plot(
  sample_positions,
  real_part,
  "real part"
)
p_imag <- create_plot(
  sample_positions,
  imag_part,
  "imaginary part"
)
p_magnitude <- create_plot(
  sample_positions,
  magnitude,
  "magnitude"
)
p_phase <- create_plot(
  sample_positions,
  phase(Ft),
  "phase"
)

p_real + p_imag + p_magnitude + p_phase
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-1-rev-dft.png}

}

\caption{\label{fig-dft-cos-1-rev-dft}Real parts, imaginary parts,
magnitudes and phases of the Fourier coefficients, obtained on a pure
cosine that performs a single revolution over the sampling period.
Imaginary parts as well as phases are all zero.}

\end{figure}

It's fair to say that we have no reason to doubt what
\texttt{torch\_fft\_fft()} has done. But with a pure sinusoid like this,
we can understand exactly what's going on by computing the DFT
ourselves, by hand. Doing this now will significantly help us later,
when we're writing the code.

\hypertarget{reconstructing-the-magic}{%
\subsection{Reconstructing the magic}\label{reconstructing-the-magic}}

One caveat about this section. With a topic as rich as the Fourier
Transform, and an audience who I imagine to vary widely on a dimension
of math and sciences education, my chances to meet \emph{your}
expectations, dear reader, must be very close to zero. Still, I want to
take the risk. If you're an expert on these things, you'll anyway be
just scanning the text, looking out for pieces of \texttt{torch} code.
If you're moderately familiar with the DFT, you may still like being
reminded of its inner workings. And -- most importantly -- if you're
rather new, or even completely new, to this topic, you'll hopefully take
away (at least) one thing: that what seems like one of the greatest
wonders of the universe (assuming there is a reality somehow
corresponding to what goes on in our minds) may well be a \emph{wonder},
but neither ``magic'' nor a thing reserved to the initiated.

In a nutshell, the Fourier Transform is a \emph{basis
transformation}\index{Fourier Transform!as a basis transformation}. In
the case of the DFT -- the Discrete Fourier Transform, where time and
frequency representations both are finite vectors, not functions -- the
new basis looks like this:

\[
\begin{aligned}
&\mathbf{w}^{0n}_N = e^{i\frac{2 \pi}{N}* 0 * n} = 1\\
&\mathbf{w}^{1n}_N = e^{i\frac{2 \pi}{N}* 1 * n} = e^{i\frac{2 \pi}{N} n}\\
&\mathbf{w}^{2n}_N = e^{i\frac{2 \pi}{N}* 2 * n} = e^{i\frac{2 \pi}{N}2n}\\& ... \\
&\mathbf{w}^{(N-1)n}_N = e^{i\frac{2 \pi}{N}* (N-1) * n} = e^{i\frac{2 \pi}{N}(N-1)n}\\
\end{aligned}
\]

Here \(N\), as before, is the number of samples (64, in our case); thus,
there are \(N\) basis vectors. With \(k\) running through the basis
vectors, they can be written:

\begin{equation}\protect\hypertarget{eq-dft-1}{}{
\mathbf{w}^{kn}_N = e^{i\frac{2 \pi}{N}k n}
}\label{eq-dft-1}\end{equation}

Like \(k\), \(n\) runs from \(0\) to \(N-1\). To understand what these
basis vectors are doing, it is helpful to temporarily switch to a
shorter sampling period, \(N = 4\), say. If we do so, we have four basis
vectors: \(\mathbf{w}^{0n}_N\), \(\mathbf{w}^{1n}_N\),
\(\mathbf{w}^{2n}_N\), and \(\mathbf{w}^{3n}_N\). The first one looks
like this:

\[
\mathbf{w}^{0n}_N
=
\begin{bmatrix}
   e^{i\frac{2 \pi}{4}* 0 * 0}\\
   e^{i\frac{2 \pi}{4}* 0 * 1}\\
   e^{i\frac{2 \pi}{4}* 0 * 2}\\
   e^{i\frac{2 \pi}{4}* 0 * 3}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   1\\
   1\\
   1\\
\end{bmatrix}
\]

The second, like so:

\[
\mathbf{w}^{1n}_N
=
\begin{bmatrix}
   e^{i\frac{2 \pi}{4}* 1 * 0}\\
   e^{i\frac{2 \pi}{4}* 1 * 1}\\
   e^{i\frac{2 \pi}{4}* 1 * 2}\\
   e^{i\frac{2 \pi}{4}* 1 * 3}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   e^{i\frac{\pi}{2}}\\
   e^{i \pi}\\
   e^{i\frac{3 \pi}{4}}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   i\\
   -1\\
   -i\\
\end{bmatrix}
\]

This is the third:

\[
\mathbf{w}^{2n}_N
=
\begin{bmatrix}
   e^{i\frac{2 \pi}{4}* 2 * 0}\\
   e^{i\frac{2 \pi}{4}* 2 * 1}\\
   e^{i\frac{2 \pi}{4}* 2 * 2}\\
   e^{i\frac{2 \pi}{4}* 2 * 3}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   e^{i\pi}\\
   e^{i 2 \pi}\\
   e^{i\frac{3 \pi}{2}}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   -1\\
   1\\
   -1\\
\end{bmatrix}
\]

And finally, the fourth:

\[
\mathbf{w}^{3n}_N
=
\begin{bmatrix}
   e^{i\frac{2 \pi}{4}* 3 * 0}\\
   e^{i\frac{2 \pi}{4}* 3 * 1}\\
   e^{i\frac{2 \pi}{4}* 3 * 2}\\
   e^{i\frac{2 \pi}{4}* 3 * 3}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   e^{i\frac{3 \pi}{2}}\\
   e^{i 3 \pi}\\
   e^{i\frac{9 \pi}{2}}
\end{bmatrix}
=
\begin{bmatrix}
   1\\
   -i\\
   -1\\
   i\\
\end{bmatrix}
\]

We can characterize these four basis vectors in terms of their
``speed'': how fast they move around the unit
circle\index{Fourier Transform!basis vector actions}. To do this, we
simply look at the rightmost column vectors, where the final calculation
results appear. The values in that column correspond to positions
pointed to by the revolving basis vector at different points in time.
This means that looking at a single ``update of position'', we can see
how fast the vector is moving in a single time step.

Looking first at \(\mathbf{w}^{0n}_N\), we see that it does not move at
all. \(\mathbf{w}^{1n}_N\) goes from \(1\) to \(i\) to \(-1\) to \(-i\);
one more step, and it would be back where it started. That's one
revolution in four steps, or a step size of \(\frac{\pi}{2}\). Then
\(\mathbf{w}^{2n}_N\) goes at double that pace, moving a distance of
\(\pi\) along the circle. That way, it ends up completing two
revolutions overall. Finally, \(\mathbf{w}^{3n}_N\) achieves three
complete loops, for a step size of \(\frac{3 \pi}{2}\).

The thing that makes these basis vectors so useful is that they are
mutually orthogonal. That is, their dot product is zero:

\begin{equation}\protect\hypertarget{eq-dft-2}{}{
\langle \mathbf{w}^{kn}_N, \mathbf{w}^{ln}_N \rangle \ = \ \sum_{n=0}^{N-1} ({e^{i\frac{2 \pi}{N}k n}})^* e^{i\frac{2 \pi}{N}l n} = \ \sum_{n=0}^{N-1} ({e^{-i\frac{2 \pi}{N}k n}})e^{i\frac{2 \pi}{N}l n} = 0
}\label{eq-dft-2}\end{equation}

Let's take, for example, \(\mathbf{w}^{2n}_N\) and
\(\mathbf{w}^{3n}_N\). Indeed, their dot product evaluates to zero.

\[
\begin{bmatrix}
   1 & -1 & 1 & -1\\
\end{bmatrix}
\begin{bmatrix}
   1\\
   -i\\
   -1\\
   i\\
\end{bmatrix}
=
1 + i + (-1) + (-i)  = 0
\]

Now, we're about to see how the orthogonality of the Fourier basis
substantially simplifies the calculation of the
DFT\index{Fourier Transform!orthogonality of basis vectors}. Did you
notice the similarity between these basis vectors and the way we wrote
the example signal? Here it is again:

\[
f(x) = cos(\frac{2 \pi}{N} k x)
\]

If we manage to represent this function in terms of the basis vectors
\(\mathbf{w}^{kn}_N = e^{i\frac{2 \pi}{N}k n}\), the inner product
between the function and each basis vector will be either zero (the
``default'') or a multiple of one (in case the function has a component
matching the basis vector in question). Luckily, sines and cosines can
easily be converted into complex exponentials. In our example, this is
how that goes:\footnote{I'll be writing \(\mathbf{x}_n\) instead of
  \(f(x)\) from now on to indicate that we're working with discrete
  samples, not the continuous function itself.}

\[
\begin{aligned}
\mathbf{x}_n &= cos(\frac{2 \pi}{64} n) \\
&= \frac{1}{2} (e^{i\frac{2 \pi}{64} n} + e^{-i\frac{2 \pi}{64} n}) \\
&= \frac{1}{2} (e^{i\frac{2 \pi}{64} n} + e^{i\frac{2 \pi}{64} 63n}) \\
&= \frac{1}{2} (\mathbf{w}^{1n}_N + \mathbf{w}^{63n}_N)
\end{aligned}
\]

Here the first step directly results from Euler's formula\footnote{Euler's
  formula relates complex exponentials to sines and cosines, stating
  that \(e^{i \theta} = cos \theta + i sin \theta\).}, and the second
reflects the fact that the Fourier coefficients are periodic, with
frequency -1 being the same as 63, -2 equaling 62, and so on.

Now, the \(k\)th Fourier coefficient is obtained by projecting the
signal onto basis vector \(k\).

Due to the orthogonality of the basis vectors, only two coefficients
will not be zero: those for \(\mathbf{w}^{1n}_N\) and
\(\mathbf{w}^{63n}_N\). They are obtained by computing the inner product
between the function and the basis vector in question, that is, by
summing over \(n\). For each \(n\) ranging between \(0\) and \(N-1\), we
have a contribution of \(\frac{1}{2}\), leaving us with a final sum of
\(32\) for both coefficients. For example, for \(\mathbf{w}^{1n}_N\):

\[
\begin{aligned}
X_1 &= \langle \mathbf{w}^{1n}_N, \mathbf{x}_n \rangle \\
&= \langle \mathbf{w}^{1n}_N, \frac{1}{2} (\mathbf{w}^{1n}_N + \mathbf{w}^{63n}_N) \rangle \\
&= \frac{1}{2} * 64 \\
&= 32
\end{aligned}
\]

And analogously for \(X_{63}\).

Now, looking back at what \texttt{torch\_fft\_fft()} gave us, we see we
were able to arrive at the same result. And we've learned something
along the way.

As long as we stay with signals composed of one or more basis vectors,
we can compute the DFT in this way. At the end of the chapter, we'll
develop code that will work for all signals, but first, let's see if we
can dive even deeper into the workings of the DFT. Three things we'll
want to explore:

\begin{itemize}
\item
  What would happen if frequencies changed -- say, a melody were sung at
  a higher pitch?
\item
  What about amplitude changes -- say, the music were played twice as
  loud?
\item
  What about phase -- e.g., there were an offset before the piece
  started?
\end{itemize}

In all cases, we'll call \texttt{torch\_fft\_fft()} only once we've
determined the result ourselves.

And finally, we'll see how complex sinusoids, made up of different
components, can still be analyzed in this way, provided they can be
expressed in terms of the frequencies that make up the basis.

\hypertarget{varying-frequency}{%
\subsection{\texorpdfstring{Varying
frequency\index{Fourier Transform!varying frequency}}{Varying frequency}}\label{varying-frequency}}

Assume we quadrupled the frequency, giving us a signal that looked like
this:

\[
\mathbf{x}_n = cos(\frac{2 \pi}{N}*4*n)
\]

Following the same logic as above, we can express it like so:

\[
\mathbf{x}_n = \frac{1}{2} (\mathbf{w}^{4n}_N + \mathbf{w}^{60n}_N)
\]

We already see that non-zero coefficients will be obtained only for
frequency indices \(4\) and \(60\). Picking the former, we obtain

\[
\begin{aligned}
X_4 &= \langle \mathbf{w}^{4n}_N, \mathbf{x}_n \rangle \\
&= \langle \mathbf{w}^{4n}_N, \frac{1}{2} (\mathbf{w}^{4n}_N + \mathbf{w}^{60n}_N) \rangle \\
&= 32
\end{aligned}
\]

For the latter, we'd arrive at the same result.

Now, let's make sure our analysis is correct. The following code snippet
contains nothing new; it generates the signal, calculates the DFT, and
plots them both (fig.~\ref{fig-dft-cos-4-rev}).

\begin{verbatim}
x <- torch_cos(frequency(4, N) * sample_positions)

plot_ft <- function(x) {

  df <- data.frame(x = sample_positions, y = as.numeric(x))
  p_signal <- ggplot(df, aes(x = x, y = y)) +
    geom_line() +
    xlab("time") +
    ylab("amplitude") +
    theme_minimal()

  # in the code, I'm using Ft instead of X because not
  # all operating systems treat variables as case-sensitive
  Ft <- torch_fft_fft(x)

  p_real <- create_plot(
    sample_positions,
    Ft$real,
    "real part"
  )
  p_imag <- create_plot(
    sample_positions,
    Ft$imag,
    "imaginary part"
  )
  p_magnitude <- create_plot(
    sample_positions,
    torch_abs(Ft),
    "magnitude"
  )
  p_phase <- create_plot(
    sample_positions,
    phase(Ft),
    "phase"
  )

  (p_signal | plot_spacer()) /
    (p_real | p_imag) /
    (p_magnitude | p_phase)
}

plot_ft(x)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-4-rev.png}

}

\caption{\label{fig-dft-cos-4-rev}A pure cosine that performs four
revolutions over the sampling period, and its DFT. Imaginary parts and
phases are still are zero.}

\end{figure}

This does indeed confirm our calculations.

A special case arises when signal frequency rises to the highest one
``allowed'', in the sense of being detectable without aliasing. That
will be the case at one half of the number of sampling points. Then, the
signal will look like so:

\[
\mathbf{x}_n = \frac{1}{2} (\mathbf{w}^{32n}_N + \mathbf{w}^{32n}_N)
\]

Consequently, we end up with a single coefficient, corresponding to a
frequency of 32 revolutions per sample period, of double the magnitude
(64, thus). Here are the signal and its DFT
(fig.~\ref{fig-dft-cos-32-rev.png}):

\begin{verbatim}
x <- torch_cos(frequency(32, N) * sample_positions)
plot_ft(x)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-32-rev.png}

}

\caption{\label{fig-dft-cos-32-rev.png}A pure cosine that performs
thirty-two revolutions over the sampling period, and its DFT. This is
the highest frequency where, given sixty-four sample points, no aliasing
will occur. Imaginary parts and phases still zero.}

\end{figure}

\hypertarget{varying-amplitude}{%
\subsection{\texorpdfstring{Varying
amplitude\index{Fourier Transform!varying amplitude}}{Varying amplitude}}\label{varying-amplitude}}

Now, let's think about what happens when we vary amplitude. For example,
say the signal gets twice as loud. Now, there will be a multiplier of 2
that can be taken outside the inner product. In consequence, the only
thing that changes is the magnitude of the coefficients.

Let's verify this. The modification is based on the example we had
before the very last one, with four revolutions over the sampling period
(fig.~\ref{fig-dft-cos-mult-amplitude}):

\begin{verbatim}
x <- 2 * torch_cos(frequency(4, N) * sample_positions)
plot_ft(x)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-mult-amplitude.png}

}

\caption{\label{fig-dft-cos-mult-amplitude}Pure cosine with four
revolutions over the sampling period, and doubled amplitude. Imaginary
parts and phases still zero.}

\end{figure}

So far, we have not once seen a coefficient with non-zero imaginary
part. To change this, we add in \emph{phase}.

\hypertarget{adding-phase}{%
\subsection{\texorpdfstring{Adding
phase\index{Fourier Transform!varying phase}}{Adding phase}}\label{adding-phase}}

Changing the phase of a signal means shifting it in time. Our example
signal is a cosine, a function whose value is 1 at \(t=0\). (That also
was the -- arbitrarily chosen -- starting point of the signal.)

Now assume we shift the signal forward by \(\frac{\pi}{2}\). Then the
peak we were seeing at zero moves over to \(\frac{\pi}{2}\); and if we
still start ``recording'' at zero, we must find a value of zero there.
An equation describing this is the following. For convenience, we assume
a sampling period of \(2 \pi\) and \(k=1\), so that the example is a
simple cosine:

\[
f(x) = cos(x - \phi)
\]

The minus sign may look unintuitive at first. But it does make sense: We
now want to obtain a value of 1 at \(x=\frac{\pi}{2}\), so \(x - \phi\)
should evaluate to zero. (Or to any multiple of \(\pi\).) Summing up, a
\emph{delay} in time will appear as a \emph{negative phase shift}.

Now, we're going to calculate the DFT for a shifted version of our
example signal. But if you like, take a peek at the phase-shifted
version of the time-domain picture now already. You'll see that a
cosine, delayed by \(\frac{\pi}{2}\), is nothing else than a sine
starting at 0.

To compute the DFT, we follow our familiar-by-now strategy. The signal
now looks like this:

\[
\mathbf{x}_n = cos(\frac{2 \pi}{N}*4*x - \frac{\pi}{2})
\]

First, we express it in terms of basis vectors:

\[
\begin{aligned}
\mathbf{x}_n &= cos(\frac{2 \pi}{64} 4 n - \frac{\pi}{2}) \\
&= \frac{1}{2} (e^{i\frac{2 \pi}{64} 4n - \frac{pi}{2}} + e^{i\frac{2 \pi}{64} 60n - \frac{pi}{2}}) \\
&= \frac{1}{2} (e^{i\frac{2 \pi}{64} 4n}  e^{-i \frac{\pi}{2}} + e^{i\frac{2 \pi}{64} 60n}  e^{i\frac{pi}{2}}) \\
&= \frac{1}{2} (e^{-i \frac{\pi}{2}} \mathbf{w}^{4n}_N + e^{i \frac{\pi}{2}} \mathbf{w}^{60n}_N)
\end{aligned}
\]

Again, we have non-zero coefficients only for frequencies \(4\) and
\(60\). But they are complex now, and both coefficients are no longer
identical. Instead, one is the complex conjugate of the other. First,
\(X_4\):

\[
\begin{aligned}
X_4 &= \langle \mathbf{w}^{4n}_N, \mathbf{x}_n \rangle \\
&=\langle \mathbf{w}^{4n}_N, \frac{1}{2} (e^{-i \frac{\pi}{2}} \mathbf{w}^{4n}_N + e^{i \frac{\pi}{2}} \mathbf{w}^{60n}_N) \rangle\\
&= 32 *e^{-i \frac{\pi}{2}} \\
&= -32i
\end{aligned}
\]

And here, \(X_{60}\):

\[
\begin{aligned}
X_{60} &= \langle \mathbf{w}^{60n}_N, \mathbf{x}_N \rangle \\
&= 32 *e^{i \frac{\pi}{2}} \\
&= 32i
\end{aligned}
\]

As usual, we check our calculation using \texttt{torch\_fft\_fft()}
(fig.~\ref{fig-dft-cos-phase-pi12}).

\begin{verbatim}
x <- torch_cos(frequency(4, N) * sample_positions - pi / 2)

plot_ft(x)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-cos-phase-pi12.png}

}

\caption{\label{fig-dft-cos-phase-pi12}Delaying a pure cosine wave by
\(\pi/2\) yields a pure sine wave. Now the real parts of all
coefficients are zero; instead, non-zero imaginary values are appearing.
The phase shift at those positions is \(\pi/2\).}

\end{figure}

For a pure sine wave, the non-zero Fourier coefficients are imaginary.
The phase shift in the coefficients, reported as \(\frac{\pi}{2}\),
reflects the time delay we applied to the signal.

Finally -- before we write some code -- let's put it all together, and
look at a wave that has more than a single sinusoidal component.

\hypertarget{superposition-of-sinusoids}{%
\subsection{\texorpdfstring{Superposition of
sinusoids\index{Fourier Transform!superposition}}{Superposition of sinusoids}}\label{superposition-of-sinusoids}}

The signal we construct may still be expressed in terms of the basis
vectors, but it is no longer a pure sinusoid. Instead, it is a linear
combination of such:

\[
\begin{aligned}
\mathbf{x}_n &= 3 sin(\frac{2 \pi}{64} 4n) + 6 cos(\frac{2 \pi}{64} 2n) +2cos(\frac{2 \pi}{64} 8n)
\end{aligned}
\]

I won't go through the calculation in detail, but it is no different
from the previous ones. You compute the DFT for each of the three
components, and assemble the results. Without any calculation, however,
there's quite a few things we can say:

\begin{itemize}
\tightlist
\item
  Since the signal consists of two pure cosines and one pure sine, there
  will be four coefficients with non-zero real parts, and two with
  non-zero imaginary parts. The latter will be complex conjugates of
  each other.
\item
  From the way the signal is written, it is easy to locate the
  respective frequencies, as well: The all-real coefficients will
  correspond to frequency indices 2, 8, 56, and 62; the all-imaginary
  ones to indices 4 and 60.
\item
  Finally, amplitudes will result from multiplying with \(\frac{64}{2}\)
  the scaling factors obtained for the individual sinusoids.
\end{itemize}

Let's check (fig.~\ref{fig-dft-mix-of-sinuisoids}):

\begin{verbatim}
x <- 3 * torch_sin(frequency(4, N) * sample_positions) +
  6 * torch_cos(frequency(2, N) * sample_positions) +
  2 * torch_cos(frequency(8, N) * sample_positions)

plot_ft(x)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-mix-of-sinuisoids.png}

}

\caption{\label{fig-dft-mix-of-sinuisoids}Superposition of pure
sinusoids, and its DFT.}

\end{figure}

Now, how do we calculate the DFT for less convenient signals?

\hypertarget{coding-the-dft}{%
\section{\texorpdfstring{Coding the
DFT\index{Discrete Fourier Transform (DFT)}}{Coding the DFT}}\label{coding-the-dft}}

Fortunately, we already know what has to be done. We want to project the
signal onto each of the basis vectors. In other words, we'll be
computing a bunch of inner products. Logic-wise, nothing changes: The
only difference is that in general, it will not be possible to represent
the signal in terms of just a few basis vectors, like we did before.
Thus, all projections will actually have to be calculated. But isn't
automation of tedious tasks one thing we have computers for?

Let's start by stating input, output, and central logic of the algorithm
to be implemented. As throughout this chapter, we stay in a single
dimension. The input, thus, is a one-dimensional tensor, encoding a
signal. The output is a one-dimensional vector of Fourier coefficients,
of the same length as the input, each holding information about a
frequency. The central idea is: To obtain a coefficient, project the
signal onto the corresponding basis vector.

To implement that idea, we need to create the basis vectors, and for
each one, compute its inner product with the signal. This can be done in
a loop. Surprisingly little code is required to accomplish the goal:

\begin{verbatim}
dft <- function(x) {
  n_samples <- length(x)

  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)

  Ft <- torch_complex(
    torch_zeros(n_samples), torch_zeros(n_samples)
  )

  for (k in 0:(n_samples - 1)) {
    w_k <- torch_exp(-1i * 2 * pi / n_samples * k * n)
    dot <- torch_matmul(w_k, x$to(dtype = torch_cfloat()))
    Ft[k + 1] <- dot
  }
  Ft
}
\end{verbatim}

To test the implementation, we can take the last signal we analysed, and
compare with the output of \texttt{torch\_fft\_fft()}.

\begin{verbatim}
Ft <- dft(x)
torch_round(Ft$real) %>% as.numeric()
torch_round(Ft$imag) %>% as.numeric()
\end{verbatim}

\begin{verbatim}
[1]  0 0 192 0 0 0 0 0 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
[57] 64 0 0 0 0 0 192 0

[1]  0 0 0 0 -96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
[57] 0 0 0 0 96 0 0 0
\end{verbatim}

Reassuringly -- if you look back -- the results are the same.

Above, did I say ``little code''? In fact, a loop is not even needed.
Instead of working with the basis vectors one-by-one, we can stack them
in a matrix. Then each row will hold the conjugate of a basis vector,
and there will be \(N\) of them. The columns correspond to positions
\(0\) to \(N-1\); there will be \(N\) of them as well. For example, this
is how the matrix would look for \(N=4\):

\begin{equation}\protect\hypertarget{eq-dft-3}{}{
\mathbf{W}_4
=
\begin{bmatrix}
   e^{-i\frac{2 \pi}{4}* 0 * 0} &   e^{-i\frac{2 \pi}{4}* 0 * 1}  & e^{-i\frac{2 \pi}{4}* 0 * 2} &  e^{-i\frac{2 \pi}{4}* 0 * 3}\\
e^{-i\frac{2 \pi}{4}* 1 * 0} &   e^{-i\frac{2 \pi}{4}* 1 * 1}  & e^{-i\frac{2 \pi}{4}* 1 * 2} &  e^{-i\frac{2 \pi}{4}* 1 * 3}\\
e^{-i\frac{2 \pi}{4}* 2 * 0} &   e^{-i\frac{2 \pi}{4}* 2 * 1}  & e^{-i\frac{2 \pi}{4}* 2 * 2} &  e^{-i\frac{2 \pi}{4}* 2 * 3}\\
e^{-i\frac{2 \pi}{4}* 3 * 0} &   e^{-i\frac{2 \pi}{4}* 3 * 1}  & e^{-i\frac{2 \pi}{4}* 3 * 2} &  e^{-i\frac{2 \pi}{4}* 3 * 3}\\
\end{bmatrix}
}\label{eq-dft-3}\end{equation}

Or, evaluating the expressions:

\[
\mathbf{W}_4
=
\begin{bmatrix}
   1 &   1  & 1 &  1\\
1 &   -i  & -1 &  i\\
1 &   -1  & 1 &  -1\\
1 &   i  & -1 &  -i\\
\end{bmatrix}
\]

With that modification, the code looks a lot more elegant:

\begin{verbatim}
dft_vec <- function(x) {
  n_samples <- length(x)

  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)
  k <- torch_arange(0, n_samples - 1)$unsqueeze(2)

  mat_k_m <- torch_exp(-1i * 2 * pi / n_samples * k * n)

  torch_matmul(mat_k_m, x$to(dtype = torch_cfloat()))
}
\end{verbatim}

As you can easily verify, the result is the same.

Before we move on to the next chapter -- and the Fast Fourier Transform
-- we should test our implementation on something more complex.

\hypertarget{fun-with-sox}{%
\section{\texorpdfstring{Fun with
\texttt{sox}}{Fun with sox}}\label{fun-with-sox}}

\href{http://sox.sourceforge.net/}{\texttt{sox}} is a command line tool
for processing audio, most often (I guess) used to quickly play some
sound file, for resampling, or to convert between different file
formats. However, it can \emph{generate} sound, as well! (Admittedly,
the syntax is not the most intuitive.) Here is the command I used in
generating the test sound analysed below:

\begin{verbatim}
sox --combine concatenate \
  "|sox -r 4000 -n -p synth 0.2 \
  sine 941 sine 1336 pinknoise remix -" \
  "|sox -r 4000 -n -p synth 0.2 \
  sine 852 sine 1336 pinknoise remix - pad 0.15 0.15" \
  "|sox -r 4000 -n -p synth 0.2 \
  sine 852 sine 1477 pinknoise remix -" \
  resources/dft-dial.wav
\end{verbatim}

The result is a concatenation of three distinct sound events, of
duration \texttt{0.2} seconds, separated by slightly shorter periods of
silence (\texttt{pad\ 0.15\ 0.15}). Each sound event is a superposition
of three components: two sine waves of different frequency, and pink
noise. The frequency combinations used are not accidental: They
represent the encoding of the numbers 0, 8, and 9 in the
\href{https://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling}{Dual-tone
multi-frequency signaling (DTMF)} system, respectively.

For this signal, what would we expect to see in a spectrogram? We expect
to see three distinct phases, clearly separated by ``nothing'', that
each show two dominant frequencies. The first and the second should have
one frequency in common; the same holds for phases two and three. In
addition, for all three phases, we should see contributions from all
other frequencies, with lower frequencies having stronger impact than
higher ones. (That's what defines pink noise.)

At this point, allow me to call \texttt{sox} one last time. If you've
been using that program before, did you know that it can create
spectrograms? This one-liner uses all the default settings; yet the
result has all the information we want (fig.~\ref{fig-dft-spectrogram}):

\begin{verbatim}
sox resources/dial.wav -n spectrogram \ 
  -m -l -w kaiser -o dial-spectrogram.png
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-dial-spectrogram.png}

}

\caption{\label{fig-dft-spectrogram}Spectrogram, created by
\texttt{sox\ resources/dial.wav\ -n\ spectrogram\ -m\ -l\ -w\ kaiser\ -o\ dial-spectrogram.png}.}

\end{figure}

Now, let's load this file into R, making use of \texttt{torchaudio}, a
package we already know from the audio classification chapter.

\begin{verbatim}
library(torchaudio)

wav <- tuneR_loader("resources/dft-dial.wav")
wav
\end{verbatim}

\begin{verbatim}
WaveMC Object
  Number of Samples:      3600
  Duration (seconds):     0.9
  Samplingrate (Hertz):   4000
  Number of channels:     1
  PCM (integer format):   TRUE
  Bit (8/16/24/32/64):    32
\end{verbatim}

From all the information stored in \texttt{tuneR}'s \texttt{WaveMC}
object, we need just the sampling rate and the data itself.

\begin{verbatim}
waveform_and_sample_rate <- transform_to_tensor(wav)
waveform <- waveform_and_sample_rate[[1]]
sample_rate <- waveform_and_sample_rate[[2]]

dim(waveform)
\end{verbatim}

\begin{verbatim}
[1]    1 3600
\end{verbatim}

As expected, a plot of amplitude over time does not reveal too much
(fig.~\ref{fig-dft-dial-waveform}):

\begin{verbatim}
df <- data.frame(
  x = 1:dim(waveform)[2],
  y = as.numeric(waveform$squeeze(1))
)
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-dial-waveform.png}

}

\caption{\label{fig-dft-dial-waveform}Three consecutive ringtones, time
domain representation.}

\end{figure}

Now, we will compute the Fourier Transform -- twice. First, with
\texttt{torch\_fft\_fft()}, so we know the ``truth''. Then, using our
own \texttt{dft()}. For both, we will display the magnitudes of the
lower half of the spectrum, that is, all frequencies up to the Nyquist
rate. Finally, with help of \texttt{torch\_fft\_ifft()}, we will make
sure we can regenerate the time domain representation from what
\texttt{dft()} gave us.

In order to create a meaningful plot, there's one additional step to be
taken. So far, the magnitudes displayed in the frequency-domain plots
have simply been ordered by \(k\), the index of the basis vector in
question. In fact, there was no concept of a \emph{real-world
frequency}. But now, we want to see frequencies treated the way we
conceptualize them: as number of cycles per second, measured in Hertz.

To be able to do the conversion, we need the sampling rate, which we
already saw is 4000 Hertz. We then map the lower-half sample indices
(\texttt{bins\_below\_nyquist}) to real-world frequencies, multiplying
by the ratio of sampling rate to overall number of samples:

\begin{verbatim}
num_samples <- dim(waveform)[2]
nyquist_cutoff <- num_samples / 2 + 1
bins_below_nyquist <- 1:nyquist_cutoff

frequencies_per_bin <- sample_rate / num_samples
real_world_frequencies <- frequencies_per_bin *
  bins_below_nyquist
\end{verbatim}

Here, then, is the magnitude plot for \texttt{torch\_fft\_fft()}
(fig.~\ref{fig-dft-dial-fft}):

\begin{verbatim}
dial_fft <- torch_fft_fft(waveform)$squeeze()

p_magnitude <- create_plot(
  real_world_frequencies,
  torch_abs(dial_fft)[1:nyquist_cutoff], "magnitude"
)

p_magnitude
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-dial-fft.png}

}

\caption{\label{fig-dft-dial-fft}DFT of the ringtone signal, computed by
means of \texttt{torch\_fft\_fft()} . Displayed are the magnitudes of
frequencies below the Nyquist rate.}

\end{figure}

The spectrum reflects the noise component in the signal, but the four
peaks (which we know to be located at 852, 941, 1336, and 1477 Hertz)
are clearly visible.

Now, does our hand-written code yield the same result
(fig.~\ref{fig-dft-dial-dft})?

\begin{verbatim}
dial_dft <- dft(waveform$squeeze())

p_magnitude <- create_plot(
  real_world_frequencies,
  torch_abs(dial_fft)[1:nyquist_cutoff], "magnitude"
)

p_magnitude
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-dial-dft.png}

}

\caption{\label{fig-dft-dial-dft}DFT of the ringtone signal, using our
hand-written code. Displayed are the magnitudes of frequencies below the
Nyquist rate.}

\end{figure}

It does. Finally, let's use the Inverse DFT to recreate the signal
(fig.~\ref{fig-dft-dial-ifft}).

\begin{verbatim}
reconstructed <- torch_fft_ifft(dial_dft)
df <- data.frame(
  x = 1:num_samples,
  y = as.numeric(reconstructed$real)
)

ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/dft-dial-ifft.png}

}

\caption{\label{fig-dft-dial-ifft}Reconstruction of the time domain
signal from the output of \texttt{dft()}.}

\end{figure}

And there we are. We've implemented the DFT ourselves, and learned quite
a bit on our way. For the Fast Fourier Transform, the topic of the next
chapter, the plan is exactly the same.

\hypertarget{sec:signal-processing-2}{%
\chapter{The Fast Fourier Transform
(FFT)}\label{sec:signal-processing-2}}

In the last chapter, we saw that to code the Discrete Fourier Transform
(DFT), we need just a few lines of code -- notwithstanding the depth and
richness of the theory. Surprisingly, it's not that different for the
FFT, the famous \emph{Fast Fourier
Transform}\index{Fast Fourier Transform (FFT)}.

In this chapter, we'll first look at what the FFT (more precisely: its
``most classic'' version) actually does, building on the understanding
we gained in the previous chapter.

Then, we'll code up a few different implementations, and time their
performance. None of the hand-coded implementations will, of course, be
able to outperform \texttt{torch\_fft\_fft()}, \texttt{torch} delegating
to highly optimized C++ code. But our results will be interesting in
many ways: We'll see that in practice, it is not just the algorithm per
se that matters. The programming language, too, plays an essential role,
in the sense that its properties substantially affect feasibility of
improvements. Put differently, there is an \emph{interaction} between
algorithmic and language-inherent properties that will decide on the
final outcome.

First though, in this book's spirit of aiming to shed a light on the
ideas and concepts involved, we would like to understand what is
involved in the Fast Fourier Transform.

\hypertarget{some-terminology}{%
\section{Some terminology}\label{some-terminology}}

Let me make clear right at the outset that, as compared to the DFT, the
FFT is \emph{not another transform}. Its output is just the same as that
of the DFT. With the FFT, it's all about the ``fast''. Also, there is no
``one'' FFT. Instead, there are different families; and in each family,
there are various sub-types.

Here, I'm focusing on the ``classic among the classics'': the one that
goes by \emph{radix-2 decimation-in-time (DIT)}. Here ``radix-2'' refers
to an implementation detail; it indicates that the algorithm will
require input size to equal a power of two. ``Decimation in time'', on
the other hand, relates to the overall strategy being employed:
divide-and-conquer. The input is recursively split into halves, and
partial results are combined in a clever way. (Just as an aside, there
is a very similar algorithm called ``decimation in frequency''. There,
it is the frequency domain where recursive splitting occurs.)

Now, we discuss how this works. You'll see that once we've worked out
clearly what we want to do, a (naive) implementation does not require
more code than the straightforward DFT from the last chapter. And please
be assured that although this section has many equations, each and every
manipulation will be explained in words.

\hypertarget{radix-2-decimation-in-timedit-walkthrough}{%
\section{\texorpdfstring{Radix-2 decimation-in-time(DIT)
walkthrough\index{Fast Fourier Transform (FFT)!decimation in time}}{Radix-2 decimation-in-time(DIT) walkthrough}}\label{radix-2-decimation-in-timedit-walkthrough}}

The simplifications resulting from \emph{decimation in time} can be
presented as a two-step logic. Reflecting that view, and re-using (with
different semantics) terminology employed in the convolution chapter, I
could call them ``input-view phase'' and ``output-view phase''. However,
these are not phases in a temporal sense, and more importantly -- in a
software-focused book -- we'll see that as ported to code, their
respective impacts differ a lot. Therefore, I'll name the upcoming two
sections in ways reflecting importance instead. (I'll still make clear
what I mean by input and output views here, though.)

\hypertarget{the-main-idea-recursive-split}{%
\subsection{The main idea: Recursive
split}\label{the-main-idea-recursive-split}}

As we would expect for a divide-and-conquer algorithm, the main and most
impactful observation is that if the input is split up recursively, the
problem can be divided into sub-problems that get increasingly easier to
solve -- provided we know how to combine the partial results into a
final solution.

Before we start, let me recall some notation, and make a slight
modification that will turn out convenient. In the previous chapter,
with \(N\) the size of the input (equivalently, the number of resulting
frequency coefficients), and \(k\) (ranging from zero to \(N-1\))
referencing the vector in question, the DFT basis vectors were defined
like so:

\[
\mathbf{w}^{kn}_N = e^{i\frac{2 \pi}{N}k n}
\]

Then, the \(k\)th frequency coefficient was obtained by computing the
inner product between \(\mathbf{w}^{kn}_N\) and the input,
\(\mathbf{x}_n\):

\[
\begin{aligned}
X_k &= \langle \mathbf{w}^{kn}_N, \mathbf{x}_n \rangle \\ &= \sum_{n=0}^{N-1} x[n] \ w^{-nk}_N\\
\end{aligned}
\]

In this chapter, we will be working with the complex conjugates of the
basis vectors throughout, since it's those that actually get
(element-wise) multiplied with the input vector. For convenience, we
thus slightly change notation, and let \(w^{kn}_N\) refer to the
conjugated complex exponential\footnote{Note that now we have a scalar,
  not a vector.}:

\begin{equation}\protect\hypertarget{eq-fft-1}{}{
w^{kn}_N = e^{-i\frac{2 \pi}{N}k n}
}\label{eq-fft-1}\end{equation}

Then, abstracting over \(n\), we also have
\begin{equation}\protect\hypertarget{eq-fft-2}{}{
w^k_N = e^{-i\frac{2 \pi}{N}k}
}\label{eq-fft-2}\end{equation}

That said, we state again what we want to compute: the frequency
coefficients \(X_k\).

\[
\begin{aligned}
X_k &=  \sum_{n=0}^{N-1} x[n] \ w^{nk}_N\\
\end{aligned}
\]

Now comes what I was thinking to refer to as the ``input view''. We take
the input sequence, and divide up the computation into two parts. One
will deal with the even, the other, with the odd indices of the signal.
So expressed, the sums only go up to \(N/2 - 1\).

\begin{equation}\protect\hypertarget{eq-fft-3}{}{
\begin{aligned}
X_k &=  \sum_{n=0}^{(N/2-1)} x[2n] \ w^{2nk}_N + \sum_{n=0}^{N/2-1} x[2n+1] \ w^{(2n+1)k}_N \\
\end{aligned}
}\label{eq-fft-3}\end{equation}

Now, that second sum can be rewritten, splitting up the \(w^{2nk+k}_N\)
into two factors:

\[
\sum_{n=0}^{N/2-1} x[2n+1] \ w^{2nk+k}_N = \sum_{n=0}^{N/2-1} x[2n+1] \ w^{2nk}_N w^{k}_N
\]

The second factor is exactly the \(w^{k}_N\) we were introducing above.
Since it does not depend on \(n\), we can move it out of the sum. This
yields

\[
\begin{aligned}
X_k &=  \sum_{n=0}^{(N/2-1)} x[2n] \ w^{2nk}_N + w^k_N \sum_{n=0}^{N/2-1} x[2n+1] \ w^{2nk}_N \\
\end{aligned}
\]

Now the exponential factor is the same in both sums. Let's inspect it a
bit more closely. It is the multiplication factor of a DFT of size
\(N\), at (frequency-by-time) position \(2nk\). If we write this out, we
see that we can move the factor \(2\) from the numerator to the
denominator of the fraction:

\[
w^{2nk}_N = e^{-i\frac{2 \pi}{N}2nk} = e^{-i\frac{2 \pi}{N/2}nk} = w^{nk}_{N/2}
\]

Why does this matter? The result is actually the corresponding basis
vector of a DFT of size \(N/2\), at position \(nk\). Which means that
now, we are actually computing a DFT of \emph{half the size} -- or
rather, two such DFTs:

\begin{equation}\protect\hypertarget{eq-fft-4}{}{
X_k =  \sum_{n=0}^{(N/2-1)} x[2n] \ w^{nk}_{N/2} + w^k_N \ \sum_{n=0}^{N/2-1} x[2n+1] \ w^{nk}_{N/2} \\
}\label{eq-fft-4}\end{equation}

Let's write this in more readable form:

\begin{equation}\protect\hypertarget{eq-fft-5}{}{
X_k =  X^{even}_k +  w^k_N \ X^{odd}_k
}\label{eq-fft-5}\end{equation}

Now, you probably see where this is going. What we've done once -- halve
the size of the computation -- we can do again \ldots{} and again. It is
this recursive halving that allows the FFT to obtain it famous reduction
in computational cost.

This is the main ingredient of the magic, but it is not quite everything
yet.

\hypertarget{one-further-simplification}{%
\subsection{One further
simplification}\label{one-further-simplification}}

There is one additional simplification we can make. Compared to the
first, it is of lesser significance, at least as far as computational
performance is concerned. However, it definitely matters from an
aesthetics point of view.

Did you notice something strange in our final formula? We are computing
DFTs of size \(N/2\), but still, the factor \(w^k_N\) appears! This
isn't a problem, but it is not ``nice'', either. Fortunately, for those
who mind, the ``inconsequence'' can be eliminated.

What follows is what I was tempted to name the ``output-side view''.
That's because now, we roll up things from the end, starting from the
computation's output. We take the set of Fourier coefficients \(X_k\),
and independently consider the first and the second half. Note how here,
like in the input-centric view, we apply a split-in-two strategy; just
this time, the halving is done in a different way.

Looking at both halves, we notice that both have their dedicated subsets
of multiplication factors \(w^k_N\), one with \(k\) ranging from \(0\)
to \(N/2-1\), the other, from \(N/2\) to \(N-1\). For the first, this
means we can change the upper limit in the sum, yielding

\[
X^{upper}_k =  X^{even}_k +  w^k_N \ X^{odd}_k \ , \ \ k = 0 ... N/2-1
\]

For the second, we can achieve the desired result by summing over the
same range, but adding \(N/2\) to \(k\) everywhere.

\[
X^{lower}_{k+N/2} =  \sum_{n=0}^{N/2-1} x[2n] \ w^{n (k+N/2)}_{N/2} + w^{k+N/2}_N \ \sum_{n=0}^{N/2-1} x[2n + 1] \ w^{n (k+ N/2)}_{N/2} \\
\]

Now, in the first of the sums that make up \(X^{lower}\), the
exponential can be factored, and we see that the factor containing the
\(N/2\) evaluates to \(1\) (and thus, disappears):

\[
\begin{aligned}
w^{n(k+N/2)}_{N/2}&= e^{-i\frac{2 \pi}{N/2} n (k + N/2)} \\ &= e^{-i\frac{2 \pi}{N/2}n k} *  e^{-i\frac{2 \pi}{N/2}n (N/2)} \\ &= e^{-i\frac{2 \pi}{N/2}n k} *  e^{-i\frac{2 \pi}{N/2}(N/2)} \\ &= e^{-i\frac{2 \pi}{N/2}n k} *  1
\end{aligned}
\]

As a result, the first of the sums now looks like this:

\[
X^{lower}_{k+N/2} =  \sum_{n=0}^{N/2-1} x[2n] \ w^{n k}_{N/2} + [...]
\]

The same thing can be done in the second sum:

\[
X^{lower}_{k+N/2} =  [...] + w^{k+N/2}_N \ \sum_{n=0}^{N/2-1} x[2n + 1] \ w^{n k}_{N/2} \\
\]

Now there's just one last inconvenient index of \(k + N/2\) remaining. A
calculation similar to that above shows we can replace it by a minus
sign:

\[
\begin{aligned}
w^{k+N/2}_N&= e^{-i\frac{2 \pi}{N} (k + N/2)} \\
&= e^{-i\frac{2 \pi}{N} k} * e^{-i\frac{2 \pi}{N} N/2}\\
&= e^{-i\frac{2 \pi}{N} k} * e^{-i \pi}\\
&= e^{-i\frac{2 \pi}{N} k} * (-1)\\
\end{aligned}
\]

In consequence, the complete second part can be written like this:

\[
X^{lower}_{k+N/2} =  \sum_{n=0}^{N/2-1} x[2n] \ w^{n k}_{N/2} - w^k_N \ \sum_{n=0}^{N/2-1} x[2n + 1] \ w^{n k}_{N/2}
\]

And now, the second half looks nearly like the first one, just with a
change in sign. Here, then, is the final algorithm:

\begin{equation}\protect\hypertarget{eq-fft-6}{}{
\begin{aligned}
& X^{upper}_k =  X^{even}_k +  w^k_N \ X^{odd}_k \ , \ \ k = 0 ... N/2-1 \\
& X^{lower}_{k+N/2} =  X^{even}_k -  w^k_N \ X^{odd}_k \ , \ \ k = 0 ... N/2-1
\end{aligned}
}\label{eq-fft-6}\end{equation}

Owed to a popular form of visualization, this representation is often
referred to as the ``butterfly''. If you're curious, you won't have
problems finding related diagrams on the net. Personally, I don't find
them very helpful, which is why I'm not reproducing them here.

In conclusion, we've now arrived at a rule that tells us how to simplify
an FFT of size \(N\) by replacing it with an FFT of size \(N/2\). The
complete algorithm then consists in recursive application of that rule.

We're ready to start thinking about how to implement this.

\hypertarget{fft-as-matrix-factorization}{%
\section{\texorpdfstring{FFT as matrix
factorization\index{Fast Fourier Transform (FFT)!as a matrix factorization}}{FFT as matrix factorization}}\label{fft-as-matrix-factorization}}

Below, we'll explore different ways of coding the FFT. In two of them,
you'll directly recognize the rule Equation~\ref{eq-fft-6}. The third is
different, though. It makes direct use of the fact that the DFT matrix
\(\mathbf{W}_N\) can be factored into three sparse matrices, each
materializing one of the three stages inherent in the rule: split up the
input into even and odd indices; compute the two half-sized FFTs;
recombine the results.

For example, take \(\mathbf{W}_4\), the matrix we analyzed in the
previous chapter:

\[
\mathbf{W}_4
=
\begin{bmatrix}
1 &   1  & 1 &  1\\
1 &   -i  & -1 &  i\\
1 &   -1  & 1 &  -1\\
1 &   i  & -1 &  -i\\
\end{bmatrix}
\]

This can be factorized into three matrices like so:

\[
\begin{aligned}
\mathbf{W}_4 
&=
\begin{bmatrix}
1 &   1  & 1 &  1\\
1 &   -i  & -1 &  i\\
1 &   -1  & 1 &  -1\\
1 &   i  & -1 &  -i\\
\end{bmatrix} \\ 
&=
\begin{bmatrix}
1 &   0  & 1 &  0\\
0 &   1  & 0 &  -i\\
1 &   0  & -1 &  0\\
0 &   1  & 0 &  i\\
\end{bmatrix}
\begin{bmatrix}
1 &   1  & 0 &  0\\
1 &   -1  &  0 &  0\\
0 &   0  & 1 &  1\\
0 &   0  & 1 &  -1\\
\end{bmatrix}
\begin{bmatrix}
1 &   0  & 0 &  0\\
0 &   0  & 1 &  0\\
0 &   1  & 0 &  0\\
0 &   0  & 0 &  1\\
\end{bmatrix}
\end{aligned}
\]

The rightmost matrix (call it \(P\), for permutation) reorders the
input. Here's how it acts on a suitably-sized input vector.

\[
\mathbf{P}_4 \mathbf{x}
=
\begin{bmatrix}
1 &   0  & 0 &  0\\
0 &   0  & 1 &  0\\
0 &   1  & 0 &  0\\
0 &   0  & 0 &  1\\
\end{bmatrix}
\begin{bmatrix}
x1 \\
x2 \\
x3 \\
x4 \\
\end{bmatrix}
=
\begin{bmatrix}
x1 \\
x3 \\
x2 \\
x4 \\
\end{bmatrix}
\]

Now that even- and odd-indexed values are nicely separated, we can
construct a block matrix that applies a DFT to each of those groups in
isolation. In this case, the DFT in question is of size two. If you look
at the central matrix above, you'll see that it contains two instances
of \(\mathbf{W}_2\), with \(\mathbf{W}_2\) being

\[
\mathbf{W}_2 =
\begin{bmatrix}
1 &   1  \\
1 &   -1 \\
\end{bmatrix}
\]

Taking as input the permuted signal, \(\mathbf{P}_4 \mathbf{x}\), the
block matrix produces the following output:

\[
\mathbf{W}_{2*2}\mathbf{P}_{4}\mathbf{x}
=
\begin{bmatrix}
1 &   1  & 0 &  0\\
1 &   -1  &  0 &  0\\
0 &   0  & 1 &  1\\
0 &   0  & 1 &  -1\\
\end{bmatrix}
\begin{bmatrix}
x1 \\
x3 \\
x2 \\
x4 \\
\end{bmatrix}
=
\begin{bmatrix}
x1+x3 \\
x1-x3 \\
x2+x4 \\
x2-x4 \\
\end{bmatrix}
\]

Next, the two sets of coefficients need to be recombined in the correct
way. Personally, I find it hard to mentally picture how the leftmost
matrix in the factorization does it; so let's try if we can build up the
matrix ourselves.

The FFT rule Equation~\ref{eq-fft-6} tells us what has to happen. Here
it is again:

\[
X^{upper}_k =  X^{even}_k +  w^k_N \ X^{odd}_k \ , \ \ k = 0 ... N/2-1
\]

\[
X^{lower}_{k+N/2} =  X^{even}_k -  w^k_N \ X^{odd}_k \ , \ \ k = 0 ... N/2-1
\]

In this example, \(N/2\) is 2; we thus need \(w^0_4\) and \(w^1_4\).
Their values are

\[
\begin{aligned}
&w^0_4 = e^{-i\frac{2 \pi}{4}0} = 1\\
&w^1_4 = e^{-i\frac{2 \pi}{4}1} = -i\\
\end{aligned}
\]

As an aside, we could also have read them off the transformation matrix,
\(W_4\): These are the first two basis vectors, computed at \(n=1\), and
thus are found right on top of the second column.

Now, we just mechanically apply the rule.

\[
\begin{aligned}
&X_0 = X^{upper}_0 = X^{even}_0 +  w^0_4 \ X^{odd}_0 = (x_1 + x_3) + 1 * (x_2 + x_4)  \\
&X_1 = X^{upper}_1 = X^{even}_1 +  w^1_4 \ X^{odd}_1 = (x_1 - x_3) - i * (x_2 - x_4)  \\
&X_2 = X^{lower}_2 = X^{even}_0 -  w^0_4 \ X^{odd}_0 = (x_1 + x_3) - 1 * (x_2 + x_4)  \\
&X_3 = X^{lower}_3 = X^{even}_1 -  w^1_4 \ X^{odd}_1 = (x_1 - x_3) + i * (x_2 - x_4)  \\
\end{aligned}
\]

This gives us the multiplication factors to be applied to the vector
input. All that remains to be done is put them into a matrix. Directly
reading off the above equations, and filling in zeroes whenever an input
is not used, this is what we obtain for the ``butterfly'' matrix
\(\mathbf{B}\):

\[
\mathbf{B}_4
=
\begin{bmatrix}
1 &   0  & 1 &  0\\
0 &   1  & 0 &  -i\\
1 &   0  & -1 &  0\\
0 &   1  & 0 &  i\\
\end{bmatrix}
\]

Comparing with the leftmost matrix in the factorization, we see we've
arrived at the correct result.

One thing that's not immediately clear, however, is how to implement
this recursively. It certainly seems like a lot of overhead to compute
the complete matrix factorization at every recursive step. Fortunately,
this is not needed. For one, the sorting can be done just once, right in
the beginning. And secondly, it turns out that the matrices I've been
referring to as \(\mathbf{W}_{2*2}\) and \(\mathbf{B}_{4}\) are
intimately related: \(\mathbf{B}_{4}\) is what would go into a block
matrix \(\mathbf{W}_{4*4}\).

The recursive procedure can then be laid out very clearly. Here, for
example, is how the complete procedure would look for an input size of
8:

\[
\mathbf{W}_8
=
\mathbf{B}_8
\begin{bmatrix}
 \mathbf{B}_4 &  \mathbf{0}\\
 \mathbf{0} &  \mathbf{B}_4\\
\end{bmatrix}
\begin{bmatrix}
\mathbf{B}_2 &   \mathbf{0}  & \mathbf{0} &  \mathbf{0}\\
\mathbf{0} &   \mathbf{B}_2  &  \mathbf{0} &  \mathbf{0}\\
\mathbf{0} &   \mathbf{0}  & \mathbf{B}_2 &  \mathbf{0}\\
\mathbf{0} &   \mathbf{0}  & \mathbf{0} &  \mathbf{B}_2\\
\end{bmatrix}
\mathbf{R}
\]

\(\mathbf{R}\) is the matrix that, once and for all, sorts the input in
the required way. I've written \(\mathbf{R}\) for ``bit reversal'',
since that is the actual algorithm used. We won't go into its workings
here, but explanations are readily found on the web.

Having discussed DFT matrix factorization, we're ready to look at some
code.

\hypertarget{implementing-the-fft}{%
\section{\texorpdfstring{Implementing the
FFT\index{Fast Fourier Transform (FFT)!comparing performance}}{Implementing the FFT}}\label{implementing-the-fft}}

We discuss and compare for performance three different implementations
of the FFT, plus our two DFT versions from the last chapter. Let me
start by listing, again, what we did there.

\hypertarget{dft-the-loopy-way}{%
\subsection{DFT, the ``loopy'' way}\label{dft-the-loopy-way}}

This was the way we first coded the DFT, computing the dot products
between input and each basis vector in a loop.

\begin{verbatim}
library(torch)

dft <- function(x) {
  n_samples <- length(x)
  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)
  F <- torch_complex(
    torch_zeros(n_samples),
    torch_zeros(n_samples)
  )

  for (k in 0:(n_samples - 1)) {
    w_k <- torch_exp(-1i * 2 * pi / n_samples * k * n)
    dot <- torch_matmul(w_k, x$to(dtype = torch_cfloat()))
    F[k + 1] <- dot
  }
  F
}
\end{verbatim}

\hypertarget{dft-vectorized}{%
\subsection{DFT, vectorized}\label{dft-vectorized}}

Next, we went on to replace the loop by arranging the basis vectors in a
matrix.

\begin{verbatim}
dft_vec <- function(x) {
  n_samples <- length(x)

  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)
  k <- torch_arange(0, n_samples - 1)$unsqueeze(2)
  mat_k_m <- torch_exp(-1i * 2 * pi/n_samples * k * n)

  torch_matmul(mat_k_m, x$to(dtype = torch_cfloat()))
}
\end{verbatim}

\hypertarget{radix-2-decimation-in-time-fft-recursive}{%
\subsection{Radix-2 decimation in time FFT,
recursive}\label{radix-2-decimation-in-time-fft-recursive}}

Just like we did for the DFT algorithm per se, we can straightforwardly,
by-specification implement the FFT. This time, the logically-imposed
design is recursive, not iterative. In each call to \texttt{fft()}, the
input is split into even and odd indices, respective half-size FFTs are
computed, and the two sets of outputs are combined as required.

\begin{verbatim}
# straightforward, recursive implementation of the FFT.
# Expects input size to be a power of 2.
fft <- function(x) {
  n_samples <- length(x)
  if (n_samples == 1) {
    return(x)
  }

  X_upper <- fft(x[1:n_samples:2])
  X_lower <- fft(x[2:n_samples:2])

  w_k <- torch_exp(
    -2 * pi * torch_complex(0, 1) *
      torch_arange(0, n_samples / 2 - 1) / n_samples
  )
  torch_cat(list(
    X_upper + w_k * X_lower,
    X_upper - w_k * X_lower
  ))
}
\end{verbatim}

This function expects input size to equal a power of two.

\hypertarget{radix-2-decimation-in-time-fft-by-matrix-factorization}{%
\subsection{Radix-2 decimation in time FFT by matrix
factorization}\label{radix-2-decimation-in-time-fft-by-matrix-factorization}}

Next, we implement the matrix-factorization strategy described above.
\texttt{fft\_vec()} is a generalization of the logic spelt out in Brad
Osgood's wonderful book on the Fourier Transform (Osgood (2019)).

We sort the input tensor (a single time), apply successive block
matrices of doubled-in-size ``butterflies'', and multiply the result
with a single butterfly matrix of size matching the number of inputs.

The sorting may conveniently be done using \texttt{bitrevorder()}, a
function provided by the R package \texttt{gsignal}.

In the loop, you can see how the butterfly matrices are built: They
consist of a combination of identity matrices with diagonal matrices
holding the \(w^k_N\).

\begin{verbatim}
library(torch)
library(gsignal)

# requirements: input length is at least 4, and a power of 2
fft_matrix <- function(x) {

  # perform sorting just once, a the beginning
  x <- torch_tensor(
    bitrevorder(as.numeric(x)),
    dtype = torch_cfloat()
  )

  n_samples <- length(x)

  # smallest butterfly matrix, needed for all valid inputs
  B2 <- torch_tensor(
    c(1, 1, 1, -1),
    dtype = torch_cfloat()
  )$view(c(2, 2))
  B2_block <- torch_block_diag(
    B2$`repeat`(c(n_samples / 2, 1))$split(2)
  )
  acc <- torch_matmul(B2_block, x)

  # iterative implementation then starts with B4
  n <- 4

  while (n <= n_samples) {

    # build up current butterfly matrix
    I <- torch_eye(n / 2)
    O <- torch_diag(
      torch_exp(
        -1i * 2 * pi *
          torch_arange(0, n / 2 - 1) / (n / 2 * 2)
      )
    )
    B <- torch_cat(list(
      torch_cat(list(I, O), dim = 2),
      torch_cat(list(I, -O), dim = 2)
    ), dim = 1)

    # in the final multiplication,
    # B directly matches input length
    if (n == n_samples) {
      return(torch_matmul(B, acc))
    }

    # create block-diagonal matrix from butterflies
    # at each iteration,
    # we need to replicate B {n_samples/rank(B) times}
    # this is achieved by first repeating B row-wise,
    # then splitting up into rank(n) parts)
    B_block <- torch_block_diag(
      B$`repeat`(c(n_samples / n, 1))$split(n)
    )
    acc <- torch_matmul(B_block, acc)
    n <- n * 2
  }
  acc
}
\end{verbatim}

\hypertarget{radix-2-decimation-in-time-fft-optimized-for-vectorization}{%
\subsection{Radix-2 decimation in time FFT, optimized for
vectorization}\label{radix-2-decimation-in-time-fft-optimized-for-vectorization}}

Finally, let me present one more implementation. It is a literal
translation of the Python code published by
\href{http://jakevdp.github.io/blog/2013/08/28/understanding-the-fft/\#Vectorized-Numpy-Version}{Jake
van der Plas} on his blog. Although it looks more involved than the
recursive \texttt{fft()} above, it really implements the same algorithm,
all while making use of vectorization as much as possible. In other
words, it is to \texttt{fft()} what \texttt{dft\_vec()} is to
\texttt{dft()}.

\begin{verbatim}
# torch translation of
# http://jakevdp.github.io/blog/2013/08/28/
# understanding-the-fft/#Vectorized-Numpy-Version

fft_vec <- function(x) {
  n_samples <- length(x)
  # could be chosen higher for performance reasons
  n_min <- 2 

  # Perform an O[N^2] DFT on all length-N_min 
  # sub-problems at once
  n <- torch_arange(0, n_min - 1)$unsqueeze(1)
  k <- torch_arange(0, n_min - 1)$unsqueeze(2)

  # by starting with one (vectorized-by-matmul)
  # "classic DFT" (instead of a block matrix of B_mins),
  # we don't need the bitrevorder step
  mat_k_m <- torch_exp(-1i * 2 * pi / n_min * k * n)
  F <- torch_matmul(
    mat_k_m,
    x$to(dtype = torch_cfloat())$reshape(list(n_min, -1))
  )

  # build-up each level of the recursive calculation
  # all at once
  while (dim(F)[1] < n_samples) {
    F_first <- F[, 1:(dim(F)[2] / 2)]
    F_second <- F[, (dim(F)[2] / 2 + 1):dim(F)[2]]
    # only need first half of w_ks
    w_k <- torch_exp(
      -1i * pi *
        torch_arange(0, dim(F)[1] - 1) / dim(F)[1]
    )$unsqueeze(2)
    F <- torch_vstack(list(
      F_first + w_k * F_second,
      F_first - w_k * F_second
    ))
    # w_k * F_second multiplies both at once (column-wise)
  }
  F$ravel()
}
\end{verbatim}

\hypertarget{checking-against-torch_fft_fft}{%
\subsection{\texorpdfstring{Checking against
\texttt{torch\_fft\_fft()}}{Checking against torch\_fft\_fft()}}\label{checking-against-torch_fft_fft}}

Now, before we compare those five functions performance-wise, let's
check whether they yield the same results as \texttt{torch\_fft\_fft()}.
We don't expect identity over a large number of decimal places; but it
will be good to know about eventual differences in accuracy between the
different implementations.

\begin{verbatim}
x <- torch_randn(2^13)
atol <- 1e-4

y_ref <- torch_fft_fft(x)

y_dft <- dft(x)
y_dft_vec <- dft_vec(x)
y_fft <- fft(x)
y_fft_vec <- fft_vec(x)
y_fft_matrix <- fft_matrix(x)

torch_allclose(y_dft, y_ref, atol = atol)
torch_allclose(y_dft_vec, y_ref, atol = atol)
torch_allclose(y_fft, y_ref, atol = atol)
torch_allclose(y_fft_vec, y_ref, atol = atol)
torch_allclose(y_fft_matrix, y_ref, atol = atol)
\end{verbatim}

\begin{verbatim}
[1] FALSE
[1] FALSE
[1] TRUE
[1] TRUE
[1] TRUE
\end{verbatim}

Reassuringly, the FFT implementations all seem sufficiently accurate.

\hypertarget{comparing-performance}{%
\subsection{Comparing performance}\label{comparing-performance}}

To assess relative performance, we again use \texttt{bench::mark()},
with twenty iterations per function (fig.~\ref{fig-fft-perf}).

\begin{verbatim}
set.seed(777)
torch_manual_seed(777)
library(bench)
library(dplyr)
library(ggplot2)

res <- mark(dft(x),
  dft_vec(x),
  fft(x),
  fft_vec(x),
  fft_matrix(x),
  torch_fft_fft(x),
  iterations = 20,
  check = FALSE
)

res %>%
  mutate(
    expression =
      forcats::fct_reorder(as.character(expression),
        min,
        .desc = TRUE
      )
  ) %>%
  as_bench_mark() %>%
  autoplot(type = "ridge") + theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/fft-perf.png}

}

\caption{\label{fig-fft-perf}Benchmarking various FFT and DFT
implementations (see text). Also includes \texttt{torch\_fft\_fft()} for
reference.}

\end{figure}

Unsurprisingly, none of the implementations comes close to
\texttt{torch\_fft\_fft()}. Intriguingly, though, we see enormous
differences in execution time between our manual implementations. There
is an important insight to be had. Algorithmic properties are like
Platonic ideas: pure and noble in theory, but often, hard to enliven in
the lowlands of everyday life. Here, the hard facts of reality concern
the defining features of the software stack one is using, and in
particular, the characteristics of the programming language involved.
Let me elaborate.

In \texttt{torch} as well as in R, the language, vectorized operations
are much more efficient than iteration (let alone recursion). In that
light, we can make sense of the results like so:

\begin{itemize}
\tightlist
\item
  \texttt{dft()} and \texttt{fft()}, the straightforward realizations,
  are slowest, since they don't use vectorized operations. Among those
  two, \texttt{fft()}, which -- in theory -- should be superior, is
  severely punished for relying on recursion, whose use is strongly
  discouraged in R.
\item
  Both vectorized implementations, \texttt{dft\_vec()} and
  \texttt{fft\_vec()}, perform decidedly better than their un-vectorized
  counterparts. Among these, \texttt{fft\_vec()} is a lot faster than
  \texttt{dft\_vec()}, which is exactly what we'd like to see.
\item
  Given its conceptual and aesthetic appeal, the outcome for
  \texttt{fft\_matrix()} is a bit disappointing.
\end{itemize}

Inspired by wishful thinking, is there anything we could do?

\hypertarget{making-use-of-just-in-time-jit-compilation}{%
\subsection{\texorpdfstring{Making use of Just-in-Time (JIT)
compilation\index{Just-in-Time compilation (JIT)}}{Making use of Just-in-Time (JIT) compilation}}\label{making-use-of-just-in-time-jit-compilation}}

We can try. In \texttt{torch}, a functionality named Just-in-Time (JIT)
compilation allows us to \emph{trace} a function, or a model, to obtain
a highly optimized graph representation. This capability is useful in a
number of ways: to reduce execution time, evidently; but also if, for
example, you'd like to deploy a model (trained in R) in an environment
that does not have R installed. If you're interested, please see the
\href{https://github.com/mlverse/torch/blob/main/vignettes/torchscript.Rmd}{vignette},
as well as a dedicated
\href{https://blogs.rstudio.com/ai/posts/2021-08-10-jit-trace-module/}{blog
post}. Here, we'll just directly put JIT compilation to practical use.

All we have to do is call a function, \texttt{jit\_trace()}, with a
dummy tensor, whose shape has to match the shape of future intended
inputs. We want to do this for the two most promising FFT
implementations, \texttt{fft\_vec()} and \texttt{fft\_matrix()}, as well
as \texttt{torch}'s own \texttt{torch\_fft\_fft()}. The one thing to be
aware of is that, should we think the improvement is worth it, we will
need to trace the function for all input shapes we're interested in.
(Considering that our implementations expect input size to be a power of
two, that wouldn't be too much of a problem.)

Above, the input size we used for benchmarking amounted to 2\^{}13, so
this is what we'll use for tracing, too. Here, first, we run
\texttt{jit\_trace()} for \texttt{fft\_vec()} and
\texttt{torch\_fft\_fft()}:

\begin{verbatim}
x_trace <- torch_randn(2^13)

fft_vec_jit <- jit_trace(fft_vec, x_trace)
fft_fft_jit <- jit_trace(torch_fft_fft, x_trace)
\end{verbatim}

The case of \texttt{fft\_matrix()}, however, is a bit special: Inside,
we're calling \texttt{bitrevorder()}, a function that should not be part
of the \texttt{torch} graph. A workaround, however, is quickly found:
Just move that part of the logic outside the function -- it scarcely
contributes to execution time, anyway. The redefined function now looks
like this:

\begin{verbatim}
fft_matrix_for_jit <- function(x) {
  x <- x$to(dtype = torch_cfloat())

  n_samples <- length(x)

  # smallest butterfly matrix, needed for all valid inputs
  B2 <- torch_tensor(
    c(1, 1, 1, -1),
    dtype = torch_cfloat()
  )$view(c(2, 2))
  B2_block <- torch_block_diag(
    B2$`repeat`(c(n_samples / 2, 1))$split(2)
  )
  acc <- torch_matmul(B2_block, x)

  # iterative implementation then starts with B4
  n <- 4

  while (n <= n_samples) {

    # build up current butterfly matrix
    I <- torch_eye(n / 2)
    O <- torch_diag(
      torch_exp(
        -1i * 2 * pi *
          torch_arange(0, n / 2 - 1) / (n / 2 * 2)
      )
    )
    B <- torch_cat(list(
      torch_cat(list(I, O), dim = 2),
      torch_cat(list(I, -O), dim = 2)
    ), dim = 1)

    # in the final multiplication,
    # B directly matches input length
    if (n == n_samples) {
      return(torch_matmul(B, acc))
    }

    # create block-diagonal matrix from butterflies
    # at each iteration,
    # we need to replicate B {n_samples/rank(B) times}
    # this is achieved by first repeating B row-wise,
    # then splitting up into rank(n) parts)
    B_block <- torch_block_diag(
      B$`repeat`(c(n_samples / n, 1))$split(n)
    )
    acc <- torch_matmul(B_block, acc)
    n <- n * 2
  }
  acc
}
\end{verbatim}

We trace it with pre-sorted input:

\begin{verbatim}
fft_matrix_jit <- jit_trace(
  fft_matrix_for_jit,
  torch_tensor(
    bitrevorder(as.numeric(x_trace)),
    dtype = torch_cfloat()
  )
)
\end{verbatim}

In benchmarking, we call \texttt{fft\_matrix\_jit()} with a pre-sorted
tensor, as well. Here, then, is a comparison of \texttt{fft\_vec(),}
\texttt{fft\_matrix()}, \texttt{torch\_fft\_fft()}, and their respective
optimized versions (fig.~\ref{fig-fft-perf-jit}):

\begin{verbatim}
x_rev <- torch_tensor(
  bitrevorder(
    as.numeric(x_trace)
  ),
  dtype = torch_cfloat()
)

res <- mark(fft_vec(x),
  fft_matrix(x),
  torch_fft_fft(x),
  fft_vec_jit(x),
  fft_matrix_jit(x_rev),
  fft_fft_jit(x),
  iterations = 20,
  check = FALSE
)

res %>%
  mutate(
    expression = forcats::fct_reorder(
      as.character(expression),
      min,
      .desc = TRUE
    )
  ) %>%
  as_bench_mark() %>%
  autoplot(type = "ridge") + theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/fft-perf-jit.png}

}

\caption{\label{fig-fft-perf-jit}Exploring the effect of Just-in-Time
Compilation (JIT) on the performance of \texttt{fft\_vec(),}
\texttt{fft\_matrix()}, and \texttt{torch\_fft\_fft()}.}

\end{figure}

Two things, I'd say, are remarkable about this result. First, each of
the three implementations benefits from getting JIT-compiled, even
\texttt{torch\_fft\_fft()}.

Second, there is one that profits \emph{a lot}: namely,
\texttt{fft\_vec()}. In its compiled version, it is nearly as fast as
\texttt{torch\_fft\_fft()} -- a function that, remember, has all
calculations done in highly optimized C++ code. This again underlines
the enormous impact ``linguistic fit'' has on final performance
outcomes.

By now, we've learned a lot about the Discrete Fourier Transform,
including how to implement it in an efficient way. With all that
knowledge, we should feel comfortable enough making direct use of
\texttt{torch\_fft\_ftt()}. This is exactly what we'll do in the next --
and final -- chapter: We'll complement classic Fourier Analysis with its
younger counterpart, \emph{wavelets}.

\hypertarget{sec:wavelets}{%
\chapter{Wavelets}\label{sec:wavelets}}

We conclude the book with a practical application of \emph{wavelet
analysis} using \texttt{torch}.

With the Fourier Transform, it was all about understanding the concepts,
so we can use \texttt{torch\_fft\_ftt()} with confidence. This time,
there is no ready-to-use function around; but we'll see that computing
the Wavelet Transform ourselves is not that hard. (And yes,
\texttt{torch\_fft\_ftt()} will figure prominently, here, too.)

What are wavelets? Like the Fourier basis, they're functions; but they
don't extend infinitely. Instead, they are localized in time: Away from
the center, they quickly decay to zero. In addition to a \emph{location}
parameter, they also have a \emph{scale}: At different scales, they
appear squished or stretched. Squished, they will do better at detecting
high frequencies; the converse applies when they're stretched out in
time.

The basic operation involved in the Wavelet Transform is convolution --
have the (flipped) wavelet slide over the data, computing a sequence of
dot products. This way, the wavelet is basically looking for
\emph{similarity}.

As to the wavelet functions themselves, there are many of them. In a
practical application, we'd want to experiment and pick the one that
works best for the given data. Compared to the DFT and spectrograms,
more experimentation tends to be involved in wavelet analysis.

The topic of wavelets is very different from that of Fourier transforms
in other respects, as well. Notably, there is a lot less standardization
in terminology, use of symbols, and actual practices. For these reasons,
I will refrain from giving a systematic, general introduction, the way I
did in the DFT chapter. Instead, I'm going to heavily lean on one
specific exposition, the one in Arnt Vistnes' very nice book on waves
(Vistnes 2018). In other words, both terminology and examples will
reflect the choices made in that book.

One choice Vistnes made was to favor depth over breadth, and to focus on
a single wavelet.

\hypertarget{introducing-the-morlet-wavelet}{%
\section{\texorpdfstring{Introducing the Morlet
wavelet\index{Wavelet Transform!Morlet wavelet}}{Introducing the Morlet wavelet}}\label{introducing-the-morlet-wavelet}}

The Morlet, also known as Gabor\footnote{After Dennis Gabor, who came up
  with the idea of using Gaussian-windowed complex exponentials for
  time-frequency decomposition, and Jean Morlet, who elaborated on and
  formalized the resulting transform.}, wavelet is defined like so:

\begin{equation}\protect\hypertarget{eq-wavelets-1}{}{
\Psi_{\omega_{a},K,t_{k}}(t_n) = (e^{-i \omega_{a} (t_n - t_k)} - e^{-K^2}) \ e^{- \omega_a^2 (t_n - t_k )^2 /(2K )^2} 
}\label{eq-wavelets-1}\end{equation}

This formulation pertains to discretized data, the kinds of data we work
with in practice. Thus, \(t_k\) and \(t_n\) designate points in time, or
equivalently, individual time-series samples.

This equation looks daunting at first, but we can ``tame'' it a bit by
analyzing its structure, and pointing to the main actors. For
concreteness, though, we first look at an example wavelet.

We start by implementing Equation~\ref{eq-wavelets-1}:

\begin{verbatim}
library(torch)
library(ggplot2)
library(patchwork)
library(tidyr)
library(zeallot)
  
morlet <- function(omega, K, t_k, t) {
  (torch_exp(-1i * omega * (t - t_k)) -
    torch_exp(-torch_square(K))) *
    torch_exp(-torch_square(omega) * torch_square(t - t_k) /
      torch_square(2 * K))
}
\end{verbatim}

Comparing code and mathematical formulation, we notice a difference. The
function itself takes one argument, \(t_n\); its realization, four
(\texttt{omega}, \texttt{K}, \texttt{t\_k}, and \texttt{t}). This is
because the \texttt{torch} code is vectorized: On the one hand,
\texttt{omega}, \texttt{K}, and \texttt{t\_k}, which, in
Equation~\ref{eq-wavelets-1}, correspond to \(\omega_{a}\), \(K\), and
\(t_k\) , are scalars. (In the equation, they're assumed to be fixed.)
\texttt{t}, on the other hand, is a vector; it will hold the measurement
times of the series to be analyzed.

We pick example values for \texttt{omega}, \texttt{K}, and
\texttt{t\_k}, as well as a range of times to evaluate the wavelet on,
and plot its values (fig.~\ref{fig-wav-morlet-example}):

\begin{verbatim}
omega <- 6 * pi
K <- 6
t_k <- 5
 
sample_time <- torch_arange(3, 7, 0.0001)

create_wavelet_plot <- function(omega, K, t_k, sample_time) {
  morlet <- morlet(omega, K, t_k, sample_time)
  df <- data.frame(
    x = as.numeric(sample_time),
    real = as.numeric(morlet$real),
    imag = as.numeric(morlet$imag)
  ) %>%
    pivot_longer(-x, names_to = "part", values_to = "value")
  ggplot(df, aes(x = x, y = value, color = part)) +
    geom_line() +
    scale_colour_grey(start = 0.8, end = 0.4) +
    xlab("time") +
    ylab("wavelet value") +
    ggtitle("Morlet wavelet",
      subtitle = paste0("_a = ", omega / pi, " , K = ", K)
    ) +
    theme_minimal()
}

create_wavelet_plot(omega, K, t_k, sample_time)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-morlet-example.png}

}

\caption{\label{fig-wav-morlet-example}A Morlet wavelet.}

\end{figure}

What we see here is a complex sine curve -- note the real and imaginary
parts, separated by a phase shift of \(\pi/2\) -- that decays on both
sides of the center. Looking back at Equation~\ref{eq-wavelets-1}, we
can identify the factors responsible for both features. The first term
in the equation, \(e^{-i \omega_{a} (t_n - t_k)}\), generates the
oscillation; the third, \(e^{- \omega_a^2 (t_n - t_k )^2 /(2K )^2}\),
causes the exponential decay away from the center. (In case you're
wondering about the second term, \(e^{-K^2}\): For given \(K\), it is
just a constant.)

The third term actually is a Gaussian, with location parameter \(t_k\)
and scale \(K\). We'll talk about \(K\) in great detail soon, but what's
with \(t_k\)? \(t_k\) is the center of the wavelet; for the Morlet
wavelet, this is also the location of maximum amplitude. As distance
from the center increases, values quickly approach zero. This is what is
meant by wavelets being localized: They are ``active'' only on a short
range of time.

\hypertarget{the-roles-of-k-and-omega_a}{%
\section{\texorpdfstring{The roles of
\(K\)\index{Wavelet Transform!scale} and
\(\omega_a\)\index{Wavelet Transform!analysis frequency}}{The roles of K and \textbackslash omega\_a}}\label{the-roles-of-k-and-omega_a}}

Now, we already said that \(K\) is the scale of the Gaussian; it thus
determines how far the curve spreads out in time. But there is also
\(\omega_a\). Looking back at the Gaussian term, it, too, will impact
the spread.

First though, what is \(\omega_a\)? The subscript \(a\) stands for
``analysis''; thus, \(\omega_a\) denotes a single frequency being
probed. At this point, a quick aside on workflow -- this will become
much clearer later, when we actually run the Wavelet Transform):

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A single transform, resulting in spectrogram-like, three-dimensional
output, operates on a set of analysis frequencies (\(\omega_a\)), and
covers the whole time range (\(t_n\)).

That covering is achieved by having the wavelet slide over the input,
its position at each step being identified by its center, \(t_k\).

What's missing from this characterization is \(K\). That's because you
run a separate wavelet analysis for each \(K\) of interest.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Now, let's first inspect visually the respective impacts of \(\omega_a\)
and \(K\) (fig.~\ref{fig-wav-K-omega}).

\begin{verbatim}
p1 <- create_wavelet_plot(6 * pi, 4, 5, sample_time)
p2 <- create_wavelet_plot(6 * pi, 6, 5, sample_time)
p3 <- create_wavelet_plot(6 * pi, 8, 5, sample_time)
p4 <- create_wavelet_plot(4 * pi, 6, 5, sample_time)
p5 <- create_wavelet_plot(6 * pi, 6, 5, sample_time)
p6 <- create_wavelet_plot(8 * pi, 6, 5, sample_time)

(p1 | p4) /
  (p2 | p5) /
  (p3 | p6)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-K-omega.png}

}

\caption{\label{fig-wav-K-omega}Morlet wavelet: Effects of varying scale
and analysis frequency.}

\end{figure}

In the left column, we keep \(\omega_a\) constant, and vary \(K\). On
the right, \(\omega_a\) changes, and \(K\) stays the same.

Firstly, we observe that the higher \(K\), the more the curve gets
spread out. In a wavelet analysis, this means that more points in time
will contribute to the transform's output, resulting in high precision
as to frequency content, but loss of resolution in time. (We'll return
to this -- central -- trade-off soon.)

As to \(\omega_a\), its impact is twofold. On the one hand, in the
Gaussian term, it counteracts -- \emph{exactly}, even -- the scale
parameter, \(K\). On the other, it determines the frequency, or
equivalently, the period, of the wave. To see this, take a look at the
right column. Corresponding to the different frequencies, we have, in
the interval between 4 and 6, four, six, or eight peaks, respectively.

This double role of \(\omega_a\) is the reason why, all-in-all, it
\emph{does} make a difference whether we shrink \(K\), keeping
\(\omega_a\) constant, or increase \(\omega_a\), holding \(K\) fixed.

This state of things sounds complicated, but is less problematic than it
might seem. In practice, understanding the role of \(K\) is important,
since we need to pick sensible \(K\) values to try. As to the
\(\omega_a\), on the other hand, there will be a multitude of them,
corresponding to the range of frequencies we analyze.

So we can understand the impact of \(K\) in more detail, we need to take
a first look at the Wavelet Transform.

\hypertarget{wavelet-transform-a-straightforward-implementation}{%
\section{\texorpdfstring{Wavelet Transform: A straightforward
implementation\index{Wavelet Transform!in the time domain}}{Wavelet Transform: A straightforward implementation}}\label{wavelet-transform-a-straightforward-implementation}}

While overall, the topic of wavelets is more multifaceted, and thus, may
seem more enigmatic than Fourier analysis, the transform itself is
easier to grasp. It is a sequence of local convolutions between wavelet
and signal. Here is the formula for specific scale parameter \(K\),
analysis frequency \(\omega_a\), and wavelet location \(t_k\):

\[
W_{K, \omega_a, t_k} = \sum_n  x_n \Psi_{\omega_{a},K,t_{k}}^*(t_n)
\]

This is just a dot product, computed between signal and
complex-conjugated wavelet. (Here complex conjugation flips the wavelet
in time, making this \emph{convolution}, not correlation -- a fact that
matters a lot, as you'll see soon.)

Correspondingly, straightforward implementation results in a sequence of
dot products, each corresponding to a different alignment of wavelet and
signal. Below, in \texttt{wavelet\_transform()}, arguments
\texttt{omega} and \texttt{K} are scalars, while \texttt{x}, the signal,
is a vector. The result is the wavelet-transformed signal, for some
specific \texttt{K} and \texttt{omega} of interest.

\begin{verbatim}
wavelet_transform <- function(x, omega, K) {
  n_samples <- dim(x)[1]
  W <- torch_complex(
    torch_zeros(n_samples), torch_zeros(n_samples)
  )
  for (i in 1:n_samples) {
    # move center of wavelet
    t_k <- x[i, 1]
    m <- morlet(omega, K, t_k, x[, 1])
    # compute local dot product
    # note wavelet is conjugated
    dot <- torch_matmul(
      m$conj()$unsqueeze(1),
      x[, 2]$to(dtype = torch_cfloat())
    )
    W[i] <- dot
  }
  W
}
\end{verbatim}

To test this, we generate a simple sine wave that has a frequency of 100
Hertz in its first part, and double that in the second
(fig.~\ref{fig-wav-signal-short}).

\begin{verbatim}
gencos <- function(amp, freq, phase, fs, duration) {
  x <- torch_arange(0, duration, 1 / fs)[1:-2]$unsqueeze(2)
  y <- amp * torch_cos(2 * pi * freq * x + phase)
  torch_cat(list(x, y), dim = 2)
}

# sampling frequency
fs <- 8000

f1 <- 100
f2 <- 200
phase <- 0
duration <- 0.25

s1 <- gencos(1, f1, phase, fs, duration)
s2 <- gencos(1, f2, phase, fs, duration)

s3 <- torch_cat(list(s1, s2), dim = 1)
s3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] <-
  s3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] + duration

df <- data.frame(
  x = as.numeric(s3[, 1]),
  y = as.numeric(s3[, 2])
)
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-signal-short.png}

}

\caption{\label{fig-wav-signal-short}An example signal, consisting of a
low-frequency and a high-frequency half.}

\end{figure}

Now, we run the Wavelet Transform on this signal, for an analysis
frequency of 100 Hertz, and with a \texttt{K} parameter of 2, found
through quick experimentation
(fig.~\ref{fig-wav-signal-short-transform}):

\begin{verbatim}
K <- 2
omega <- 2 * pi * f1

res <- wavelet_transform(x = s3, omega, K)
df <- data.frame(
  x = as.numeric(s3[, 1]),
  y = as.numeric(res$abs())
)

ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("Wavelet Transform") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-signal-short-transform.png}

}

\caption{\label{fig-wav-signal-short-transform}Wavelet Transform of the
above two-part signal. Analysis frequency is 100 Hertz.}

\end{figure}

The transform correctly picks out the part of the signal that matches
the analysis frequency. If you feel like, you might want to double-check
what happens for an analysis frequency of 200 Hertz.

Now, in reality we will want to run this analysis not for a single
frequency, but a range of frequencies we're interested in. And we will
want to try different scales \texttt{K}. Now, if you executed the code
above, you might be worried that this could take a \emph{lot} of time.

Well, it by necessity takes longer to compute than its Fourier analogue,
the spectrogram. For one, that's because with spectrograms, the analysis
is ``just'' two-dimensional, the axes being time and frequency. With
wavelets there are, in addition, different scales to be explored. And
secondly, spectrograms operate on whole windows (with configurable
overlap); a wavelet, on the other hand, slides over the signal in unit
steps.

Still, the situation is not as grave as it sounds. The Wavelet Transform
being a \emph{convolution}, we can implement it in the Fourier domain
instead. We'll do that very soon, but first, as promised, let's revisit
the topic of varying \texttt{K}.

\hypertarget{resolution-in-time-versus-in-frequency}{%
\section{Resolution in time versus in
frequency}\label{resolution-in-time-versus-in-frequency}}

We already saw that the higher \texttt{K}, the more spread-out the
wavelet. We can use our first, maximally straightforward, example, to
investigate one immediate consequence. What, for example, happens for
\texttt{K} set to twenty
(fig.~\ref{fig-wav-signal-short-transform-largeK})?

\begin{verbatim}
K <- 20

res <- wavelet_transform(x = s3, omega, K)
df <- data.frame(
  x = as.numeric(s3[, 1]),
  y = as.numeric(res$abs())
)

ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("Wavelet Transform") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-signal-short-transform-largeK.png}

}

\caption{\label{fig-wav-signal-short-transform-largeK}Wavelet Transform
of the above two-part signal, with \texttt{K} set to twenty instead of
two.}

\end{figure}

The Wavelet Transform still picks out the correct region of the signal
-- but now, instead of a rectangle-like result, we get a significantly
smoothed version that does not sharply separate the two regions.

Notably, the first 0.05 seconds, too, show considerable smoothing. The
larger a wavelet, the more element-wise products will be lost at the end
and the beginning. This is because transforms are computed aligning the
wavelet at all signal positions, from the very first to the last.
Concretely, when we compute the dot product at location
\texttt{t\_k\ =\ 1}, just a single sample of the signal is considered.

Apart from possibly introducing unreliability at the boundaries, how
does wavelet scale affect the analysis? Well, since we're
\emph{correlating} (\emph{convolving}, technically; but in this case,
the effect, in the end, is the same) the wavelet with the signal,
point-wise similarity is what matters. Concretely, assume the signal is
a pure sine wave, the wavelet we're using is a windowed sinusoid like
the Morlet, and that we've found an optimal \texttt{K} that nicely
captures the signal's frequency. Then any other \texttt{K}, be it larger
or smaller, will result in less point-wise overlap.

This leads to a question we can't get around.

\hypertarget{how-is-this-different-from-a-spectrogram}{%
\section{How is this different from a
spectrogram?}\label{how-is-this-different-from-a-spectrogram}}

Chances are you have been mentally comparing what we're doing here not
with the Fourier Transform per se, but with its windowed progeny, the
spectrogram. Both methods tell us how frequency composition varies over
time. Both involve trade-offs between resolution in frequency and in
time. How, then, are they different?

Let's start with the way spectrograms are normally computed. Usually,
the choice of window size is the same for all frequency ranges. Then,
resolution of high frequencies will be better than that of lower ones,
since more periods fit into one analysis window.

To counteract this, it would be possible to choose different window
sizes for different ranges of frequencies. But then, how do you make
those choices? It is a lot more convenient to, in wavelet analysis,
experiment with different \texttt{K}, all the more since with growing
experience, those explorations will resemble to trial-and-error
endeavors less and less.

\hypertarget{performing-the-wavelet-transform-in-the-fourier-domain}{%
\section{\texorpdfstring{Performing the Wavelet Transform in the Fourier
domain\index{Wavelet Transform!in the Fourier domain}}{Performing the Wavelet Transform in the Fourier domain}}\label{performing-the-wavelet-transform-in-the-fourier-domain}}

Soon, we will run the Wavelet Transform on a longer signal. Thus, it is
time to speed up computation. We already said that here, we benefit from
time-domain convolution being equivalent to multiplication in the
Fourier domain. The overall process then is this: First, compute the DFT
of both signal and wavelet; second, multiply the results; third,
inverse-transform back to the time domain.

With the Morlet wavelet, we don't even have to run the FFT: Its
Fourier-domain representation can be stated in closed form. We'll just
make use of that formulation from the outset.

To see how this works, we first manually step through the process. In
the following section, we'll package things up for quick execution.

Here, first, is the signal. It again is composed of two different
frequencies, 100 Hertz and 200 Hertz. But this time, we repeat the
alternation several times, and map those frequencies to different
amplitudes. The latter may seem like an innocent change, but there is an
important consequence: We have to adapt our cosine-generating code to
keep track of the phase. If we don't do this, the signal will be
distorted at the transition points.

\begin{verbatim}
gencosines <- function(freqs, amps, samples_per_frame, fs) {
  t <- (1:samples_per_frame) / fs
  lastphase <- 0
  x <- torch_zeros(length(freqs), samples_per_frame)

  for (i in 1:length(freqs)) {
    amp <- torch_ones(samples_per_frame) * amps[i]
    freq <- torch_ones(samples_per_frame) * freqs[i]

    phase <- torch_tensor(2 * pi * freq * t + lastphase)
    x_frame <- amp * torch_cos(phase)

    # save phase to be used in next frame
    lastphase <- as.numeric(phase[samples_per_frame]) %%
      (2 * pi)
    x[i, ] <- x_frame
  }

  x$reshape(length(freqs) * samples_per_frame)
}
\end{verbatim}

Here is the signal (fig.~\ref{fig-wav-signal-long}):

\begin{verbatim}
freqs <- c(100, 200, 100, 200, 100, 200, 100)
amps <- c(1.2, 0.8, 1.2, 0.8, 1.2, 0.8, 1.2)
samples_per_frame <- 100
fs <- 800

x <- gencosines(freqs, amps, samples_per_frame, fs)
df <- data.frame(x = 1:dim(x)[1], y = as.numeric(x))
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("sample") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-signal-long.png}

}

\caption{\label{fig-wav-signal-long}A signal, consisting of alternating,
different-amplitude low-frequency and high-frequency halves.}

\end{figure}

The DFT of the signal is quickly computed:

\begin{verbatim}
F <- torch_fft_fft(x)
\end{verbatim}

The Fourier-domain representation of the wavelet, on the other hand, is
given by:

\begin{verbatim}
morlet_fourier <- function(K, omega_a, omega) {
  2 * (torch_exp(-torch_square(
    K * (omega - omega_a) / omega_a
  )) -
    torch_exp(-torch_square(K)) *
      torch_exp(-torch_square(K * omega / omega_a)))
}
\end{verbatim}

Comparing this statement of the wavelet to the time-domain one, we see
that -- as expected -- instead of parameters \texttt{t} and
\texttt{t\_k} it now takes \texttt{omega} and \texttt{omega\_a}. The
latter, \texttt{omega\_a}, is the analysis frequency, the one we're
probing for, a scalar; the former, \texttt{omega}, the range of
frequencies that appear in the DFT of the signal.

In instantiating the wavelet, there is one thing we need to pay special
attention to. In FFT-think, the frequencies are bins; their number is
determined by the length of the signal (a length that, for its part,
directly depends on sampling frequency). Our wavelet, on the other hand,
works with frequencies in Hertz (nicely, from a user's perspective;
since this unit is meaningful to us). What this means is that to
\texttt{morlet\_fourier}, as \texttt{omega\_a} we need to pass not the
value in Hertz, but the corresponding FFT bin. Conversion is done
relating the number of bins, \texttt{dim(x){[}1{]}}, to the sampling
frequency of the signal, \texttt{fs}:

\begin{verbatim}
# again look for 100Hz parts
omega <- 2 * pi * f1

# need the bin corresponding to some frequency in Hz
omega_bin <- f1/fs * dim(x)[1]
\end{verbatim}

We instantiate the wavelet, perform the Fourier-domain multiplication,
and inverse-transform the result:

\begin{verbatim}
K <- 3

m <- morlet_fourier(K, omega_bin, 1:dim(x)[1])
prod <- F * m
transformed <- torch_fft_ifft(prod)
\end{verbatim}

And this is the result (fig.~\ref{fig-wav-example-long-transform}):

\begin{verbatim}
df <- data.frame(
  x = as.numeric(1:dim(x)[1]) / fs,
  y = as.numeric(transformed$abs())
)
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("Wavelet Transform") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-transform.png}

}

\caption{\label{fig-wav-example-long-transform}Wavelet Transform of the
above alternating-frequency signal. Analysis frequency is 100 Hertz.}

\end{figure}

All 100-Hertz-regions are identified flawlessly. Also, looking at the
y-axis, we see that the amplitude is correct.

Putting together wavelet instantiation and the steps involved in the
analysis, we have the following. (Note how to
\texttt{wavelet\_transform\_fourier}, we now, conveniently, pass in the
frequency value in Hertz.)

\begin{verbatim}
wavelet_transform_fourier <- function(x, omega_a, K, fs) {
  N <- dim(x)[1]
  omega_bin <- omega_a / fs * N
  m <- morlet_fourier(K, omega_bin, 1:N)
  x_fft <- torch_fft_fft(x)
  prod <- x_fft * m
  w <- torch_fft_ifft(prod)
  w
}
\end{verbatim}

Let's use this to probe for the second frequency used, 200 Hertz
(fig.~\ref{fig-wav-example-long-transform2}).

\begin{verbatim}
K <- 6

transformed <- wavelet_transform_fourier(x, f2, K, fs)
df <- data.frame(
  x = 1:dim(x)[1] / fs,
  y = as.numeric(transformed$abs())
)
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("time") +
  ylab("Wavelet Transform") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-transform2.png}

}

\caption{\label{fig-wav-example-long-transform2}Wavelet Transform of the
above alternating-frequency signal. Analysis frequency is 200 Hertz.}

\end{figure}

Again, we find that matching regions, as well as the amplitude, are
correctly identified.

We've already made significant progress. We're ready for the final step:
automating analysis over a range of frequencies of interest. This will
result in a three-dimensional representation, the wavelet diagram.

\hypertarget{creating-the-wavelet-diagram}{%
\section{\texorpdfstring{Creating the wavelet
diagram\index{Wavelet Transform!scaleogram}}{Creating the wavelet diagram}}\label{creating-the-wavelet-diagram}}

In the Fourier Transform, the number of coefficients we obtain depends
on signal length, and effectively reduces to half the sampling
frequency. With its wavelet analogue, since anyway we're doing a loop
over frequencies, we might as well decide which frequencies to analyze.

Firstly, the range of frequencies of interest can be determined running
the DFT. The next question, then, is about granularity. Here, I'll be
following the recommendation given in Vistnes' book, which is based on
the relation between current frequency value and wavelet scale,
\texttt{K}.

Iteration over frequencies is then implemented as a loop:

\begin{verbatim}
wavelet_grid <- function(x, K, f_start, f_end, fs) {
  # downsample analysis frequency range
  # as per Vistnes, eq. 14.17
  num_freqs <- 1 + log(f_end / f_start)/ log(1 + 1/(8 * K))
  freqs <- seq(f_start, f_end, length.out = floor(num_freqs))
  
  transformed <- torch_zeros(
    num_freqs, dim(x)[1],
    dtype = torch_cfloat()
    )
  for(i in 1:num_freqs) {
    w <- wavelet_transform_fourier(x, freqs[i], K, fs)
    transformed[i, ] <- w
  }
  list(transformed, freqs)
}
\end{verbatim}

Calling \texttt{wavelet\_grid()} will give us the analysis frequencies
used, together with the respective outputs from the Wavelet Transform.

Next, we create a utility function that visualizes the result. By
default, \texttt{plot\_wavelet\_diagram()} displays the magnitude of the
wavelet-transformed series; it can, however, plot the squared
magnitudes, too, as well as their square root, a method much recommended
by Vistnes whose effectiveness we will soon have opportunity to witness.

The function deserves a few further comments.

Firstly, same as we did with the analysis frequencies, we down-sample
the signal itself, avoiding to suggest a resolution that is not actually
present. The formula, again, is taken from Vistnes' book.

Then, we use interpolation to obtain a new time-frequency grid. This
step may even be necessary if we keep the original grid, since when
distances between grid points are very small, R's \texttt{image()} may
refuse to accept axes as evenly spaced.

Finally, note how frequencies are arranged on a log scale. This leads to
much more useful visualizations.

\begin{verbatim}
plot_wavelet_diagram <- function(x,
                                 freqs,
                                 grid,
                                 K,
                                 fs,
                                 f_end,
                                 type = "magnitude") {
  grid <- switch(type,
    magnitude = grid$abs(),
    magnitude_squared = torch_square(grid$abs()),
    magnitude_sqrt = torch_sqrt(grid$abs())
  )

  # downsample time series
  # as per Vistnes, eq. 14.9
  new_x_take_every <- max(K / 24 * fs / f_end, 1)
  new_x_length <- floor(dim(grid)[2] / new_x_take_every)
  new_x <- torch_arange(
    x[1],
    x[dim(x)[1]],
    step = x[dim(x)[1]] / new_x_length
  )
  
  # interpolate grid
  new_grid <- nnf_interpolate(
    grid$view(c(1, 1, dim(grid)[1], dim(grid)[2])),
    c(dim(grid)[1], new_x_length)
  )$squeeze()
  out <- as.matrix(new_grid)

  # plot log frequencies
  freqs <- log10(freqs)
  
  image(
    x = as.numeric(new_x),
    y = freqs,
    z = t(out),
    ylab = "log frequency [Hz]",
    xlab = "time [s]",
    col = hcl.colors(12, palette = "Light grays")
  )
  main <- paste0("Wavelet Transform, K = ", K)
  sub <- switch(type,
    magnitude = "Magnitude",
    magnitude_squared = "Magnitude squared",
    magnitude_sqrt = "Magnitude (square root)"
  )

  mtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)
  mtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)
}
\end{verbatim}

Now, let's see a few wavelet diagrams. We are going to compare choices
in two categories. The first concerns \emph{what} is displayed:
magnitude, magnitude squared, or the square root. Since options are
finite, in practice you could always try them all. The second, however,
is essential: It is about \texttt{K}, the scale of the Morlet wavelet.

Though, in theory, the range of possible \texttt{K}s is infinite, as
soon as we start to enlarge or shrink \texttt{K} we see the respective
effects very quickly. And in a publication you can, of course, provide
diagrams for a few different \texttt{K}. (Maybe you even \emph{should};
the information conveyed might be complementary.)

We start with a fixed \texttt{K} of 12, and stay with the default
display (which is magnitude; see fig.~\ref{fig-wav-example-long-diag1}).
As to the range of frequencies to be analysed, it directly follows from
our having generated the data ourselves.

\begin{verbatim}
f_start <- 70
f_end <- 230

K <- 12
c(grid, freqs) %<-% wavelet_grid(
  x, K, f_start, f_end, fs
)
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag1.png}

}

\caption{\label{fig-wav-example-long-diag1}Wavelet diagram of the above
alternating-frequency signal for K = 12. Displaying magnitude as per
default.}

\end{figure}

Now compare this with how it looks for squared magnitudes
(fig.~\ref{fig-wav-example-long-diag2}).

\begin{verbatim}
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,
  type = "magnitude_squared"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag2.png}

}

\caption{\label{fig-wav-example-long-diag2}Wavelet diagram of the above
alternating-frequency signal for K = 12. Displaying magnitude squared.}

\end{figure}

As well as square roots (fig.~\ref{fig-wav-example-long-diag3}):

\begin{verbatim}
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,
  type = "magnitude_sqrt"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag3.png}

}

\caption{\label{fig-wav-example-long-diag3}Wavelet diagram of the above
alternating-frequency signal for K = 12. Displaying the square root of
the magnitude.}

\end{figure}

In this case it is clear which display mode works best. But that's just
because we've created a \emph{very} regular signal.

Now, let's see what happens for smaller \texttt{K}: 6, say. We only plot
magnitude squared (fig.~\ref{fig-wav-example-long-diag4}).

\begin{verbatim}
K <- 6
c(grid, freqs) %<-% wavelet_grid(
  x, K, f_start, f_end, fs
)
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,
  type = "magnitude_squared"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag4.png}

}

\caption{\label{fig-wav-example-long-diag4}Wavelet diagram of the above
alternating-frequency signal for K = 6.}

\end{figure}

While frequency resolution clearly got worse, there is no complementary
improvement in how time is handled. Again, this is no surprise, the
signal being what it is.

What about the other direction? For \texttt{K\ =\ 24} we have
(fig.~\ref{fig-wav-example-long-diag5}):

\begin{verbatim}
K <- 24
c(grid, freqs) %<-% wavelet_grid(
  x, K, f_start, f_end, fs
)
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,
  type = "magnitude_squared"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag5.png}

}

\caption{\label{fig-wav-example-long-diag5}Wavelet diagram of the above
alternating-frequency signal for K = 24.}

\end{figure}

At this scale, we do see a decline in time resolution. Evidently, our
initial choice of 12 was better. Just for curiosity's sake, though,
let's see what would happen for yet bigger \texttt{K}
(fig.~\ref{fig-wav-example-long-diag6}).

\begin{verbatim}
K <- 48
c(grid, freqs) %<-% wavelet_grid(
  x, K, f_start, f_end, fs
)
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,
  type = "magnitude_squared"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-example-long-diag6.png}

}

\caption{\label{fig-wav-example-long-diag6}Wavelet diagram of the above
alternating-frequency signal for K = 48.}

\end{figure}

The result speaks for itself.

Finally -- now that we have available re-usable pieces of code, and
acquired some first intuition on how time-frequency trade-off works for
wavelets -- we conclude with a real-world example.

\hypertarget{a-real-world-example-chaffinchs-song}{%
\section{A real-world example: Chaffinch's
song}\label{a-real-world-example-chaffinchs-song}}

For the case study, I've chosen what, to me, was the most impressive
wavelet analysis shown in Vistnes' book. It's a sample of a chaffinch's
singing, and it's available on Vistnes' website.

\begin{verbatim}
url <- "http://www.physics.uio.no/pow/wavbirds/chaffinch.wav"

download.file(
 file.path(url),
 destfile = "resources/chaffinch.wav"
)
\end{verbatim}

We use \texttt{torchaudio} to load the file, and convert from stereo to
mono using \texttt{tuneR}'s appropriately named \texttt{mono()}. (For
the kind of analysis we're doing, there is no point in keeping two
channels around.)

\begin{verbatim}
library(torchaudio)
library(tuneR)

wav <- tuneR_loader("resources/chaffinch.wav")
wav <- mono(wav, "both")
wav
\end{verbatim}

\begin{verbatim}
Wave Object
    Number of Samples:      1864548
    Duration (seconds):     42.28
    Samplingrate (Hertz):   44100
    Channels (Mono/Stereo): Mono
    PCM (integer format):   TRUE
    Bit (8/16/24/32/64):    16 
\end{verbatim}

For analysis, we don't need the complete sequence. Helpfully, Vistnes
also published a recommendation as to which range of samples to analyze.

\begin{verbatim}
waveform_and_sample_rate <- transform_to_tensor(wav)
x <- waveform_and_sample_rate[[1]]$squeeze()
fs <- waveform_and_sample_rate[[2]]

# http://www.physics.uio.no/pow/wavbirds/chaffinchInfo.txt
start <- 34000
N <- 1024 * 128
end <- start + N - 1
x <- x[start:end]

dim(x)
\end{verbatim}

\begin{verbatim}
[1] 131072
\end{verbatim}

See fig.~\ref{fig-wav-chaffinch} for a time-domain view. (Don't miss out
on the occasion to actually \emph{listen} to it, on your laptop.)

\begin{verbatim}
df <- data.frame(x = 1:dim(x)[1], y = as.numeric(x))
ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  xlab("sample") +
  ylab("amplitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-chaffinch.png}

}

\caption{\label{fig-wav-chaffinch}Chaffinch's song.}

\end{figure}

Now, we need to determine a reasonable range of analysis frequencies. To
that end, we run the FFT:

\begin{verbatim}
F <- torch_fft_fft(x)
\end{verbatim}

On the x-axis, we plot frequencies, not sample numbers, and for better
visibility, we zoom in a bit (fig.~\ref{fig-wav-chaffinch-fft}).

\begin{verbatim}
bins <- 1:dim(F)[1]
freqs <- bins / N * fs

# the bin, not the frequency
cutoff <- N/4

df <- data.frame(
  x = freqs[1:cutoff],
  y = as.numeric(F$abs())[1:cutoff]
)
ggplot(df, aes(x = x, y = y)) +
  geom_col() +
  xlab("frequency (Hz)") +
  ylab("magnitude") +
  theme_minimal()
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-chaffinch-fft.png}

}

\caption{\label{fig-wav-chaffinch-fft}Chaffinch's song, Fourier spectrum
(excerpt).}

\end{figure}

Based on this distribution, we can safely restrict the range of analysis
frequencies to between, approximately, 1800 and 8500 Hertz. (This is
also the range recommended by Vistnes.)

First, though, let's anchor expectations by creating a spectrogram for
this signal. Suitable values for FFT size and window size were found
experimentally. And though, in spectrograms, you don't see this done
often, I found that displaying square roots of coefficient magnitudes
yielded the most informative output.

\begin{verbatim}
fft_size <- 1024
window_size <- 1024
power <- 0.5

spectrogram <- transform_spectrogram(
  n_fft = fft_size,
  win_length = window_size,
  normalized = TRUE,
  power = power
)

spec <- spectrogram(x)
dim(spec)
\end{verbatim}

\begin{verbatim}
[1] 513 257
\end{verbatim}

Like we do with wavelet diagrams, we plot frequencies on a log scale
(fig.~\ref{fig-wav-chaffinch-spectrogram}).

\begin{verbatim}
bins <- 1:dim(spec)[1]
freqs <- bins * fs / fft_size
log_freqs <- log10(freqs)

frames <- 1:(dim(spec)[2])
seconds <- (frames / dim(spec)[2])  * (dim(x)[1] / fs)

image(x = seconds,
      y = log_freqs,
      z = t(as.matrix(spec)),
      ylab = 'log frequency [Hz]',
      xlab = 'time [s]',
      col = hcl.colors(12, palette = "Light grays")
)
main <- paste0("Spectrogram, window size = ", window_size)
sub <- "Magnitude (square root)"
mtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)
mtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-chaffinch-spectrogram.png}

}

\caption{\label{fig-wav-chaffinch-spectrogram}Chaffinch's song,
spectrogram.}

\end{figure}

The spectrogram already shows a distinctive pattern. Let's see what can
be done with wavelet analysis. Having experimented with a few different
\texttt{K}, I agree with Vistnes that \texttt{K\ =\ 48} makes for an
excellent choice (fig.~\ref{fig-wav-chaffinch-waveletdiag}):

\begin{verbatim}
f_start <- 1800
f_end <- 8500

K <- 48
c(grid, freqs) %<-% wavelet_grid(x, K, f_start, f_end, fs)
plot_wavelet_diagram(
  torch_tensor(1:dim(grid)[2]),
  freqs, grid, K, fs, f_end,
  type = "magnitude_sqrt"
)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{images/wav-chaffinch-waveletdiag.png}

}

\caption{\label{fig-wav-chaffinch-waveletdiag}Chaffinch's song, wavelet
diagram.}

\end{figure}

The gain in resolution, on both the time and the frequency axis, is
utterly impressive.

With this example, that hopefully has been an inspiring one, I conclude
this chapter, and the book. I wish you all the best in your future
endeavours with \texttt{torch}!

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-abs-2104-13478}{}}%
Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic.
2021. {``Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and
Gauges.''} \emph{CoRR} abs/2104.13478.
\url{https://arxiv.org/abs/2104.13478}.

\leavevmode\vadjust pre{\hypertarget{ref-2019EA000740}{}}%
Cho, Dongjin, Cheolhee Yoo, Jungho Im, and Dong-Hyun Cha. 2020.
{``Comparative Assessment of Various Machine Learning-Based Bias
Correction Methods for Numerical Weather Prediction Model Forecasts of
Extreme Air Temperatures in Urban Areas.''} \emph{Earth and Space
Science} 7 (4): e2019EA000740.
https://doi.org/\url{https://doi.org/10.1029/2019EA000740}.

\leavevmode\vadjust pre{\hypertarget{ref-ChoMGBSB14}{}}%
Cho, Kyunghyun, Bart van Merrienboer, aglar Glehre, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. {``Learning Phrase
Representations Using {RNN} Encoder-Decoder for Statistical Machine
Translation.''} \emph{CoRR} abs/1406.1078.
\url{http://arxiv.org/abs/1406.1078}.

\leavevmode\vadjust pre{\hypertarget{ref-2016arXiv160307285D}{}}%
Dumoulin, Vincent, and Francesco Visin. 2016. {``{A guide to convolution
arithmetic for deep learning}.''} \emph{arXiv e-Prints}, March,
arXiv:1603.07285. \url{https://arxiv.org/abs/1603.07285}.

\leavevmode\vadjust pre{\hypertarget{ref-HeZRS15}{}}%
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. {``Deep
Residual Learning for Image Recognition.''} \emph{CoRR} abs/1512.03385.
\url{http://arxiv.org/abs/1512.03385}.

\leavevmode\vadjust pre{\hypertarget{ref-hochreiter1997long}{}}%
Hochreiter, Sepp, and Jrgen Schmidhuber. 1997. {``Long Short-Term
Memory.''} \emph{Neural Computation} 9 (8): 1735--80.

\leavevmode\vadjust pre{\hypertarget{ref-ioffe2015batch}{}}%
Ioffe, Sergey, and Christian Szegedy. 2015. {``Batch Normalization:
Accelerating Deep Network Training by Reducing Internal Covariate
Shift.''} \url{https://arxiv.org/abs/1502.03167}.

\leavevmode\vadjust pre{\hypertarget{ref-LoshchilovH16a}{}}%
Loshchilov, Ilya, and Frank Hutter. 2016. {``{SGDR:} Stochastic Gradient
Descent with Restarts.''} \emph{CoRR} abs/1608.03983.
\url{http://arxiv.org/abs/1608.03983}.

\leavevmode\vadjust pre{\hypertarget{ref-olah2017feature}{}}%
Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017.
{``Feature Visualization.''} \emph{Distill}.
\url{https://doi.org/10.23915/distill.00007}.

\leavevmode\vadjust pre{\hypertarget{ref-Osgood}{}}%
Osgood, Brad. 2019. \emph{Lectures on the Fourier Transform and Its
Applications}. American Mathematical Society.

\leavevmode\vadjust pre{\hypertarget{ref-RonnebergerFB15}{}}%
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. {``U-Net:
Convolutional Networks for Biomedical Image Segmentation.''} \emph{CoRR}
abs/1505.04597. \url{http://arxiv.org/abs/1505.04597}.

\leavevmode\vadjust pre{\hypertarget{ref-abs-1801-04381}{}}%
Sandler, Mark, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and
Liang-Chieh Chen. 2018. {``Inverted Residuals and Linear Bottlenecks:
Mobile Networks for Classification, Detection and Segmentation.''}
\emph{CoRR} abs/1801.04381. \url{http://arxiv.org/abs/1801.04381}.

\leavevmode\vadjust pre{\hypertarget{ref-Smith15a}{}}%
Smith, Leslie N. 2015. {``No More Pesky Learning Rate Guessing Games.''}
\emph{CoRR} abs/1506.01186. \url{http://arxiv.org/abs/1506.01186}.

\leavevmode\vadjust pre{\hypertarget{ref-abs-1708-07120}{}}%
Smith, Leslie N., and Nicholay Topin. 2017. {``Super-Convergence: Very
Fast Training of Residual Networks Using Large Learning Rates.''}
\emph{CoRR} abs/1708.07120. \url{http://arxiv.org/abs/1708.07120}.

\leavevmode\vadjust pre{\hypertarget{ref-10.5555ux2f2627435.2670313}{}}%
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,
and Ruslan Salakhutdinov. 2014. {``Dropout: A Simple Way to Prevent
Neural Networks from Overfitting.''} \emph{J. Mach. Learn. Res.} 15 (1):
1929--58.

\leavevmode\vadjust pre{\hypertarget{ref-Trefethen}{}}%
Trefethen, Lloyd N., and David Bau. 1997. \emph{Numerical Linear
Algebra}. SIAM.

\leavevmode\vadjust pre{\hypertarget{ref-waves}{}}%
Vistnes, Arnt Inge. 2018. \emph{Physics of Oscillations and Waves. With
Use of Matlab and Python}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-abs-1804-03209}{}}%
Warden, Pete. 2018. {``Speech Commands: {A} Dataset for
Limited-Vocabulary Speech Recognition.''} \emph{CoRR} abs/1804.03209.
\url{http://arxiv.org/abs/1804.03209}.

\leavevmode\vadjust pre{\hypertarget{ref-2017arXiv171009412Z}{}}%
Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz.
2017. {``{mixup: Beyond Empirical Risk Minimization}.''} \emph{arXiv
e-Prints}, October, arXiv:1710.09412.
\url{https://arxiv.org/abs/1710.09412}.

\end{CSLReferences}



\printindex

\end{document}
